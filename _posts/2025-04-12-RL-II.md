---
layout: distill
title: Reinforcement Learning Notes II - Policy Gradients & Actor-Critic Methods
categories: Notes
date: 2025-04-12
description: Policy Gradients, Actor-Critic Methods, Model-based RL
tags:
  - Reinforcement Learning
  - Artificial Intelligence
# pseudocode: true
giscus_comments: true
related_posts: false
authors:
  - name: Pu Fanyi
    url: "https://pufanyi.com"
    affiliations:
      name: NTU, Singapore
      url: "https://www.ntu.edu.sg/"
bibliography: 2025-04-12-RL-II.bib
---

## Direct Policy Differentiation

REINFORCE Algorithm <d-cite key="williams1992simple"></d-cite>:

$$
\begin{aligned}
\nabla_{\theta} \mathcal{J}(\theta) &= \int \nabla_{\theta} \pi_{\theta}(\tau) r(\tau) \mathrm{d}\tau \\
&= \mathbb{E}_{\tau \sim \pi_{\theta}(r)}\left[r(\tau) \cdot \nabla_{\theta} \log \pi_{\theta}(\tau)\right] \\
&= \mathbb{E}_{\tau \sim \pi_{\theta}(r)}\left[\left(\sum_{t=1}^T \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\right) \left(\sum_{t=1}^T r(s_t, a_t)\right)\right]
\end{aligned}
$$

所以说 $$r(\tau) \cdot {\nabla}_{\theta} \log \pi_{\theta}(\tau)$$ 是 $${\nabla}_{\theta} \mathcal{J}(\theta)$$ 的无偏估计，可惜这玩意儿的 variance 很高。

## Reduce Variance

仔细观察这个式子，其实这玩意儿是 MLE 那个梯度对 $$r(s_t, a_t)$$ 加权了：

$$
\nabla_{\theta} \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(r)}\left[\sum_{t=0}^T \Psi_t \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\right]
$$

当前我们是有

$$
\Psi_t = \sum_{t'=0}^T r(s_t', a_t')
$$

## Don't Let the Past Distract You

一种简单的方法来减小 varience，是我们令

$$
\Psi_t = \sum_{t'=t}^T r(s_t', a_t')
$$

因为其实对于 $$a_t$$ 来说，他做啥对于 $$t$$ 之前的 reward 来说是不具有参考价值的。因此我们主要考虑后面的 reward。这玩意儿直觉上挺清楚的，但数学上想了半天才想明白为啥是对的。主要参考了[这篇文章](https://spinningup.openai.com/en/latest/spinningup/extra_pg_proof1.html) <d-cite key="SpinningUp2018"></d-cite>。

证明的不造为啥让我想起了 MLE。主要用到的就是一个叫做 EGLP lemma 的东西（其实好像用这个 lemma 需要积分和导数的可交换性，貌似 <d-cite key="hogg2013introduction"></d-cite> 里写挺详细的）：

$$
\mathbb{E}_{x \sim \mathbb{P}_{\theta}}\left[ \nabla_{\theta} \log \mathbb{P}_{\theta}(x)\right] = \int \mathbb{P}_{\theta}(x) \frac{\nabla_{\theta} \mathbb{P}_{\theta}(x)}{\mathbb{P}_{\theta}(x)} \mathrm{d}x = \nabla_{\theta} \int \mathbb{P}_{\theta}(x) \mathrm{d}x = 0
$$

其实跟 MLE 是一样的嘛：

$$
\mathbb{E}\left[ \nabla_{\theta} \log \mathcal{L}\left(x \mid \theta\right)\right] = 0
$$

其实我们是要证明嘟是：

$$
\mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}\left[\sum_{t=0}^T \sum_{t' < t} r\left(s_t', a_t'\right) {\nabla}_{\theta} \log \pi_{\theta}\left(a_t \mid s_t\right)\right] = 0
$$

也就是要证明当 $t'<t$ 这个时候：

$$
\mathbb{E}_{s_t, a_t, s_t', a_t' \sim \pi_{\theta}\left(s_t, a_t, s_t', a_t'\right)}\left[r\left(s_t', a_t'\right) \nabla_{\theta} \log \pi_{\theta}\left(a_t \mid s_t\right)\right] = 0
$$

那么中心思想其实就是咋来区分 $t'<t$ 捏，我们考虑 $t'<t$ 是先 reward，再选择：

$$
\mathbb{E}_{s_t', a_t' \sim \pi_{\theta}}\left[r\left(s_t', a_t'\right) \cdot \mathbb{E}_{s_t, a_t \sim \pi_{\theta}\left(s_t, a_t\right)}\left[\nabla_{\theta} \log \pi_{\theta}\left(a_t \mid s_t\right) \mid s_{t'}, a_{t'}\right]\right] = 0
$$

其实也就是当 $$r(s_t', a_t')$$ 不依赖于 $$s_t, a_t$$ 的时候，本身这个

$$
\mathbb{E}_{s_t, a_t \sim \pi_{\theta}\left(s_t, a_t\right)}\left[\nabla_{\theta} \log \pi_{\theta}\left(a_t \mid s_t\right) \mid s_{t'}, a_{t'}\right]
$$

他就是 $$0$$。

所以说最终结果是整个期望 $$0$$。
