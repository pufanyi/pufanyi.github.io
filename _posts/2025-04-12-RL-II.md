---
layout: distill
title: Reinforcement Learning Notes II - Policy Gradients & Actor-Critic Methods
categories: Notes
date: 2025-04-12
description: Policy Gradients, Actor-Critic Methods, Model-based RL
tags:
  - Reinforcement Learning
  - Artificial Intelligence
# pseudocode: true
giscus_comments: true
related_posts: false
authors:
  - name: Pu Fanyi
    url: "https://pufanyi.github.io"
    affiliations:
      name: NTU, Singapore
      url: "https://www.ntu.edu.sg/"
bibliography: 2025-04-12-RL-II.bib
---

## Direct policy differentiation

REINFORCE Algorithm <d-cite key="williams1992simple"></d-cite>:

$$
\begin{aligned}
\nabla_{\theta} \cal{J}(\theta) &= \int \nabla_{\theta} \pi_{\theta}(\tau) r(\tau) \mathrm{d}\tau \\
&= \mathbb{E}_{\tau \sim \pi_{\theta}(r)}\left[r(\tau) \cdot \nabla_{\theta} \log \pi_{\theta}(\tau)\right] \\
&= \mathbb{E}_{\tau \sim \pi_{\theta}(r)}\left[\left(\sum_{t=1}^T \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\right) \left(\sum_{t=1}^T r(s_t, a_t)\right)\right]
\end{aligned}
$$

所以说 $$r(\tau) \cdot \nabla_{\theta} \log \pi_{\theta}(\tau)$$ 是 $$\nabla_{\theta} \cal{J}(\theta)$$ 的无偏估计，可惜这玩意儿的 variance 很高。

## Reduce Variance

仔细观察这个式子，其实这玩意儿是 MLE 那个梯度对 $$r(s_t, a_t)$$ 加权了：

$$
\nabla_{\theta} \cal{J}(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(r)}\left[\sum_{t=0}^T \Psi_t \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\right]
$$

当前我们是有

$$
\Psi_t = \sum_{t'=0}^T r(s_t', a_t')
$$
