---
layout: distill
title: Deep Learning Notes I - Introduction
categories: Notes
date: 2025-04-01
description: Introduction, Neural Networks Construction
tags:
  - Machine Learning
  - Artificial Intelligence
  - Statistics
  - Probability Theory
# pseudocode: true
giscus_comments: true
related_posts: false
authors:
  - name: Pu Fanyi
    url: "https://pufanyi.github.io"
    affiliations:
      name: NTU, Singapore
      url: "https://www.ntu.edu.sg/"
---

> All modern machine learning algorithms are just nearest neighbors. It's only that the neural networks are telling you the space in which to compute the distance.

## Introduction

**Risk**: expected loss

$$
R(\theta) = \mathbb{E}\left[\mathcal{L}(\theta, X, Y)\right]
$$

**Empirical Risk**: average loss

$$
\hat{R}(\theta) = \frac{1}{N}\sum_{i=1}^N\mathcal{L}(\theta, x_i, y_i)
$$

Empirical Risk too high: underfitting; too low: overfitting.

### From MLE to MAP

其实就是用 Bayesian 的视角来看待 Regularization。

就是我们现在是要最大化

$$
p(\theta \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \theta)\cdot p(\theta)}{p(\mathcal{D})}
$$

这东西叫 MAP 也就是 Maximum A Posteriori。

取对数之后，我们就可以得到

$$
\begin{aligned}
\arg\max_{\theta}\log p(\theta \mid \mathcal{D}) &= \arg\max_{\theta}\left\{\log p(\mathcal{D} \mid \theta) + \log p(\theta) - \log p(\mathcal{D})\right\} \\
&= \arg\max_{\theta}\left\{\log p(\mathcal{D} \mid \theta) + \log p(\theta)\right\}
\end{aligned}
$$

而 $$\log p(\mathcal{D} \mid \theta)$$ 其实就是 loss 嘛。那 $$\log p(\theta)$$ 其实就是 regularization。

就比如说我们现在假设 $$\theta\sim\mathcal{N}(0, \sigma^2I)$$，那么我们就可以得到

$$
\log p(\theta) = -\frac{1}{2}\sum_{i=1}^n\left(\frac{\theta_i}{\sigma}\right)^2\log\left(\frac{1}{\sigma\sqrt{2\pi}}\right) = \lambda\Vert\theta\Vert^2
$$

其中 $$\lambda$$ 是一个常数。

---

## Single-layer Network: Regression

### Linear Regression

$$
\hat{y}(x, w) = \sum_{j=0}^{M-1}w_j\phi_j(x) = w^{\top}\phi(x)
$$

其中 $$\phi$$ 叫 bias functions。一般 $$\phi_0(x)=1$$。

$$
\mathcal{L}(w) = \frac{1}{2}\left(y - \Phi w\right)^{\top}\left(y - \Phi w\right) + \frac{\lambda}{2}w^{\top}w
$$

容易得到

$$
\hat{w} = (\Phi^{\top}\Phi + \lambda I)^{-1}\Phi^{\top}y
$$

这边简单复习一下上学期学的东西。假设没有 regularization，那么我们就有

$$
\hat{w} = (\Phi^{\top}\Phi)^{-1}\Phi^{\top}y
$$

然后我们令

$$
H = \Phi(\Phi^{\top}\Phi)^{-1}\Phi^{\top}
$$

那么 $$H$$ 和 $$I - H$$ 是 projection matrix，并且 $$Hy \perp (I - H)y$$：

$$
\begin{cases}
H\Phi = \Phi \\
H^2 = H \\
(I - H)^2 = I - H \\
H^{\top} = H \\
(I - H)^{\top} = I - H
\end{cases}
$$
