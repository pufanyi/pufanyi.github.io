---
layout: distill
title: Deep Learning Notes I - Introduction & Single-layer Networks
categories: Notes
date: 2025-04-01
description: SVD, MLE, Entropy
tags:
  - Machine Learning
  - Artificial Intelligence
  - Statistics
  - Probability Theory
# pseudocode: true
giscus_comments: true
related_posts: false
authors:
  - name: Pu Fanyi
    url: "https://pufanyi.github.io"
    affiliations:
      name: NTU, Singapore
      url: "https://www.ntu.edu.sg/"
---

> All modern machine learning algorithms are just nearest neighbors. It's only that the neural networks are telling you the space in which to compute the distance.

## Introduction

**Risk**: expected loss

$$
R(\theta) = \mathbb{E}\left[\mathcal{L}(\theta, X, Y)\right]
$$

**Empirical Risk**: average loss

$$
\hat{R}(\theta) = \frac{1}{N}\sum_{i=1}^N\mathcal{L}(\theta, x_i, y_i)
$$

Empirical Risk too high: underfitting; too low: overfitting.

### Regularization from Bayesian Perspective

这个东西其实从 Bayesian 的角度来看，比较好理解。

就是我们现在是要最大化

$$
p(\theta \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \theta)\cdot p(\theta)}{p(\mathcal{D})}
$$

所以说取对数之后，我们就可以得到

$$
\begin{aligned}
\arg\max_{\theta}\log p(\theta \mid \mathcal{D}) &= \arg\max_{\theta}\left\{\log p(\mathcal{D} \mid \theta) + \log p(\theta) - \log p(\mathcal{D})\right\} \\
&= \arg\max_{\theta}\left\{\log p(\mathcal{D} \mid \theta) + \log p(\theta)\right\}
\end{aligned}
$$

而 $$\log p(\mathcal{D} \mid \theta)$$ 其实就是 loss 嘛。那 $$\log p(\theta)$$ 其实就是 regularization。

就比如说我们现在假设 $$\theta\sim\mathcal{N}(0, \sigma^2I)$$，那么我们就可以得到

$$
\log p(\theta) = -\frac{1}{2}\sum_{i=1}^n\left(\frac{\theta_i}{\sigma}\right)^2\log\left(\frac{1}{\sigma\sqrt{2\pi}}\right) = \lambda\Vert\theta\Vert^2
$$

其中 $$\lambda$$ 是一个常数。
