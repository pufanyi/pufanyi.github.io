---
layout: distill
title: Reinforcement Learning Notes I - Introduction & Imitation Learning
categories: Notes
date: 2025-04-12
description: Introduction to Reinforcement Learning
tags:
  - Reinforcement Learning
  - Artificial Intelligence
# pseudocode: true
giscus_comments: true
related_posts: false
authors:
  - name: Pu Fanyi
    url: "https://pufanyi.github.io"
    affiliations:
      name: NTU, Singapore
      url: "https://www.ntu.edu.sg/"
---

## Introduction

我们把 $$\left<s_t, a_t\right>$$ 打包起来，其实他就构成了一个 Markov chain:

$$ p_\theta (\tau) = p_\theta (s_1) \prod_{t=1}^T \pi_\theta (a_t \mid s_t) \cdot \mathbb{P}(s_{t+1} \mid s_t, a_t) $$

对于 learning objective：

$$ \theta^* = \arg \max_\theta \cal{J}(\theta) = \arg \max_\theta \mathbb{E}_{\tau \sim \pi_\theta (\tau)}[r(\tau)] $$

我们定义我现在在 $$s_t$$，做了 action $$a_t$$，然后按照 $$\pi$$ 所得到的期望 reward 为

$$ \cal{Q}^\pi (s_t, a_t) = \sum_{t'=t}^T \mathbb{E}_{(s_t', a_t') \sim \pi}\left[r(s_t', a_t') \mid s_t, a_t\right] $$

然后我们定义 $$\cal{V}$$ 表示现在我在 $$s_t$$ 的时候遵循 $$\pi$$ 所得到的期望 reward：

$$ \cal{V}^\pi (s_t) = \sum_{t'=t}^T \mathbb{E}_{(s_t', a_t') \sim \pi}[r(s_t', a_t') \mid s_t] = \mathbb{E}_{a_t \sim \pi(a_t \mid s_t)}\left[\cal{Q}^\pi (s_t, a_t)\right] $$

## Types of Algorithms

- Policy Gradients: 跟往常一样，就是求导然后去做优化
- Value-based: 去直接计算最优解的 $$\cal{Q}$$ 和 $$\cal{V}$$，然后通过这个推出 $$\pi$$
- Actor-critic: 通过计算当前 $$\hat{\theta}$$ 的 $$\hat{\cal{Q}}$$ 和 $$\hat{\cal{V}}$$，然后通过这个去进行优化，得到最终的 $$\pi$$
- Model-based RL: 使用模型来代表各种参数，然后进行优化
