---
layout: post
title: Stochastic Processes and Reinforcement Learning
categories: Notes
date: 2024-10-16
description: Notes for NTU MH3512 Stochastic Processes
tags:
  - Probability Theory
  - Reinforcement Learning
giscus_comments: true
related_posts: false
toc:
  sidebar: left
---

看我能坚持多久。。。

## Markov Chains

[教材捏](https://link.springer.com/book/10.1007/978-981-13-0659-4)

[划了重点的教材捏](https://personal.ntu.edu.sg/ariel.neufeld/script_MH3512_Marked.pdf)（prof 划的，虽然我感觉他把整本书划了一遍。。。）

### Gambling Problems

有 $$S$$ 块钱，$$A$$ 有 $$K$$ 块钱，$$B$$ 有 $$S-K$$ 块。每次有 $$p$$ 的概率 $$A$$ 从 $$B$$ 拿走一块，$$q=1-p$$ 的概率 $$B$$ 从 $$A$$ 拿走一块。谁拿到 $$S$$ 块钱就算赢。

$$
X_{n+1}=\begin{cases}
  X_n+1 & p \\
  X_n-1 & q
\end{cases}
$$

$$f_S(k)$$ 表示 $$A$$ 赢的概率。很显然我们有：

$$
f_S(k)=pf_{S}(k+1)+qf_{S}(k-1)
$$

不难解出

$$
f_S(k)=\frac{(p/q)^{S-k}-1}{(p/q)^S-1}
$$

$$T_{0, S}$$ 表示一个游戏啥时候结束，$$h_S(k)=\mathbb{E}\left[T_{0, S}\mid X_0=K\right]$$。

很显然

$$
h_S(k)=1+ph_S(k+1)+qh_S(k-1)
$$

这个方程的特解比较难搞，注意到 $$p+q=1$$，我们可以改写成差分方程：

$$
-1=p\left(h_S(k+1)-h_S(k)\right)+q\left(h_S(k)-h_S(k-1)\right)
$$

观察出方程在 $$p\neq q$$ 的时候一个特解为 $$\frac{k}{q-p}$$。

不难解出齐次方程 $$h_S(k)=ph_S(k+1)+qh_S(k-1)$$ 的解，最终可以得到在 $$p\neq q$$ 时

$$
h_s(k)=\frac{1}{q-p}\left(k-S\cdot\frac{1-(q/p)^k}{1-(q/p)^S}\right)
$$

当 $$p=q=\frac{1}{2}$$ 时，我们有特解 $$-k^2$$。所以说 $$p=q$$ 时

$$
h_S(k)=k(S-k)
$$

### Random Walks

Bernoulli Random Walks: $$X_n$$ 相互独立，其中

$$
\begin{cases}
\mathbb{P}(X_k=+1)=p \\
\mathbb{P}(X_k=-1)=q
\end{cases}
$$

$$p+q=1$$，然后我们定义

$$
S_n=\sum_{i=1}^nX_i
$$

很显然 $$\mathbb{P}(S_{2n}=2k+1)=\mathbb{P}(S_{2n+1}=2k)=0$$，然后

$$
\begin{cases}
\mathbb{P}(S_{2n}=2k)=\binom{2n}{n+k}p^{n+k}q^{n-k} \\
\mathbb{P}(S_{2n+1}=2k+1)=\binom{2n+1}{n+k+1}p^{n+k+1}q^{n-k}
\end{cases}
$$

难度在我们怎么计算他啥时候回到 $$0$$。我们令

$$
T_0^r=\inf\{n\geq 1: S_n=0\}
$$

表示第一次回到 $$0$$ 的时间。

然后我们设

$$
g(n)=\mathbb{P}\left(T_0^r=n\mid S_0=0\right)
$$

也就是在第 $$n$$ 步第一次回到 $$0$$ 的概率。那很显然 $$g(2k+1)=0$$。

接下来是一个比较神奇的套路，就是我们假设 $$h(n)$$ 为第 $$n$$ 步回到 $$0$$ 的概率，那我们可以得到一个卷积式子：

$$
h(n)=\sum_{k=0}^{n-2}g(n-k)h(k)
$$

也就是先走 $$n-k$$ 步第一次回到 $$0$$，然后继续走 $$k$$ 步回到 $$0$$。所以我们现在只要解决 $$h$$ 就可以了。

我们考虑 $$h(n)$$ 的生成函数

$$
H(s)=\mathbb{E}\left[s^{T_0^r}\cdot\mathbb{1}_{T_0^r<\infty}\right]=\sum_{n=0}^\infty h(n)s^n
$$

大概推推：

$$
\begin{aligned}
H(s)&=\sum_{n=0}^\infty h(n)s^n\\
&=\sum_{n=0}^\infty\binom{2n}{n}p^nq^ns^{2n}\\
&=\sum_{n=0}^\infty\frac{(2n)!}{(n!)^2}\left(pqs^2\right)^n\\
&=\sum_{n=0}^\infty\frac{\left(\prod_{i=1}^n2i\right)\left(\prod_{i=1}^n(2i-1)\right)}{(n!)^2}\left(pqs^2\right)^n\\
&=\sum_{n=0}^\infty\frac{\prod_{i=1}^n(2i-1)}{n!}\left(2pqs^2\right)^n\\
&=\sum_{n=0}^\infty\frac{(-2)^n\prod_{i=1}^n\left(-\frac{1}{2}-(i-1)\right)}{n!}\left(2pqs^2\right)^n\\
&=\sum_{n=0}^\infty\frac{\left(-\frac{1}{2}\right)^{\underline{i}}}{n!}\left(-4pqs^2\right)^n\\
&=\left(1-4pqs^2\right)^{-\frac{1}{2}}
\end{aligned}
$$

又考虑到：

$$
\begin{aligned}
G(s)H(s)&=\left(\sum_{i=1}^\infty s^ig(i)\right)\left(\sum_{j=0}^\infty s^jh(j)\right)\\
&=\sum_{i=2}^\infty\sum_{j=0}^\infty s^{i+j}g(i)h(j)\\
&=\sum_{k=2}^\infty s^k\sum_{i=2}^\infty g(i)h(k-i)\\
&=\sum_{k=2}^\infty s^k h(k)\\
&=-1 + \sum_{k=0}^\infty s^kh(k)\\
&=-1 + H(s)
\end{aligned}
$$

于是乎

$$
G(s)=1-\frac{1}{H(s)}=1-\sqrt{1-4pqs^2}
$$

所以

$$
\begin{aligned}
\mathbb{P}\left(T_0^r=\infty\mid S_0=0\right)&=1-\mathbb{P}\left(T_0^r<\infty\mid S_0=0\right)\\
&=1-G(1)=\sqrt{1-4pq}\\
&=\sqrt{4p^2-4p+1}\\
&=\lvert 2p-1\rvert\\
&=\lvert p-q\rvert
\end{aligned}
$$

而根据前面的 Gambling Problems，其实我们已经解出当 $$k\neq 0$$ 时：

$$
\mathbb{P}\left(T_0^r=\infty\mid S_0=k\right)=1-\lim_{S\to\infty}f_S(k)=\max\left\{0, 1-\left(\frac{q}{p}\right)^k\right\}
$$

然后我们来计算 $$\mathbb{E}\left[T_0^r\mid S_0=0\right]$$ 的时候会发现如果 $$\mathbb{P}\left(T_0^r\mid S_0=0\right)>0$$ 的时候这个期望肯定是 $$\infty$$。而 $$\mathbb{P}\left(T_0^r\mid S_0=0\right)$$ 只有在 $$p=q=\frac{1}{2}$$ 的时候才会为 $$0$$。然鹅当 $$p=q=\frac{1}{2}$$ 时：

$$
\mathbb{E}\left[T_0^r\mid S_0=0\right]=\mathbb{E}\left[T_0^r \cdot\mathbb{1}_{\left\{T_0^r<\infty\right\}}\mid S_0=0\right]=\left.\frac{\partial G}{\partial s}\right|_{s=1}=\infty
$$

所以我们不管 $$p, q$$ 都有：

$$
\mathbb{E}\left[T_0^r\mid S_0=0\right]=\infty
$$

关于 first time 的 distribution 的话，我们不难算出

$$
\mathbb{P}\left(T_0^r=2k\mid S_0=0\right)=\frac{1}{(2k)!}\left.\frac{\partial^{2k}G}{\partial s^{2k}}\right|_{s=0}=\frac{1}{2k-1}\binom{2k}{k}(pq)^k
$$

### Discrete-Time Markov Chains

Markov property 指的是下一步的 distribution 只跟当前有关：

$$
\mathbb{P}\left(Z_{n+1}=j\mid Z_n=i_n, \cdots, Z_0=i_0\right)=\mathbb{P}\left(Z_{n+1}=j\mid Z_n=i_n\right)
$$

$$\pi$$ 和 $$\eta$$ 是行向量，转移方程

$$
\eta=\pi P
$$
