---
layout: distill
title: NTU MH3500 Statistics
categories: Notes
date: 2024-01-26
description: Notes for NTU MH3500 Statistics.
tags:
- Statistics
- Probability Theory
giscus_comments: true
related_posts: false

authors:
  - name: Pu Fanyi
    url: "https://pufanyi.github.io"
    affiliations:
      name: NTU, Singapore
      url: "https://www.ntu.edu.sg/"

toc:
  - name: 什么是 statistics
---

这学期学了门叫做 Statistics 的课，然后发现有点学不明白，所以记点笔记。

***

## 什么是 statistics

首先是 population, property, population distribution, random sample 这些名词，应该不用咋写。

Static inference 是指用 sample 来推断整个 population 的性质。

就是假设我们随机抽取了 $$n$$ 个 sample $$x_1, x_2, \dots, x_n$$，我们称 $$x_1, x_2, \dots, x_n$$ are the realizations of i.i.d. random variables $$X_1, X_2, \dots, X_n$$。也称 $$x_1, x_2, \dots, x_n$$ are observations of $$X_1, X_2, \dots, X_n$$。

而 statistic 事实上指的是 a real valued function, $$T(X_1, X_2, \dots, X_n)$$。需要注意的是，这个 function 之和 $$X_1, X_2, \dots, X_n$$ 有关，而不是 $$x_1, x_2, \dots, x_n$$。

然后 the distribution of a statistic is called a sampling distribution。也就是说，sampling distribution 是很多变量的 distribution，而 population 只是一个变量的 distribution。

Sample mean 是 $$\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i$$，sample variance 是 $$S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2$$。

{% details 为什么是 $$\frac{1}{n-1}$$ %}

感性理解是，你这 $$n$$ 个东西的平均值，事实上是根据这 $$n$$ 个东西的“趋向”有一定偏移的。就比如说我现在有一个 $$N(0, 1)$$，我去两个 sample，假设拿到了 $$x_1=-1, x_2=2$$，这个时候他的 $\overline{X}$ 其实不是 $$0$$，而是 $$\frac{1}{2}$$，也就是被往右边拉过去了一点的。这时候如果我们用 $$\frac{1}{n}$$ 计算，那事实上算出来的值应该是偏小的（因为这里的 $$\overline{X}$$ 根据样本的抽取结果“调整”了一下）。而 $$\frac{1}{n-1}$$ 正好抵消了这一点。

下面是严格的数学证明。

我们有 $$\mathbb{E}(S^2)=\sigma^2$$。

我们考虑 $$\mathbb{E}(S^2)=\frac{1}{n-1}\sum_{i-1}^n\mathbb{E}\left[(X_i-\overline{X})^2\right]$$，于是我们考虑计算 $$\mathbb{E}\left[(X_i-\overline{X})^2\right]$$：

$$
\begin{aligned}
\mathbb{E}\left[\left(X_i-\overline{X}\right)^2\right] &= \mathbb{E}\left[\left(\left(X_i-\mu\right)-\left(\overline{X}-\mu\right)\right)^2\right] \\
&= \mathbb{E}\left[(X_i-\mu)^2+(\overline{X}-\mu)^2-2(X_i-\mu)(\overline{X}-\mu)\right]\\
&=\mathbb{E}\left[(X_i-\mu)^2\right]+\mathbb{E}\left[(\overline{X}-\mu)^2\right]-2\mathbb{E}\left[(X_i-\mu)(\overline{X}-\mu)\right]\\
&=\mathrm{Var}(X_i)+\mathrm{Var}(\overline{X})-2\cdot\mathrm{Cov}(X_i,\overline{X})\\
&=\sigma^2+\mathrm{Var}\left(\frac{1}{n}\sum_{j=1}^nX_j\right)-2\cdot\mathrm{Cov}\left(X_i,\frac{1}{n}\sum_{j-1}^nX_j\right)\\
&=\sigma^2+\frac{1}{n^2}\sum_{j=1}^n\mathrm{Var}(X_j)-\frac{2}{n}\mathrm{Cov}(X_i,X_i)\\
&=\sigma^2+\frac{1}{n}\sigma^2-\frac{2}{n}\sigma^2\\
&=\frac{n-1}{n}\sigma^2
\end{aligned}
$$

于是我们有 $$\mathbb{E}(S^2)=\frac{1}{n-1}\sum_{i-1}^n\mathbb{E}\left[(X_i-\overline{X})^2\right]=\frac{1}{n-1}\sum_{i-1}^n\frac{n-1}{n}\sigma^2=\sigma^2$$。

{% enddetails %}
