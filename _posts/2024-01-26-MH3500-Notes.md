---
layout: distill
title: NTU MH3500 Statistics
categories: Notes
date: 2024-01-26
description: Notes for NTU MH3500 Statistics.
tags:
- Statistics
- Probability Theory
giscus_comments: true
related_posts: false

authors:
  - name: Pu Fanyi
    url: "https://pufanyi.github.io"
    affiliations:
      name: NTU, Singapore
      url: "https://www.ntu.edu.sg/"

toc:
  - name: 什么是 statistics
  - name: From Normal Distribution

bibliography: 2024-01-26-MH3500-Notes.bib
---

这学期学了门叫做 Statistics 的课，然后发现有点学不明白，所以记点笔记。

除了上课听到的，应该还会记一些书里看到的和自己想到的。

教材用的是这三本：<d-cite key="hogg2019introduction"></d-cite><d-cite key="rice2006mathematical"></d-cite><d-cite key="casella2021statistical"></d-cite>

***

## 什么是 statistics

首先是 population, property, population distribution, random sample 这些名词，应该不用咋写。

Static inference 是指用 sample 来推断整个 population 的性质。

就是假设我们随机抽取了 $$n$$ 个 sample $$x_1, x_2, \dots, x_n$$，我们称 $$x_1, x_2, \dots, x_n$$ are the realizations of i.i.d. random variables $$X_1, X_2, \dots, X_n$$。也称 $$x_1, x_2, \dots, x_n$$ are observations of $$X_1, X_2, \dots, X_n$$。

而 statistic 事实上指的是 a real valued function, $$T(X_1, X_2, \dots, X_n)$$。需要注意的是，这个 function 之和 $$X_1, X_2, \dots, X_n$$ 有关，而不是 $$x_1, x_2, \dots, x_n$$。

然后 the distribution of a statistic is called a sampling distribution。也就是说，sampling distribution 是很多变量的 distribution，而 population 只是一个变量的 distribution。

Sample mean 是 $$\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i$$，sample variance 是 $$S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2$$。

{% details 为什么是 $$\frac{1}{n-1}$$ %}

感性理解是，你这 $$n$$ 个东西的平均值，事实上是根据这 $$n$$ 个东西的“趋向”有一定偏移的。就比如说我现在有一个 $$N(0, 1)$$，我取两个 sample，假设拿到了 $$x_1=-1, x_2=2$$，这个时候他的 $\overline{X}$ 其实不是 $$0$$，而是 $$\frac{1}{2}$$，也就是被往右边拉过去了一点的。这时候如果我们用 $$\frac{1}{n}$$ 计算，那事实上算出来的值应该是偏小的（因为这里的 $$\overline{X}$$ 根据样本的抽取结果“调整”了一下）。而 $$\frac{1}{n-1}$$ 正好抵消了这一点。

下面是严格的数学证明。

我们有 $$\mathbb{E}(S^2)=\sigma^2$$。

我们考虑 $$\mathbb{E}(S^2)=\frac{1}{n-1}\sum_{i-1}^n\mathbb{E}\left[(X_i-\overline{X})^2\right]$$，于是我们考虑计算 $$\mathbb{E}\left[(X_i-\overline{X})^2\right]$$：

$$
\begin{aligned}
\mathbb{E}\left[\left(X_i-\overline{X}\right)^2\right] &= \mathbb{E}\left[\left((X_i-\mu)-(\overline{X}-\mu)\right)^2\right] \\
&= \mathbb{E}\left[(X_i-\mu)^2+(\overline{X}-\mu)^2-2(X_i-\mu)(\overline{X}-\mu)\right]\\
&=\mathbb{E}\left[(X_i-\mu)^2\right]+\mathbb{E}\left[(\overline{X}-\mu)^2\right]-2\mathbb{E}\left[(X_i-\mu)(\overline{X}-\mu)\right]\\
&=\mathrm{Var}(X_i)+\mathrm{Var}(\overline{X})-2\cdot\mathrm{Cov}(X_i,\overline{X})\\
&=\sigma^2+\mathrm{Var}\left(\frac{1}{n}\sum_{j=1}^nX_j\right)-2\cdot\mathrm{Cov}\left(X_i,\frac{1}{n}\sum_{j-1}^nX_j\right)\\
&=\sigma^2+\frac{1}{n^2}\sum_{j=1}^n\mathrm{Var}(X_j)-\frac{2}{n}\mathrm{Cov}(X_i,X_i)\\
&=\sigma^2+\frac{1}{n}\sigma^2-\frac{2}{n}\sigma^2\\
&=\frac{n-1}{n}\sigma^2
\end{aligned}
$$

于是我们有 $$\mathbb{E}(S^2)=\frac{1}{n-1}\sum_{i-1}^n\mathbb{E}\left[(X_i-\overline{X})^2\right]=\frac{1}{n-1}\sum_{i-1}^n\frac{n-1}{n}\sigma^2=\sigma^2$$。

{% enddetails %}

有一个好玩的性质是 如果 $$X_i\sim N(\mu, \sigma^2)$$ 的话，$$\overline{X}$$ 和 $$S^2$$ 是独立的。上课老师讲了中方法但我有点没搞懂，Rice 书里有一个证法，感觉清楚一些。

{% details 上课老师讲的证法 %}

其实我至今没搞懂他到底是咋搞的。因为他用了一个结论，就是两个协方差为 $$0$$ 的 normal distribution 是相互独立的。不是很清楚不用 MGF 这玩意儿还能咋证（用 MGF 的话，那还不如 Rice 的证法呢）。

当然如果这玩意儿是成立的，那就好办了。就是毕竟我们是知道 $$\overline{X}\sim N\left(\mu, \frac{\sigma^2}{n}\right)$$，然后 $$X_i-\overline{X}\sim N\left(0, \sigma^2\right)$$ 的。那我们只要搞出他俩的 covariance 就行了。

那我们算算：

$$
\begin{aligned}
\mathrm{Cov}\left(\overline{X}, X_i-\overline{X}\right)&=\mathrm{Cov}\left(\overline{X},X_i\right)-\mathrm{Cov}\left(\overline{X},\overline{X}\right)\\
&=\mathrm{Cov}\left(\frac{1}{n}\sum_{j=1}^nX_j,X_i\right)-\frac{\sigma^2}{n}\\
&=\frac{1}{n}\mathrm{Cov}\left(X_i, X_i\right)-\frac{\sigma^2}{n}\\
&=\frac{\sigma^2}{n}-\frac{\sigma^2}{n}\\
&=0
\end{aligned}
$$

然后他们就独立了。

{% enddetails %}

{% details Rice 书里的证法 %}

考虑到 $$S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2$$，我们其实只要证明 $$\overline{X}$$ 和 $$X_i-\overline{X}$$ 是独立的就可以了。

首先我们需要知道一件事情，就是假设有一个 $$r$$，使得：

$$
M_{\boldsymbol{X}}(\boldsymbol{t})=M_{\boldsymbol{X}}(t_1, t_2, \dots, t_r, 0, \dots, 0)\cdot M_{\boldsymbol{X}}(0, \dots, 0, t_{r+1}, t_{r+2}, t_n)
$$

那么 $$(t_1, t_2, \cdots, t_r)$$ 和 $$(t_{r+1}, t_{r+2}, \dots, t_n)$$ 是独立的。

这个证明是假设我们随机两次，第一次是 $$\boldsymbol{X}$$，第二次是 $$\boldsymbol{\widetilde{X}}$$。我们考虑：

$$
Y=(X_1, \dots, X_r, \widetilde{X}_{r+1}, \dots, \widetilde{X}_{n})
$$

由于 $$\boldsymbol{X}$$ 和 $$\boldsymbol{\widetilde{X}}$$ 是独立的，所以我们有：

$$
M_{\boldsymbol{Y}}(\boldsymbol{t})=M_{\boldsymbol{X}}(t_1, t_2, \dots, t_r, 0, \dots, 0)\cdot M_{\boldsymbol{\widetilde{X}}}(0, \dots, 0, t_{r+1}, t_{r+2}, t_n)
$$

而又因为 $$\boldsymbol{X}$$ 和 $$\boldsymbol{\widetilde{X}}$$ 其实是一样的，$$M_{\boldsymbol{X}}(\boldsymbol{t})=M_{\boldsymbol{\widetilde{X}}}(\boldsymbol{t})$$，也就是说：

$$
M_{\boldsymbol{Y}}(\boldsymbol{t})=M_{\boldsymbol{X}}(t_1, t_2, \dots, t_r, 0, \dots, 0)\cdot M_{\boldsymbol{X}}(0, \dots, 0, t_{r+1}, t_{r+2}, t_n)
$$

也就是 $$M_{\boldsymbol{X}}=M_{\boldsymbol{Y}}$$，也就是对于任意集合 $$A_1, A_2, \dots, A_n$$，我们有：
$$
\begin{aligned}
&\mathbb{P}\left\{(X_1\in A_1)\land (X_2\in A_2)\land\dots\land (X_n\in A_n)\right\}\\
=\ &\mathbb{P}\left\{(X_1\in A_1)\land \dots\land(X_r\in A_r)\land\left(\widetilde{X}_{r+1}\in A_{r+1}\right)\land\dots\land\left(\widetilde{X}_{n}\in A_n\right)\right\}\\
=\ &\mathbb{P}\left\{(X_1\in A_1)\land \dots\land(X_r\in A_r)\right\}\cdot\mathbb{P}\left\{\left(\widetilde{X}_{r+1}\in A_{r+1}\right)\land\dots\land\left(\widetilde{X}_{n}\in A_n\right)\right\}
\end{aligned}
$$

所以我们令 $$\boldsymbol{\widehat{X}}=\left(\overline X, X_1-\overline{X}, \dots, X_n-\overline{X}\right)$$，$$\boldsymbol{\widehat{t}}=(s, t_1, t_2,\dots, t_n)$$。

考虑到 $$X_1-\overline{X}, X_2-\overline{X},\dots,X_n-\overline{X}$$ 这些玩意儿的独立性是显然的，所以其实我们要证明的就是：

$$
M_{\boldsymbol{\widehat{X}}}\left(\boldsymbol{\widehat{t}}\right)=M_{\overline{X}}(s, 0, \dots, 0)\cdot M_{\boldsymbol{X}-\overline{X}}(0, t_1, \dots, t_n)
$$

然后我们接下来想做的是能不能把 $$M_{\widehat{\boldsymbol{X}}}(\widehat{\boldsymbol{t}})$$ 和 $$M_{\boldsymbol{X}}(\boldsymbol{a})$$ 联系起来。

为啥要这样转化呢？因为虽然我们现在不知道 $$\widehat{\boldsymbol{X}}$$ 的独立性，但是 $$\boldsymbol{X}$$ 的精神状态咋样我们是知道的。这样我们就能通过 $$\boldsymbol{X}$$ 把整个 MGF 给拆解开来，从而计算这个 MGF 到底是啥。

我们是这样做的：

$$
\begin{aligned}
M_{\boldsymbol{\widehat{X}}}\left(\boldsymbol{\widehat{t}}\right)&=\mathbb{E}\left[\exp\left({\widehat{\boldsymbol{X}}^\mathrm{T}\widehat{\boldsymbol{t}}}\right)\right]\\
&=\mathbb{E}\left[\exp\left({s\overline{X}+\sum_{i=1}^nt_i\left(X_i-\overline{X}\right)}\right)\right]\\
&=\mathbb{E}\left[\exp\left(\frac{s}{n}\sum_{i=1}^nX_i+\sum_{i=1}^nt_iX_i-\left(\sum_{i=1}^nt_i\right)\cdot\left(\frac{1}{n}\cdot\sum_{i=1}^nX_i\right)\right)\right]\\
&=\mathbb{E}\left[\exp\left(\sum_{i=1}^n\left(\frac{s}{n}+t_i-\overline{t}\right)X_i\right)\right]
\end{aligned}
$$

于是乎我们就可以令 $$a_i=\frac{s}{n}+t_i-\overline{t}$$，于是我们就有：

$$
M_{\boldsymbol{\widehat{X}}}\left(\boldsymbol{\widehat{t}}\right)=M_{\boldsymbol{X}}\left(\boldsymbol{a}\right)
$$

然后我们就能根据 $$\boldsymbol{X}$$ 的独立性把 $$M_{\boldsymbol{X}}\left(\boldsymbol{a}\right)$$ 拆开来了：

$$
\begin{aligned}
M_{\boldsymbol{\widehat{X}}}\left(\boldsymbol{\widehat{t}}\right)&=M_{\boldsymbol{X}}\left(\boldsymbol{a}\right)\\
&=\prod_{i=1}^nM_{X_i}\left(a_i\right)\\
&=\prod_{i = 1}^n\exp\left(\mu a_i+\frac{1}{2}\sigma^2a_i^2\right)\\
&=\exp\left(\mu\sum_{i=1}^na_i+\frac{\sigma^2}{2}\sum_{i=1}^na_i^2\right)
\end{aligned}
$$

考虑到：

$$
\begin{aligned}
\sum_{i=1}^na_i&=\sum_{i=1}^n\left(\frac{s}{n}+t_i-\overline{t}\right)\\
&=s+\sum_{i=1}^nt_i-n\overline{t}\\
&=s\\
\sum_{i=1}^na_i^2&=\sum_{i=1}^n\left(\frac{s}{n}+t_i-\overline{t}\right)^2\\
&=\sum_{i=1}^n\left(\left(\frac{s}{n}\right)^2+2\cdot\frac{s}{n}\left(t_i-\overline t\right)+\left(t_i-\overline{t}\right)^2\right)\\
&=\frac{s^2}{n}+\sum_{i=1}^n\left(t_i-\overline{t}\right)^2
\end{aligned}
$$

于是我们就有：

$$
\begin{aligned}
M_{\boldsymbol{\widehat{X}}}\left(\boldsymbol{\widehat{t}}\right)&=\exp\left(\mu\sum_{i=1}^na_i+\frac{\sigma^2}{2}\sum_{i=1}^na_i^2\right)\\
&=\exp\left(\mu s+\frac{\sigma^2}{2}\left(\frac{s^2}{n}+\sum_{i=1}^n\left(t_i-\overline{t}\right)^2\right)\right)\\
&=\exp\left(\mu s+\frac{\sigma^2}{2}\frac{s^2}{n}+\frac{\sigma^2}{2}\sum_{i=1}^n\left(t_i-\overline{t}\right)^2\right)\\
&=\exp\left(\mu s+\frac{1}{2}\cdot\frac{\sigma^2}{n}s^2\right)\cdot\prod_{i=1}^n\exp\left(\frac{\sigma^2}{2}\left(t_i-\overline{t}\right)^2\right)
\end{aligned}
$$

酱紫我们就已经把 $$M_{\boldsymbol{\widehat{X}}}\left(\boldsymbol{\widehat{t}}\right)$$ 给算出来了。简单算算就能发现

$$
M_{\boldsymbol{\widehat{X}}}\left(\boldsymbol{\widehat{t}}\right)=M_{\overline{X}}(s, 0, \dots, 0)\cdot M_{\boldsymbol{X}-\overline{X}}(0, t_1, \dots, t_n)
$$

这玩意儿是成立的。于是我们就证完了。

{% enddetails %}

***

## From Normal Distribution

第一个需要讨论的是 Chi Square Distribution。就是我现在有 $$n$$ 个 i.i.d. $$N(0, 1)$$ 的 random variables，$$X_1, X_2, \dots, X_n$$，我们定义 $$Z=\sum_{i=1}^nX_i^2$$ 遵从 $$\chi^2_n$$ distribution。

{% details Gamma Distribution %}

写到这里的时候发现一些 Gamma Distribution 的细节已经给忘光光了。

首先需要复习的是 Gamma Distribution 的引入。他主要是想要推广 exponential distribution。就是我们想知道第 $n$ 次发生事情的时候，我们需要等多久。

我们是通过 Poisson Distribution 推下来的。就是我们考虑这个新 distribution 的 CDF $$F(t)$$，其实表示的是在 $$1\sim t$$ 这段时间里要发生大于等于 $$n$$ 次。于是我们有：

$$
F(t)=\sum_{k=n}^\infty\frac{(\lambda t)^ke^{-\lambda t}}{k!}
$$

那么我们就能轻松得到他的 PDF：

$$
f(t)=\frac{\mathrm{d}F(t)}{\mathrm{d}t}
$$

{% enddetails %}
