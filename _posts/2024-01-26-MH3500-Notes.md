---
layout: distill
title: NTU MH3500 Statistics
categories: Notes
date: 2024-01-26
description: Notes for NTU MH3500 Statistics.
tags:
- Statistics
- Probability Theory
giscus_comments: true
related_posts: false

authors:
  - name: Pu Fanyi
    url: "https://pufanyi.github.io"
    affiliations:
      name: NTU, Singapore
      url: "https://www.ntu.edu.sg/"

toc:
  - name: 什么是 statistics
  - name: From Normal Distribution

bibliography: 2024-01-26-MH3500-Notes.bib
---

这学期学了门叫做 Statistics 的课，然后发现有点学不明白，所以记点笔记。

除了上课听到的，应该还会记一些书里看到的和自己想到的。

教材用的是这三本：<d-cite key="hogg2019introduction"></d-cite><d-cite key="rice2006mathematical"></d-cite><d-cite key="casella2021statistical"></d-cite>

***

## 什么是 statistics

首先是 population, property, population distribution, random sample 这些名词，应该不用咋写。

Static inference 是指用 sample 来推断整个 population 的性质。

就是假设我们随机抽取了 $$n$$ 个 sample $$x_1, x_2, \dots, x_n$$，我们称 $$x_1, x_2, \dots, x_n$$ are the realizations of i.i.d. random variables $$X_1, X_2, \dots, X_n$$。也称 $$x_1, x_2, \dots, x_n$$ are observations of $$X_1, X_2, \dots, X_n$$。

而 statistic 事实上指的是 a real valued function, $$T(X_1, X_2, \dots, X_n)$$。需要注意的是，这个 function 之和 $$X_1, X_2, \dots, X_n$$ 有关，而不是 $$x_1, x_2, \dots, x_n$$。

然后 the distribution of a statistic is called a sampling distribution。也就是说，sampling distribution 是很多变量的 distribution，而 population 只是一个变量的 distribution。

Sample mean 是 $$\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i$$，sample variance 是 $$S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2$$。

{% details 为什么是 $$\frac{1}{n-1}$$ %}

感性理解是，你这 $$n$$ 个东西的平均值，事实上是根据这 $$n$$ 个东西的“趋向”有一定偏移的。就比如说我现在有一个 $$N(0, 1)$$，我取两个 sample，假设拿到了 $$x_1=-1, x_2=2$$，这个时候他的 $\overline{X}$ 其实不是 $$0$$，而是 $$\frac{1}{2}$$，也就是被往右边拉过去了一点的。这时候如果我们用 $$\frac{1}{n}$$ 计算，那事实上算出来的值应该是偏小的（因为这里的 $$\overline{X}$$ 根据样本的抽取结果“调整”了一下）。而 $$\frac{1}{n-1}$$ 正好抵消了这一点。

下面是严格的数学证明。

我们有 $$\mathbb{E}(S^2)=\sigma^2$$。

我们考虑 $$\mathbb{E}(S^2)=\frac{1}{n-1}\sum_{i-1}^n\mathbb{E}\left[(X_i-\overline{X})^2\right]$$，于是我们考虑计算 $$\mathbb{E}\left[(X_i-\overline{X})^2\right]$$：

$$
\begin{aligned}
\mathbb{E}\left[\left(X_i-\overline{X}\right)^2\right] &= \mathbb{E}\left[\left((X_i-\mu)-(\overline{X}-\mu)\right)^2\right] \\
&= \mathbb{E}\left[(X_i-\mu)^2+(\overline{X}-\mu)^2-2(X_i-\mu)(\overline{X}-\mu)\right]\\
&=\mathbb{E}\left[(X_i-\mu)^2\right]+\mathbb{E}\left[(\overline{X}-\mu)^2\right]-2\mathbb{E}\left[(X_i-\mu)(\overline{X}-\mu)\right]\\
&=\mathrm{Var}(X_i)+\mathrm{Var}(\overline{X})-2\cdot\mathrm{Cov}(X_i,\overline{X})\\
&=\sigma^2+\mathrm{Var}\left(\frac{1}{n}\sum_{j=1}^nX_j\right)-2\cdot\mathrm{Cov}\left(X_i,\frac{1}{n}\sum_{j-1}^nX_j\right)\\
&=\sigma^2+\frac{1}{n^2}\sum_{j=1}^n\mathrm{Var}(X_j)-\frac{2}{n}\mathrm{Cov}(X_i,X_i)\\
&=\sigma^2+\frac{1}{n}\sigma^2-\frac{2}{n}\sigma^2\\
&=\frac{n-1}{n}\sigma^2
\end{aligned}
$$

于是我们有 $$\mathbb{E}(S^2)=\frac{1}{n-1}\sum_{i-1}^n\mathbb{E}\left[(X_i-\overline{X})^2\right]=\frac{1}{n-1}\sum_{i-1}^n\frac{n-1}{n}\sigma^2=\sigma^2$$。

{% enddetails %}

有一个好玩的性质是 $$\overline{X}$$ 和 $$S^2$$ 是独立的，其实是证明比较好玩。

{% details 证明 $$\overline{X}$$ 和 $$S^2$$ 是独立的 %}

首先我们需要知道一件事情，就是假设有一个 $$r$$，使得：

$$
M_{\boldsymbol{X}}(\boldsymbol{t})=M_{\boldsymbol{X}}(t_1, t_2, \dots, t_r, 0, \dots, 0)\cdot M_{\boldsymbol{X}}(0, \dots, 0, t_{r+1}, t_{r+2}, t_n)
$$

那么 $$(t_1, t_2, \cdots, t_r)$$ 和 $$(t_{r+1}, t_{r+2}, \dots, t_n)$$ 是独立的。

这个证明是假设我们随机两次，第一次是 $$\boldsymbol{X}$$，第二次是 $$\boldsymbol{\widetilde{X}}$$。我们考虑：

$$
Y=(X_1, \dots, X_r, \widetilde{X}_{r+1}, \dots, \widetilde{X}_{n})
$$

由于 $$\boldsymbol{X}$$ 和 $$\boldsymbol{\widetilde{X}}$$ 是独立的，所以我们有：

$$
M_{\boldsymbol{Y}}(\boldsymbol{t})=M_{\boldsymbol{X}}(t_1, t_2, \dots, t_r, 0, \dots, 0)\cdot M_{\boldsymbol{\widetilde{X}}}(0, \dots, 0, t_{r+1}, t_{r+2}, t_n)
$$

而又因为 $$\boldsymbol{X}$$ 和 $$\boldsymbol{\widetilde{X}}$$ 其实是一样的，$$M_{\boldsymbol{X}}(\boldsymbol{t})=M_{\boldsymbol{\widetilde{X}}}(\boldsymbol{t})$$，也就是说：

$$
M_{\boldsymbol{Y}}(\boldsymbol{t})=M_{\boldsymbol{X}}(t_1, t_2, \dots, t_r, 0, \dots, 0)\cdot M_{\boldsymbol{X}}(0, \dots, 0, t_{r+1}, t_{r+2}, t_n)
$$

也就是 $$M_{\boldsymbol{X}}=M_{\boldsymbol{Y}}$$，也就是对于任意集合 $$A_1, A_2, \dots, A_n$$，我们有：
$$
\begin{aligned}
&\mathbb{P}\left\{(X_1\in A_1)\land (X_2\in A_2)\land\dots\land (X_n\in A_n)\right\}\\
=&\mathbb{P}\left\{(X_1\in A_1)\land \dots\land(X_r\in A_r)\land\left(\widetilde{X}_{r+1}\in A_{r+1}\right)\land\dots\land\left(\widetilde{X}_{n}\in A_n\right)\right\}\\
=&\mathbb{P}\left\{(X_1\in A_1)\land \dots\land(X_r\in A_r)\right\}\cdot\mathbb{P}\left\{\left(\widetilde{X}_{r+1}\in A_{r+1}\right)\land\dots\land\left(\widetilde{X}_{n}\in A_n\right)\right\}
\end{aligned}
$$

所以我们令 $$X=\left(\overline X, X_1-\overline{X}, \dots, X_n-\overline{X}\right)$$，$$\boldsymbol{t}=(s, t_1, t_2,\dots, t_n)$$。

其实我们要证明的就是：

$$
M_{\boldsymbol{X}}(\boldsymbol{t})=M_{\overline{X}}(s)\cdot M_{X_1-\overline{X}}(t_1)\cdot M_{X_2-\overline{X}}(t_2)\cdots M_{X_n-\overline{X}}(t_n)
$$

{% enddetails %}

***

## From Normal Distribution

接下来我们讨论的是

<!-- 接下来我们需要讨论的是 distribution of sample mean for normally distributed population。我们考虑 $$X_1, \dots X_n\sim N(\mu, \sigma^2)$$，我们很容易证明 $$\overline{X}\sim N\left(\mu, \frac{\sigma^2}{n}\right)$$，也就是 $$\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\sim N(0, 1)$$ -->
