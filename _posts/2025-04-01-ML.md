---
layout: distill
title: Machine Learning
categories: Notes
date: 2025-04-01
description: Notes for Machine Learning
tags:
  - Machine Learning
  - Artificial Intelligence
# pseudocode: true
giscus_comments: true
related_posts: false
pretty_table: true
authors:
  - name: Pu Fanyi
    url: "https://pufanyi.github.io"
    affiliations:
      name: NTU, Singapore
      url: "https://www.ntu.edu.sg/"
---

## Math Review

All modern machine learning algorithms are just nearest neighbors. It's only that the neural networks are telling you the space in which to compute the distance.

#### SVD

[Notes](https://web.stanford.edu/class/cs168/l/l9.pdf)

$$
A = U \Sigma V^\top=\sum_{i=1}^{\min\{m, n\}}\sigma_i u_i v_i^\top
$$

{% include figure.liquid loading="eager" path="assets/img/2025-04-01-ML/SVD.png" class="img-fluid rounded z-depth-1" %}

Compute largest $$k$$ singular values and vectors: $$\mathcal{O}(kmn)$$.

Approximation:

$$
\hat{A} = \sum_{i=1}^{k}\sigma_i u_i v_i^\top = U_k \Sigma_k V_k^\top
$$

For all rank $$k$$ matrices $$B$$:

$$
\|A - \hat{A}\|_F \le \|A - B\|_F
$$

### Maximum Likelihood Estimation

Maximum likelihood estimation:

$$
\hat{\theta} = \arg\max_{\theta\in\Theta} p(D\mid\theta)
$$

Properties:

1. _Consistency_: more data, more accurate (but maybe biased).
2. _Statistically efficient_: least variance.
3. The value of $$p(D\mid\theta_{\text{MLE}})$$ is invariant to re-parameterization.

### Entropy

要搞一个 "degree of surprise" 函数 $$h(p(x))$$，满足：

1. $$h(p) \ge 0$$
2. $$h(p) = 0 \iff p = 1$$
3. $$x \perp y \iff h(p(x\land y)) = h(p(x)) + h(p(y))$$
4. $$h(p_1) > h(p_2)\iff p_1<p_2$$

根据 3 我们有

$$
h(p_1 p_2) = h(p_1) + h(p_2)
$$

如果我们令 $$f(\log p) = h(p)$$ 的话

