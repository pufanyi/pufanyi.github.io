---
layout: distill
title: Machine Learning Notes I - Math Review
categories: Notes
date: 2025-04-01
description: SVD, MLE, Entropy
tags:
  - Machine Learning
  - Artificial Intelligence
  - Statistics
  - Probability Theory
# pseudocode: true
giscus_comments: true
related_posts: false
pretty_table: true
authors:
  - name: Pu Fanyi
    url: "https://pufanyi.github.io"
    affiliations:
      name: NTU, Singapore
      url: "https://www.ntu.edu.sg/"
---

> All modern machine learning algorithms are just nearest neighbors. It's only that the neural networks are telling you the space in which to compute the distance.

## Matrix Derivatives

$$
f(x + \Delta) = f(x) + \frac{\partial f}{\partial x}\Delta + o(\Vert\Delta\Vert)
$$

$$
\nabla f = \left(\frac{\partial f}{\partial x}\right)^\top
$$

所以假设说 $$f: \mathbb{R}^{n}\rightarrow \mathbb{R}$$，我们就应该有

$$
\begin{aligned}
\frac{\partial f}{\partial x} &= \begin{bmatrix}
\frac{\partial f}{\partial x_1} & \frac{\partial f}{\partial x_1} & \ldots & \frac{\partial f}{\partial x_n}
\end{bmatrix}&&\in \mathbb{R}^{1\times n}\\
\nabla f &= \left(\frac{\partial f}{\partial x}\right)^\top = \begin{bmatrix}
\frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_1} \\ \vdots \\ \frac{\partial f}{\partial x_n}
\end{bmatrix}&&\in \mathbb{R}^{n}
\end{aligned}
$$

同样的，对于 $$f: \mathbb{R}^{m\times n}\rightarrow \mathbb{R}$$，我们有：

$$
\left(\frac{\partial f}{\partial X}\right)_{ij} = \frac{\partial f}{\partial X_{ji}}
$$

酱紫

$$
f(X + \Delta) = f(X) + \mathrm{Tr}\left(\frac{\partial f}{\partial X}\Delta\right) + o(\Vert \Delta\Vert)
$$

## SVD

[Notes](https://web.stanford.edu/class/cs168/l/l9.pdf)

$$
A = U \Sigma V^\top=\sum_{i=1}^{\min\{m, n\}}\sigma_i u_i v_i^\top
$$

{% include figure.liquid loading="eager" path="assets/img/2025-04-01-ML-I/SVD.png" class="img-fluid rounded z-depth-1" %}

Compute largest $$k$$ singular values and vectors: $$\mathcal{O}(kmn)$$.

Approximation:

$$
\hat{A} = \sum_{i=1}^{k}\sigma_i u_i v_i^\top = U_k \Sigma_k V_k^\top
$$

For all rank $$k$$ matrices $$B$$:

$$
\|A - \hat{A}\|_F \le \|A - B\|_F
$$

## Maximum Likelihood Estimation

Maximum likelihood estimation:

$$
\hat{\theta} = \arg\max_{\theta\in\Theta} p(D\mid\theta)
$$

Properties:

1. _Consistency_: more data, more accurate (but maybe biased).
2. _Statistically efficient_: least variance.
3. The value of $$p(D\mid\theta_{\text{MLE}})$$ is invariant to re-parameterization.

## Entropy

要搞一个 "degree of surprise" 函数 $$h(p(x))$$，满足：

1. $$h(p) \ge 0$$;
2. $$h(p) = 0 \iff p = 1$$;
3. $$x \perp y \iff h(p(x\land y)) = h(p(x)) + h(p(y))$$;
4. $$h(p_1) > h(p_2)\iff p_1<p_2$$.

根据 3 我们有

$$
h(p_1 p_2) = h(p_1) + h(p_2)
$$

如果我们令 $$f(\log p) = h(p)$$ 的话，我们有

$$
f(\log p_1 + \log p_2) = f(\log p_1) + f(\log p_2)
$$

所以 $$f(p)$$ 是一个线性函数。又因为 $$f(0)=0$$，所以 $$f(x)=-c\dot x$$。$$c>0$$ 因为 $$f$$ 要单调递减且非负。

所以

$$
h(p(x)) = -c\dot \log p(x)
$$

通常我们取 $$c=1$$ 或 $$c=\frac{1}{\log 2}$$。这边就不管了都写成 $$-\log p(x)$$ 了。

于是我们定义

$$
\mathcal{H}(x) = \mathbb{E}[h(p(x))] = -\int p(x) \log p(x)
$$

当然因为 entropy 是从物理来的，他也有一定物理意义。就是我们考虑有 $$N$$ 个东西，$$k$$ 个状态。第 $$i$$ 个状态有 $$n_i$$ 个。那么可能的排列数量为

$$
W = \frac{N!}{\prod n_i!}
$$

我们考虑定义 $$\mathcal{H}$$ 为 $$N\to\infty$$ 时候的状态

$$
\begin{aligned}
\mathcal{H} &= \lim_{N\to\infty} \frac{1}{N}\log W=-\lim_{N\to\infty}\left(\frac{n_i}{N_i}\right)\log\left(\frac{n_i}{N}\right)
\end{aligned}
$$

其中用到了 Stirling's approximation

$$
\log n! = n\log n - n + \mathcal{O}(\log n)
$$

那啥时候 $$\mathcal{H}$$ 最大捏？
