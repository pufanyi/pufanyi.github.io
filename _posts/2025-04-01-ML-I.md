---
layout: distill
title: Machine Learning Notes I - Math Review
categories: Notes
date: 2025-04-01
description: SVD, MLE, Entropy
tags:
  - Machine Learning
  - Artificial Intelligence
  - Statistics
  - Probability Theory
# pseudocode: true
giscus_comments: true
related_posts: false
pretty_table: true
authors:
  - name: Pu Fanyi
    url: "https://pufanyi.github.io"
    affiliations:
      name: NTU, Singapore
      url: "https://www.ntu.edu.sg/"
---

## Math Review

All modern machine learning algorithms are just nearest neighbors. It's only that the neural networks are telling you the space in which to compute the distance.

#### SVD

[Notes](https://web.stanford.edu/class/cs168/l/l9.pdf)

$$
A = U \Sigma V^\top=\sum_{i=1}^{\min\{m, n\}}\sigma_i u_i v_i^\top
$$

{% include figure.liquid loading="eager" path="assets/img/2025-04-01-ML-I/SVD.png" class="img-fluid rounded z-depth-1" %}

Compute largest $$k$$ singular values and vectors: $$\mathcal{O}(kmn)$$.

Approximation:

$$
\hat{A} = \sum_{i=1}^{k}\sigma_i u_i v_i^\top = U_k \Sigma_k V_k^\top
$$

For all rank $$k$$ matrices $$B$$:

$$
\|A - \hat{A}\|_F \le \|A - B\|_F
$$

### Maximum Likelihood Estimation

Maximum likelihood estimation:

$$
\hat{\theta} = \arg\max_{\theta\in\Theta} p(D\mid\theta)
$$

Properties:

1. _Consistency_: more data, more accurate (but maybe biased).
2. _Statistically efficient_: least variance.
3. The value of $$p(D\mid\theta_{\text{MLE}})$$ is invariant to re-parameterization.

### Entropy

要搞一个 "degree of surprise" 函数 $$h(p(x))$$，满足：

1. $$h(p) \ge 0$$;
2. $$h(p) = 0 \iff p = 1$$;
3. $$x \perp y \iff h(p(x\land y)) = h(p(x)) + h(p(y))$$;
4. $$h(p_1) > h(p_2)\iff p_1<p_2$$.

根据 3 我们有

$$
h(p_1 p_2) = h(p_1) + h(p_2)
$$

如果我们令 $$f(\log p) = h(p)$$ 的话，我们有

$$
f(\log p_1 + \log p_2) = f(\log p_1) + f(\log p_2)
$$

所以 $$f(p)$$ 是一个线性函数。又因为 $$f(0)=0$$，所以 $$f(x)=-c\dot x$$。$$c>0$$ 因为 $$f$$ 要单调递减且非负。

所以

$$
h(p(x)) = -c\dot \log p(x)
$$

通常我们取 $$c=1$$ 或 $$c=\frac{1}{\log 2}$$。这边就不管了都写成 $$-\log p(x)$$ 了。

于是我们定义

$$
\mathcal{H}(x) = \mathbb{E}[h(p(x))] = -\int p(x) \log p(x)
$$

当然因为 entropy 是从物理来的，他也有一定物理意义。就是我们考虑有 $$N$$ 个东西，$$k$$ 个状态。第 $$i$$ 个状态有 $$n_i$$ 个。那么可能的排列数量为

$$
W = \frac{N!}{\prod n_i!}
$$
