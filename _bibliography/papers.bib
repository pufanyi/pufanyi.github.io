---
---

@article{li2023mimicit,
    title={MIMIC-IT: Multi-Modal In-Context Instruction Tuning},
    author={Bo Li* and Yuanhan Zhang* and Liangyu Chen* and Jinghao Wang* and Fanyi Pu* and Jingkang Yang and Chunyuan Li and Ziwei Liu},
    year={2023},
    eprint={2306.05425},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    arXiv={2306.05425},
    pdf={https://arxiv.org/pdf/2306.05425.pdf},
    website={https://otter-ntu.github.io/index.html},
    bibtex_show={true},
    code={https://github.com/Luodian/Otter},
    preview={https://otter-ntu.github.io/assets/images/logo/otter_logo.png},
    abstract={High-quality instructions and responses are essential for the
zero-shot performance of large language models on interactive natural
language tasks. For interactive vision-language tasks involving
intricate visual scenes, a large quantity of diverse and creative
instruction-response pairs should be imperative to tune vision-language
models (VLMs). Nevertheless, the current availability of vision-language
instruction-response pairs in terms of quantity, diversity, and
creativity remains limited, posing challenges to the generalization of
interactive VLMs. Here we present
<strong>M</strong>ult<strong>I</strong>-<strong>M</strong>odal
<strong>I</strong>n-<strong>C</strong>ontext
<strong>I</strong>nstruction <strong>T</strong>uning
(<strong>MIMIC-IT</strong>), a dataset comprising 2.8 million multimodal
instruction-response pairs, with 2.2 million unique instructions derived
from images and videos. Each pair is accompanied by multi-modal
in-context information, forming conversational contexts aimed at
empowering VLMs in perception, reasoning, and planning. The
instruction-response collection process, dubbed as
<strong>Syphus</strong>, is scaled using an automatic annotation
pipeline that combines human expertise with GPT’s capabilities. Using
the MIMIC-IT dataset, we train a large VLM named <strong>Otter</strong>.
Based on extensive evaluations conducted on vision-language benchmarks,
it has been observed that Otter demonstrates remarkable proficiency in
multi-modal perception, reasoning, and in-context learning. Human
evaluation reveals it effectively aligns with the user’s intentions. We
release the MIMIC-IT dataset, instruction-response collection pipeline,
benchmarks, and the Otter model.}
}
