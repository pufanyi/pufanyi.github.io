<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="EPlMR3j8io1DcEScvNJuBPxLxCTSnNUjNwF4ZBhRO-I"> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Deep Learning Notes I - Introduction &amp; Math Review | Fanyi Pu </title> <meta name="author" content="Fanyi Pu"> <meta name="description" content="SVD, MLE, Entropy"> <meta name="keywords" content="Fanyi, Fanyi Pu, 濮凡轶"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?16404ec2cd2689e8d0f38f73fe0d38f9"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/avatar_very_small.webp?3828c08bd9576f9a024f464411e622b9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pufanyi.github.io/bak/2025-04-01-ML-I/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Deep Learning Notes I - Introduction & Math Review",
            "description": "SVD, MLE, Entropy",
            "published": "April 01, 2025",
            "authors": [
              
              {
                "author": "Pu Fanyi",
                "authorURL": "https://pufanyi.github.io",
                "affiliations": [
                  {
                    "name": "NTU, Singapore",
                    "url": "https://www.ntu.edu.sg/"
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Fanyi</span> Pu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="https://scholar.google.com/citations?user=58tv6skAAAAJ" rel="external nofollow noopener" target="_blank">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/oi-blog/">oi-blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume/resume.pdf">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Deep Learning Notes I - Introduction &amp; Math Review</h1> <p>SVD, MLE, Entropy</p> </d-title> <d-byline></d-byline> <d-article> <blockquote> <p>All modern machine learning algorithms are just nearest neighbors. It’s only that the neural networks are telling you the space in which to compute the distance.</p> </blockquote> <h2 id="linear-algebra">Linear Algebra</h2> <h3 id="woodbury-identity">Woodbury Identity</h3> \[(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1}+VA^{-1}U)V^{-1}\] <p>其中</p> \[A\in\mathbb{R}^{n\times n}, C\in\mathbb{R}^{k\times k}, k\ll n\] <p>如果 \(A\) 的逆很好算，那这样变换会大大降低计算量。</p> <h3 id="matrix-derivatives">Matrix Derivatives</h3> <h4 id="向量--标量">向量 / 标量</h4> \[f(x + \Delta) = f(x) + \frac{\partial f}{\partial x}\Delta + o(\Vert\Delta\Vert)\] \[\nabla f = \left(\frac{\partial f}{\partial x}\right)^\top\] <p>所以假设说 \(f: \mathbb{R}^{n}\rightarrow \mathbb{R}\)，我们就应该有</p> \[\begin{aligned} \frac{\partial f}{\partial x} &amp;= \begin{bmatrix} \frac{\partial f}{\partial x_1} &amp; \frac{\partial f}{\partial x_1} &amp; \ldots &amp; \frac{\partial f}{\partial x_n} \end{bmatrix}&amp;&amp;\in \mathbb{R}^{1\times n}\\ \nabla f &amp;= \left(\frac{\partial f}{\partial x}\right)^\top = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_1} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}&amp;&amp;\in \mathbb{R}^{n} \end{aligned}\] <h4 id="标量--矩阵">标量 / 矩阵</h4> <p>同样的，对于 \(f: \mathbb{R}^{m\times n}\rightarrow \mathbb{R}\)，我们有：</p> \[\left(\frac{\partial f}{\partial X}\right)_{ij} = \frac{\partial f}{\partial X_{ji}}\] <p>酱紫</p> \[f(X + \Delta) = f(X) + \mathrm{Tr}\left(\frac{\partial f}{\partial X}\Delta\right) + o(\Vert \Delta\Vert)\] <h4 id="jacobian-向量--向量">Jacobian: 向量 / 向量</h4> <p>假设函数是 \(z: \mathbb{R}^{d}\rightarrow \mathbb{R}^{k}\)，我们想要有</p> \[z(x+\Delta) = z(x) + J(z) \Delta + o(\Vert\Delta\Vert)\] <p>所以其实我们可以看成是 \(z\) 的每行单独拆开来嘛，也就是</p> \[J(z) = \frac{\partial z}{\partial x} = \begin{bmatrix} \frac{\partial z_1}{\partial x}\\ \frac{\partial z_2}{\partial x}\\ \vdots\\ \frac{\partial z_k}{\partial x} \end{bmatrix}=\begin{bmatrix} \frac{\partial z_1}{\partial x_1} &amp; \frac{\partial z_1}{\partial x_2} &amp;\ldots&amp; \frac{\partial z_1}{\partial x_d}\\ \frac{\partial z_2}{\partial x_1} &amp; \frac{\partial z_2}{\partial x_2} &amp;\ldots&amp; \frac{\partial z_2}{\partial x_d}\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\ \frac{\partial z_k}{\partial x_1} &amp; \frac{\partial z_k}{\partial x_2} &amp;\ldots&amp; \frac{\partial z_k}{\partial x_d}\\ \end{bmatrix}\] \[[J(z)]_{ij} = \left(\frac{\partial z}{\partial x}\right)_{ij} = \frac{\partial z_i}{\partial x_j}\] <h4 id="hessian-二阶导">Hessian: 二阶导</h4> <p>对于函数 \(f: \mathbb{R}^n\rightarrow \mathbb{R}\)，我们想要求二阶导</p> \[\nabla f(x+\Delta) = \nabla f(x) + \nabla^2 f(x)\Delta + o(\Vert\Delta\Vert)\] <p>所以其实就是</p> \[H(f) = \nabla^2 f(x) = [J(f(x))]^\top = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1\partial x_2} &amp; \ldots &amp; \frac{\partial^2f}{\partial x_1\partial x_n}\\ \frac{\partial^2 f}{\partial x_2\partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \ldots &amp; \frac{\partial^2f}{\partial x_2\partial x_n}\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\ \frac{\partial^2 f}{\partial x_n\partial x_1} &amp; \frac{\partial^2 f}{\partial x_n\partial x_2} &amp; \ldots &amp; \frac{\partial^2f}{\partial x_n^2} \end{bmatrix}\] <h4 id="derivative-rules">Derivative Rules</h4> <p>我们先来算 \(\frac{\partial}{\partial x}(AB)\)，考虑到</p> \[\begin{aligned} \left[\frac{\partial}{\partial x}(AB)\right]_{ij} &amp;= \frac{\partial}{\partial x}(AB)_{ij} = \frac{\partial}{\partial x}\sum_{k}A_{ik}B_{kj} \\&amp;= \sum_{k}\left(\frac{\partial A_{ik}}{\partial x}B_{kj} + A_{ik}\frac{\partial B_{kj}}{\partial x}\right)\\&amp;=\frac{\partial A}{\partial x}B + A\frac{\partial B}{\partial x} \end{aligned}\] <p>将 \(A^{-1}A=I\) 代入上式可以得到</p> \[\frac{\partial}{\partial x}A^{-1} = -A^{-1}\frac{\partial A}{\partial x}A^{-1}\] <h3 id="svd">SVD</h3> <p><a href="https://web.stanford.edu/class/cs168/l/l9.pdf" rel="external nofollow noopener" target="_blank">Notes</a></p> \[A = U \Sigma V^\top=\sum_{i=1}^{\min\{m, n\}}\sigma_i u_i v_i^\top\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-01-ML-I/SVD-480.webp 480w,/assets/img/2025-04-01-ML-I/SVD-800.webp 800w,/assets/img/2025-04-01-ML-I/SVD-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2025-04-01-ML-I/SVD.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Compute largest \(k\) singular values and vectors: \(\mathcal{O}(kmn)\).</p> <p>Approximation:</p> \[\hat{A} = \sum_{i=1}^{k}\sigma_i u_i v_i^\top = U_k \Sigma_k V_k^\top\] <p>For all rank \(k\) matrices \(B\):</p> \[\|A - \hat{A}\|_F \le \|A - B\|_F\] <h2 id="calculus-of-variations">Calculus of Variations</h2> <p>变分法中，我们考虑的是对于一个函数的函数 \(F(f)\)，\(f\) 稍稍改变，\(F\) 就会稍稍改变：</p> \[F[y(x) + \epsilon \eta(x)] = F[y(x)] + \epsilon\int\frac{\delta F}{\delta y(x)}\eta(x)\mathrm{d}x+\mathcal{O}(\epsilon^2)\] <p>假设</p> \[F[y] = \int G\left(y(x), y'(x), x\right)\mathrm{d}x\] <p>那么</p> \[\begin{aligned} \int\frac{\delta F}{\delta y(x)}\eta(x)\mathrm{d}x&amp;= \end{aligned}\] <h2 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h2> <p>Maximum likelihood estimation:</p> \[\hat{\theta} = \arg\max_{\theta\in\Theta} p(D\mid\theta)\] <p>Properties:</p> <ol> <li> <em>Consistency</em>: more data, more accurate (but maybe biased).</li> <li> <em>Statistically efficient</em>: least variance.</li> <li>The value of \(p(D\mid\theta_{\text{MLE}})\) is invariant to re-parameterization.</li> </ol> <h2 id="entropy">Entropy</h2> <p>要搞一个 “degree of surprise” 函数 \(h(p(x))\)，满足：</p> <ol> <li>\(h(p) \ge 0\);</li> <li>\(h(p) = 0 \iff p = 1\);</li> <li>\(x \perp y \iff h(p(x\land y)) = h(p(x)) + h(p(y))\);</li> <li>\(h(p_1) &gt; h(p_2)\iff p_1&lt;p_2\).</li> </ol> <p>根据 3 我们有</p> \[h(p_1 p_2) = h(p_1) + h(p_2)\] <p>如果我们令 \(f(\log p) = h(p)\) 的话，我们有</p> \[f(\log p_1 + \log p_2) = f(\log p_1) + f(\log p_2)\] <p>所以 \(f(p)\) 是一个线性函数。又因为 \(f(0)=0\)，所以 \(f(x)=-c\cdot x\)。\(c&gt;0\) 因为 \(f\) 要单调递减且非负。</p> <p>所以</p> \[h(p(x)) = -c\cdot \log p(x)\] <p>通常我们取 \(c=1\) 或 \(c=\frac{1}{\log 2}\)。这边就不管了都写成 \(-\log p(x)\) 了。</p> <p>于是我们定义</p> \[\mathcal{H}(x) = \mathbb{E}[h(p(x))] = -\int p(x) \log p(x)\] <p>当然因为 entropy 是从物理来的，他也有一定物理意义。就是我们考虑有 \(N\) 个东西，\(k\) 个状态。第 \(i\) 个状态有 \(n_i\) 个。那么可能的排列数量为</p> \[W = \frac{N!}{\prod n_i!}\] <p>我们考虑定义 \(\mathcal{H}\) 为 \(N\to\infty\) 时候的状态</p> \[\begin{aligned} \mathcal{H} &amp;= \lim_{N\to\infty} \frac{1}{N}\log W=-\lim_{N\to\infty}\left(\frac{n_i}{N_i}\right)\log\left(\frac{n_i}{N}\right) \end{aligned}\] <p>其中用到了 Stirling’s approximation</p> \[\log n! = n\log n - n + \mathcal{O}(\log n)\] <h2 id="deep-learning-introduction">Deep Learning Introduction</h2> <p><strong>Risk</strong>: expected loss</p> \[R(\theta) = \mathbb{E}\left[\mathcal{L}(\theta, X, Y)\right]\] <p><strong>Empirical Risk</strong>: average loss</p> \[\hat{R}(\theta) = \frac{1}{N}\sum_{i=1}^N\mathcal{L}(\theta, x_i, y_i)\] <p>Empirical Risk too high: underfitting; too low: overfitting.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"pufanyi/pufanyi.github.io","data-repo-id":"R_kgDOJnYv-A","data-category":"General","data-category-id":"DIC_kwDOJnYv-M4CW4n7","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Fanyi Pu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: April 24, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?eaf77346e117baa09987a278a117b9a7"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?8d7ebe8276cfa922ec1506a5c6b20c13"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>