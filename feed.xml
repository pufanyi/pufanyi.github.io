<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://pufanyi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://pufanyi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-11T19:39:32+00:00</updated><id>https://pufanyi.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Reinforcement Learning Notes I - Introduction &amp;amp; Imitation Learning</title><link href="https://pufanyi.github.io/blog/RL-I/" rel="alternate" type="text/html" title="Reinforcement Learning Notes I - Introduction &amp;amp; Imitation Learning"/><published>2025-04-11T00:00:00+00:00</published><updated>2025-04-11T00:00:00+00:00</updated><id>https://pufanyi.github.io/blog/RL-I</id><content type="html" xml:base="https://pufanyi.github.io/blog/RL-I/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>我们把 \(\left&lt;s_t, a_t\right&gt;\) 打包起来，其实他就构成了一个 Markov chain:</p> \[p_\theta (\tau) = p_\theta (s_1) \prod_{t=1}^T \pi_\theta (a_t \mid s_t) \cdot \mathbb{P}(s_{t+1} \mid s_t, a_t)\] <p>对于 learning objective：</p> \[\theta^{*} = \arg \max_{\theta} \cal{J}\left(\theta\right) = \arg \max_{\theta} \mathbb{E}_{\tau \sim {\pi}_{\theta} \left(\tau\right)}\left[r\left(\tau\right)\right]\] <p>我们定义我现在在 \(s_t\)，做了 action \(a_t\)，然后按照 \(\pi\) 所得到的期望 reward 为</p> \[\cal{Q}^{\pi} (s_t, a_t) = \sum_{t'=t}^T \mathbb{E}_{\left(s_t', a_t'\right) \sim \pi}\left[r\left(s_t', a_t'\right) \mid s_t, a_t\right]\] <p>然后我们定义 \(\cal{V}\) 表示现在我在 \(s_t\) 的时候遵循 \(\pi\) 所得到的期望 reward：</p> \[\cal{V}^{\pi} (s_t) = \sum_{t'=t}^T \mathbb{E}_{\left(s_t', a_t'\right) \sim \pi}[r\left(s_t', a_t'\right) \mid s_t] = \mathbb{E}_{a_t \sim \pi\left(a_t \mid s_t\right)}\left[\cal{Q}^{\pi} \left(s_t, a_t\right)\right]\] <h2 id="types-of-algorithms">Types of Algorithms</h2> <ul> <li>Policy Gradients: 跟往常一样，就是求导然后去做优化</li> <li>Value-based: 去直接计算最优解的 \(\cal{Q}\) 和 \(\cal{V}\)，然后通过这个推出 \(\pi\)</li> <li>Actor-critic: 通过计算当前 \(\hat{\theta}\) 的 \(\hat{\cal{Q}}\) 和 \(\hat{\cal{V}}\)，然后通过这个去进行优化，得到最终的 \(\pi\)</li> <li>Model-based RL: 使用模型来代表各种参数，然后进行优化</li> </ul> <h2 id="imitation-learning">Imitation Learning</h2> <p>数据 \(\mathcal{D} = \left\{(s_i, a_i)\right\}_{i=1}^N\)，然后我们要设计 \(\pi\) 去你和 \(a\)</p> <p>首先最直觉的想法是直接去最优化</p> \[\theta^* = \arg\min_\theta \frac{1}{\left|\mathcal{D}\right|} \sum_{(s, a)\in \mathcal{D}} \left\|\hat{a} - a\right\|^2\] <p>但这样肯定是不对的。比如说我现在前面有一个障碍物，一半人要左转，一半人要右转。结果这样拟合出来，最优的 \(\hat{a}\) 就成直行撞墙了。</p> <p>所以说我们考虑生成模型，也就是说去拟合 \(p_\theta (a \mid s)\)：</p> \[\theta^* = \arg\min_\theta \mathbb{E}_{(s, a)\sim \mathcal{D}}\left[-\log p_\theta (a \mid s)\right]\] <p>那怎么搞这个 \(p\) 呢？离散的好说，对于连续的，我们一般有三种方法：</p> <ol> <li>Mixture of Gaussians</li> <li>Discretize + Autoregressive</li> <li>Diffusion</li> </ol> <p>但是这些 imitation learning 有一个问题，就是 \(p_\pi(s)\neq p_\mathcal{D}(s)\)，这就导致了我们学习的东西，和我们现实中遇见的是稍有不同的。这边有个东西叫做“covariate shift”。(missing reference) 详细解释了这一现象。</p>]]></content><author><name>Pu Fanyi</name></author><category term="Notes"/><category term="Reinforcement Learning"/><category term="Artificial Intelligence"/><summary type="html"><![CDATA[Introductions, MDPs, Imitation Learning]]></summary></entry><entry><title type="html">Reinforcement Learning as a Co-Design of Product and Research</title><link href="https://pufanyi.github.io/blog/RL-in-Products/" rel="alternate" type="text/html" title="Reinforcement Learning as a Co-Design of Product and Research"/><published>2025-04-09T00:00:00+00:00</published><updated>2025-04-09T00:00:00+00:00</updated><id>https://pufanyi.github.io/blog/RL-in-Products</id><content type="html" xml:base="https://pufanyi.github.io/blog/RL-in-Products/"><![CDATA[<p>主讲人：<a href="https://karinanguyen.com/">Karina Nguyen</a></p> <h2 id="product">Product</h2> <p>Canvas：从普通的 next-token prediction 到 complex tasks 的转变。让用户和模型交流的方式更加多样（Training the model to become a collaborator）</p> <ol> <li>能 preview 代码。不仅是代码可以直接运行，前端代码可以直接看效果</li> <li>可以方便地对某个词语，某个句子选中进行 QA</li> </ol> <p>Two ways of building research-driven products:</p> <ol> <li>Familiar form factor for unfamiliar capability：直接套壳 <ol> <li>100K Context + File uploads: 这个其实现在比较稀松平常了？就是说把 context 加长来支持长 PDF claude acts as business analyst - YouTube</li> <li>\(\mathbb{P}(\text{I Know})\) / Calibration：其实这部分她语速太快稍稍有点没太听懂。好像说的是，比如一个 unreliable 的长文章，我通过模型去把有用的（或者说是保真的）信息快速提取出来。越保真就越高亮，来支持快速阅读</li> <li>Streaming model’s thoughts：这个也很平常了，就是用输出些中间过程来防止用户等的太急</li> </ol> </li> <li>Start with product belief &amp; vision -&gt; make the model do that：根据需求 finetune 模型 - <strong>I just think product designers and model trainers need to collaborate more with each other</strong> <ol> <li>Redesign 新闻 page（没咋听懂）：Most effective way to get people to readmore of storylines coverage is by adding alayer of context to our product andcoverage-across major surfaces - that connects people to the stories and information they need.</li> <li>more humane command line</li> <li>Claude 每个 chat session 的标题是根据用户语言习惯去标的，也就是说，他在生成 chat session 的时候用到了 user info。</li> <li>Claude in slack：群助手，好像功能更多，比如每周五 summarize channel，然后还有很多其他的 agent 功能。</li> </ol> </li> </ol> <p>这个是 \(\mathbb{P}(\text{I Know})\) / Calibration 的例子：</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-09-RL-in-Products/pik-480.webp 480w,/assets/img/2025-04-09-RL-in-Products/pik-800.webp 800w,/assets/img/2025-04-09-RL-in-Products/pik-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-04-09-RL-in-Products/pik.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>这个是 more humane command line 的例子：</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-09-RL-in-Products/cmdline-480.webp 480w,/assets/img/2025-04-09-RL-in-Products/cmdline-800.webp 800w,/assets/img/2025-04-09-RL-in-Products/cmdline-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-04-09-RL-in-Products/cmdline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="model-behaviors">Model Behaviors</h2> <h3 id="refusing">Refusing</h3> <p>Gives opinions but with caveats</p> <p>尽量不要直接说 “I don’t actually have a point of view” 在有争议的问题上，而是要适当的发表自己的观点，并提出这个观点有 bias</p> <p>More nuanced refusals：</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-09-RL-in-Products/refusing-480.webp 480w,/assets/img/2025-04-09-RL-in-Products/refusing-800.webp 800w,/assets/img/2025-04-09-RL-in-Products/refusing-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-04-09-RL-in-Products/refusing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>模型拒绝的时候，应该多用 “我感觉这个问题很难回答”，而不是 “你的问题很难回答”。说自己的 feeling 而非直接评价问题。</p> <p>Building evals that we would trust</p> <ul> <li><a href="https://aclanthology.org/2024.naacl-long.301.pdf">XSTest</a>: two hundred non-malicious prompts</li> <li><a href="https://huggingface.co/datasets/allenai/WildChat">WildChat</a>: ambiguous requests, codeswitching, topic-switching, and political discussions</li> </ul> <p>用 RL 来让模型学会正确的拒绝</p> <p>另外几种奇怪的需要拒绝的类别：</p> <ul> <li>Function calling refusals：需要告诉模型他没有 physical body to perform tasks in real-world</li> <li>Long doc refusals：Data in the form of “I don’t have vision capabilities to do XXX”</li> </ul> <h3 id="construct-rl-env-and-rewards">Construct RL env and Rewards</h3> <p>RL 的时候，不一定是只给用户输入，而是用户输入+用户信息。因为不同用户会有不同的 preference。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-09-RL-in-Products/subjective-480.webp 480w,/assets/img/2025-04-09-RL-in-Products/subjective-800.webp 800w,/assets/img/2025-04-09-RL-in-Products/subjective-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-04-09-RL-in-Products/subjective.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>有了用户信息，这些东西就可以变得 objective 了。就是去迎合用户。</p> <h3 id="reward-hacks">Reward Hacks</h3> <p>没细讲，但提了两篇文章，还没来得及认真看：</p> <ol> <li><a href="https://arxiv.org/abs/2503.11926">Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation</a></li> <li><a href="https://lilianweng.github.io/posts/2024-11-28-reward-hacking/">Reward Hacking in Reinforcement Learning</a></li> </ol>]]></content><author><name></name></author><category term="Notes"/><category term="Reinforcement Learning"/><category term="Product Design"/><summary type="html"><![CDATA[Notes for CS25 Transformers United V5, Lecture 2]]></summary></entry><entry><title type="html">Machine Learning Notes I - Introduction &amp;amp; Math Review</title><link href="https://pufanyi.github.io/blog/ML-I/" rel="alternate" type="text/html" title="Machine Learning Notes I - Introduction &amp;amp; Math Review"/><published>2025-04-01T00:00:00+00:00</published><updated>2025-04-01T00:00:00+00:00</updated><id>https://pufanyi.github.io/blog/ML-I</id><content type="html" xml:base="https://pufanyi.github.io/blog/ML-I/"><![CDATA[<blockquote> <p>All modern machine learning algorithms are just nearest neighbors. It’s only that the neural networks are telling you the space in which to compute the distance.</p> </blockquote> <h2 id="linear-algebra">Linear Algebra</h2> <h3 id="woodbury-identity">Woodbury Identity</h3> \[(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1}+VA^{-1}U)V^{-1}\] <p>其中</p> \[A\in\mathbb{R}^{n\times n}, C\in\mathbb{R}^{k\times k}, k\ll n\] <p>如果 \(A\) 的逆很好算，那这样变换会大大降低计算量。</p> <h3 id="matrix-derivatives">Matrix Derivatives</h3> <h4 id="向量--标量">向量 / 标量</h4> \[f(x + \Delta) = f(x) + \frac{\partial f}{\partial x}\Delta + o(\Vert\Delta\Vert)\] \[\nabla f = \left(\frac{\partial f}{\partial x}\right)^\top\] <p>所以假设说 \(f: \mathbb{R}^{n}\rightarrow \mathbb{R}\)，我们就应该有</p> \[\begin{aligned} \frac{\partial f}{\partial x} &amp;= \begin{bmatrix} \frac{\partial f}{\partial x_1} &amp; \frac{\partial f}{\partial x_1} &amp; \ldots &amp; \frac{\partial f}{\partial x_n} \end{bmatrix}&amp;&amp;\in \mathbb{R}^{1\times n}\\ \nabla f &amp;= \left(\frac{\partial f}{\partial x}\right)^\top = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_1} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}&amp;&amp;\in \mathbb{R}^{n} \end{aligned}\] <h4 id="标量--矩阵">标量 / 矩阵</h4> <p>同样的，对于 \(f: \mathbb{R}^{m\times n}\rightarrow \mathbb{R}\)，我们有：</p> \[\left(\frac{\partial f}{\partial X}\right)_{ij} = \frac{\partial f}{\partial X_{ji}}\] <p>酱紫</p> \[f(X + \Delta) = f(X) + \mathrm{Tr}\left(\frac{\partial f}{\partial X}\Delta\right) + o(\Vert \Delta\Vert)\] <h4 id="jacobian-向量--向量">Jacobian: 向量 / 向量</h4> <p>假设函数是 \(z: \mathbb{R}^{d}\rightarrow \mathbb{R}^{k}\)，我们想要有</p> \[z(x+\Delta) = z(x) + J(z) \Delta + o(\Vert\Delta\Vert)\] <p>所以其实我们可以看成是 \(z\) 的每行单独拆开来嘛，也就是</p> \[J(z) = \frac{\partial z}{\partial x} = \begin{bmatrix} \frac{\partial z_1}{\partial x}\\ \frac{\partial z_2}{\partial x}\\ \vdots\\ \frac{\partial z_k}{\partial x} \end{bmatrix}=\begin{bmatrix} \frac{\partial z_1}{\partial x_1} &amp; \frac{\partial z_1}{\partial x_2} &amp;\ldots&amp; \frac{\partial z_1}{\partial x_d}\\ \frac{\partial z_2}{\partial x_1} &amp; \frac{\partial z_2}{\partial x_2} &amp;\ldots&amp; \frac{\partial z_2}{\partial x_d}\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\ \frac{\partial z_k}{\partial x_1} &amp; \frac{\partial z_k}{\partial x_2} &amp;\ldots&amp; \frac{\partial z_k}{\partial x_d}\\ \end{bmatrix}\] \[[J(z)]_{ij} = \left(\frac{\partial z}{\partial x}\right)_{ij} = \frac{\partial z_i}{\partial x_j}\] <h4 id="hessian-二阶导">Hessian: 二阶导</h4> <p>对于函数 \(f: \mathbb{R}^n\rightarrow \mathbb{R}\)，我们想要求二阶导</p> \[\nabla f(x+\Delta) = \nabla f(x) + \nabla^2 f(x)\Delta + o(\Vert\Delta\Vert)\] <p>所以其实就是</p> \[H(f) = \nabla^2 f(x) = [J(f(x))]^\top = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1\partial x_2} &amp; \ldots &amp; \frac{\partial^2f}{\partial x_1\partial x_n}\\ \frac{\partial^2 f}{\partial x_2\partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \ldots &amp; \frac{\partial^2f}{\partial x_2\partial x_n}\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\ \frac{\partial^2 f}{\partial x_n\partial x_1} &amp; \frac{\partial^2 f}{\partial x_n\partial x_2} &amp; \ldots &amp; \frac{\partial^2f}{\partial x_n^2} \end{bmatrix}\] <h4 id="derivative-rules">Derivative Rules</h4> <p>我们先来算 \(\frac{\partial}{\partial x}(AB)\)，考虑到</p> \[\begin{aligned} \left[\frac{\partial}{\partial x}(AB)\right]_{ij} &amp;= \frac{\partial}{\partial x}(AB)_{ij} = \frac{\partial}{\partial x}\sum_{k}A_{ik}B_{kj} \\&amp;= \sum_{k}\left(\frac{\partial A_{ik}}{\partial x}B_{kj} + A_{ik}\frac{\partial B_{kj}}{\partial x}\right)\\&amp;=\frac{\partial A}{\partial x}B + A\frac{\partial B}{\partial x} \end{aligned}\] <p>将 \(A^{-1}A=I\) 代入上式可以得到</p> \[\frac{\partial}{\partial x}A^{-1} = -A^{-1}\frac{\partial A}{\partial x}A^{-1}\] <h3 id="svd">SVD</h3> <p><a href="https://web.stanford.edu/class/cs168/l/l9.pdf">Notes</a></p> \[A = U \Sigma V^\top=\sum_{i=1}^{\min\{m, n\}}\sigma_i u_i v_i^\top\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-01-ML-I/SVD-480.webp 480w,/assets/img/2025-04-01-ML-I/SVD-800.webp 800w,/assets/img/2025-04-01-ML-I/SVD-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-04-01-ML-I/SVD.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Compute largest \(k\) singular values and vectors: \(\mathcal{O}(kmn)\).</p> <p>Approximation:</p> \[\hat{A} = \sum_{i=1}^{k}\sigma_i u_i v_i^\top = U_k \Sigma_k V_k^\top\] <p>For all rank \(k\) matrices \(B\):</p> \[\|A - \hat{A}\|_F \le \|A - B\|_F\] <h2 id="calculus-of-variations">Calculus of Variations</h2> <p>变分法中，我们考虑的是对于一个函数的函数 \(F(f)\)，\(f\) 稍稍改变，\(F\) 就会稍稍改变：</p> \[F[y(x) + \epsilon \eta(x)] = F[y(x)] + \epsilon\int\frac{\delta F}{\delta y(x)}\eta(x)\mathrm{d}x+\mathcal{O}(\epsilon^2)\] <p>假设</p> \[F[y] = \int G\left(y(x), y'(x), x\right)\mathrm{d}x\] <p>那么</p> \[\begin{aligned} \int\frac{\delta F}{\delta y(x)}\eta(x)\mathrm{d}x&amp;= \end{aligned}\] <h2 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h2> <p>Maximum likelihood estimation:</p> \[\hat{\theta} = \arg\max_{\theta\in\Theta} p(D\mid\theta)\] <p>Properties:</p> <ol> <li><em>Consistency</em>: more data, more accurate (but maybe biased).</li> <li><em>Statistically efficient</em>: least variance.</li> <li>The value of \(p(D\mid\theta_{\text{MLE}})\) is invariant to re-parameterization.</li> </ol> <h2 id="entropy">Entropy</h2> <p>要搞一个 “degree of surprise” 函数 \(h(p(x))\)，满足：</p> <ol> <li>\(h(p) \ge 0\);</li> <li>\(h(p) = 0 \iff p = 1\);</li> <li>\(x \perp y \iff h(p(x\land y)) = h(p(x)) + h(p(y))\);</li> <li>\(h(p_1) &gt; h(p_2)\iff p_1&lt;p_2\).</li> </ol> <p>根据 3 我们有</p> \[h(p_1 p_2) = h(p_1) + h(p_2)\] <p>如果我们令 \(f(\log p) = h(p)\) 的话，我们有</p> \[f(\log p_1 + \log p_2) = f(\log p_1) + f(\log p_2)\] <p>所以 \(f(p)\) 是一个线性函数。又因为 \(f(0)=0\)，所以 \(f(x)=-c\cdot x\)。\(c&gt;0\) 因为 \(f\) 要单调递减且非负。</p> <p>所以</p> \[h(p(x)) = -c\cdot \log p(x)\] <p>通常我们取 \(c=1\) 或 \(c=\frac{1}{\log 2}\)。这边就不管了都写成 \(-\log p(x)\) 了。</p> <p>于是我们定义</p> \[\mathcal{H}(x) = \mathbb{E}[h(p(x))] = -\int p(x) \log p(x)\] <p>当然因为 entropy 是从物理来的，他也有一定物理意义。就是我们考虑有 \(N\) 个东西，\(k\) 个状态。第 \(i\) 个状态有 \(n_i\) 个。那么可能的排列数量为</p> \[W = \frac{N!}{\prod n_i!}\] <p>我们考虑定义 \(\mathcal{H}\) 为 \(N\to\infty\) 时候的状态</p> \[\begin{aligned} \mathcal{H} &amp;= \lim_{N\to\infty} \frac{1}{N}\log W=-\lim_{N\to\infty}\left(\frac{n_i}{N_i}\right)\log\left(\frac{n_i}{N}\right) \end{aligned}\] <p>其中用到了 Stirling’s approximation</p> \[\log n! = n\log n - n + \mathcal{O}(\log n)\] <p>那啥时候 \(\mathcal{H}\) 最大捏？</p>]]></content><author><name>Pu Fanyi</name></author><category term="Notes"/><category term="Machine Learning"/><category term="Artificial Intelligence"/><category term="Statistics"/><category term="Probability Theory"/><summary type="html"><![CDATA[SVD, MLE, Entropy]]></summary></entry><entry><title type="html">Parallel Computing and Distributed Systems</title><link href="https://pufanyi.github.io/blog/Parallel-Computing-and-Distributed-Systems/" rel="alternate" type="text/html" title="Parallel Computing and Distributed Systems"/><published>2025-03-30T00:00:00+00:00</published><updated>2025-03-30T00:00:00+00:00</updated><id>https://pufanyi.github.io/blog/Parallel-Computing-and-Distributed-Systems</id><content type="html" xml:base="https://pufanyi.github.io/blog/Parallel-Computing-and-Distributed-Systems/"><![CDATA[<h2 id="parallel-computing">Parallel Computing</h2> <p><a href="https://cs149.stanford.edu/">Stanford CS149</a></p> <h3 id="a-modern-multi-core-processor">A Modern Multi-Core Processor</h3> <p>Forms of Parallel Execution</p> <ol> <li>Superscalar: 找到不相关的语句然后并行执行</li> <li>SIMD：ALU 的并行，Single instruction, multiple data</li> <li>Multi-core：多个 core，所以想咋玩咋玩</li> </ol> <p>Vector Program: <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html">AVX intrinsics</a></p> <p>SIMD 遇到 branchs？一部分 ALU 需要等待</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-30-Parallel-Computing-and-Distributed-Systems/pc/simd_branch-480.webp 480w,/assets/img/2025-03-30-Parallel-Computing-and-Distributed-Systems/pc/simd_branch-800.webp 800w,/assets/img/2025-03-30-Parallel-Computing-and-Distributed-Systems/pc/simd_branch-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-03-30-Parallel-Computing-and-Distributed-Systems/pc/simd_branch.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="accessing-memory">Accessing Memory</h3> <p>Interleaved (temporal) multi-threading: 这个任务在 stall 了（比如在 fetch memory），先执行别的任务</p> <p>Overcoming bandwidth limits is often the most important challenge facing software developers targeting modern throughput-optimized systems.</p> <hr/> <h2 id="distributed-system">Distributed System</h2> <p><a href="https://pdos.csail.mit.edu/6.824/index.html">MIT 6.824</a></p> <p><a href="https://www.scs.stanford.edu/24sp-cs244b/">Stanford CS244B</a></p> <p>忽然发现 CS244B 的 instructor 是 <d-cite key="vamplew2016get"></d-cite> 的一作。</p> <h3 id="introduction">Introduction</h3> <p>You should try everything else before you try distributed systems.</p> <p>Why distributed systems?</p> <ul> <li>Parallelism</li> <li>Fault tolerance</li> <li>Physical</li> <li>Security (isolation)</li> </ul> <p>Infrastructure for application:</p> <ol> <li>Storage</li> <li>Communication</li> <li>Computation</li> </ol> <p>我们是要将这些复杂的 distribution 隐藏起来，让他看起来像是一个普通的系统</p> <h3 id="remote-procedure-call-rpc">Remote Procedure Call (RPC)</h3> <p>需要比普通的 procedure 多一个 “I don’t know” 的选项。</p> <p>Interface Definition Languages (IDL): Specify RPC call and return types</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-30-Parallel-Computing-and-Distributed-Systems/ds/idl-480.webp 480w,/assets/img/2025-03-30-Parallel-Computing-and-Distributed-Systems/ds/idl-800.webp 800w,/assets/img/2025-03-30-Parallel-Computing-and-Distributed-Systems/ds/idl-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-03-30-Parallel-Computing-and-Distributed-Systems/ds/idl.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><a href="https://datatracker.ietf.org/doc/html/rfc4506">External Data Representation Standard (XDR)</a></p> <h3 id="consensus">Consensus</h3> <p>在讨论 asynchronous systems 的时候，我们会保守地认为网络是很慢的。也就是说，we can’t distinguish failed agent from slow network.</p> <p>有 \(n\) 个 agents，每个 agent input 一个数字，现在要各个 agent 通过互相交流达成一致，使得每个 agent 的 output 都相同，并且是其中一个 agent 的 input。</p> <p><strong>Safety</strong>: 所有 agent 的 output 都相同（agreement）并且 output 为其中某个 agent 的 input（validity）</p> <p><strong>Liveness</strong>: 所有 non-failed 的 agents 都有输出</p> <p><strong>Fault tolerance</strong>:</p> <ul> <li>Fail-stop: 如果 agent 出错，那么他会立即停止</li> <li>Byzantine-fault-tolerant <d-cite key="lamport2019byzantine"></d-cite>: agent 出错可能出现任意行为，比如发送假消息</li> </ul> <p><strong>FLP impossibility result</strong> <d-cite key="fischer1985impossibility"></d-cite>: 对于 deterministic consensus protocol，safety, liveness, and fault tolerance 三者不可得兼。</p> <p><strong>Bivalent</strong>: An execution of a consensus protocol is in a bivalent state when the network can affect which value agents choose.</p> <p><strong>Univalent, Valent</strong>: An execution of a consensus protocol is in a univalent state when only one output value is possible. If that value is \(i\), call the state \(i\)-valent.</p> <p><strong>Stuck</strong>: An execution of a [broken] consensus protocol is in a stuck state when one or more non-faulty nodes can never output a value.</p>]]></content><author><name>Pu Fanyi</name></author><category term="Notes"/><category term="Parallel Computing"/><category term="Distributed System"/><summary type="html"><![CDATA[Notes for Parallel Computing and Distributed Systems]]></summary></entry><entry><title type="html">Stochastic Processes and Reinforcement Learning</title><link href="https://pufanyi.github.io/SPRL-Notes/SPRL.pdf" rel="alternate" type="text/html" title="Stochastic Processes and Reinforcement Learning"/><published>2024-10-16T00:00:00+00:00</published><updated>2024-10-16T00:00:00+00:00</updated><id>https://pufanyi.github.io/SPRL-Notes/SPRL</id><content type="html" xml:base="https://pufanyi.github.io/SPRL-Notes/SPRL.pdf"><![CDATA[<p>看我能坚持多久。。。</p> <h2 id="markov-chains">Markov Chains</h2> <p><a href="https://link.springer.com/book/10.1007/978-981-13-0659-4">教材捏</a></p> <p><a href="https://personal.ntu.edu.sg/ariel.neufeld/script_MH3512_Marked.pdf">划了重点的教材捏</a>（prof 划的，虽然我感觉他把整本书划了一遍。。。）</p> <h3 id="gambling-problems">Gambling Problems</h3> <p>有 \(S\) 块钱，\(A\) 有 \(K\) 块钱，\(B\) 有 \(S-K\) 块。每次有 \(p\) 的概率 \(A\) 从 \(B\) 拿走一块，\(q=1-p\) 的概率 \(B\) 从 \(A\) 拿走一块。谁拿到 \(S\) 块钱就算赢。</p> \[X_{n+1}=\begin{cases} X_n+1 &amp; p \\ X_n-1 &amp; q \end{cases}\] <p>\(f_S(k)\) 表示 \(A\) 赢的概率。很显然我们有：</p> \[f_S(k)=pf_{S}(k+1)+qf_{S}(k-1)\] <p>不难解出</p> \[f_S(k)=\frac{(p/q)^{S-k}-1}{(p/q)^S-1}\] <p>\(T_{0, S}\) 表示一个游戏啥时候结束，\(h_S(k)=\mathbb{E}\left[T_{0, S}\mid X_0=K\right]\)。</p> <p>很显然</p> \[h_S(k)=1+ph_S(k+1)+qh_S(k-1)\] <p>这个方程的特解比较难搞，注意到 \(p+q=1\)，我们可以改写成差分方程：</p> \[-1=p\left(h_S(k+1)-h_S(k)\right)+q\left(h_S(k)-h_S(k-1)\right)\] <p>观察出方程在 \(p\neq q\) 的时候一个特解为 \(\frac{k}{q-p}\)。</p> <p>不难解出齐次方程 \(h_S(k)=ph_S(k+1)+qh_S(k-1)\) 的解，最终可以得到在 \(p\neq q\) 时</p> \[h_s(k)=\frac{1}{q-p}\left(k-S\cdot\frac{1-(q/p)^k}{1-(q/p)^S}\right)\] <p>当 \(p=q=\frac{1}{2}\) 时，我们有特解 \(-k^2\)。所以说 \(p=q\) 时</p> \[h_S(k)=k(S-k)\] <h3 id="random-walks">Random Walks</h3> <p>Bernoulli Random Walks: \(X_n\) 相互独立，其中</p> \[\begin{cases} \mathbb{P}(X_k=+1)=p \\ \mathbb{P}(X_k=-1)=q \end{cases}\] <p>\(p+q=1\)，然后我们定义</p> \[S_n=\sum_{i=1}^nX_i\] <p>很显然 \(\mathbb{P}(S_{2n}=2k+1)=\mathbb{P}(S_{2n+1}=2k)=0\)，然后</p> \[\begin{cases} \mathbb{P}(S_{2n}=2k)=\binom{2n}{n+k}p^{n+k}q^{n-k} \\ \mathbb{P}(S_{2n+1}=2k+1)=\binom{2n+1}{n+k+1}p^{n+k+1}q^{n-k} \end{cases}\] <p>难度在我们怎么计算他啥时候回到 \(0\)。我们令</p> \[T_0^r=\inf\{n\geq 1: S_n=0\}\] <p>表示第一次回到 \(0\) 的时间。</p> <p>然后我们设</p> \[g(n)=\mathbb{P}\left(T_0^r=n\mid S_0=0\right)\] <p>也就是在第 \(n\) 步第一次回到 \(0\) 的概率。那很显然 \(g(2k+1)=0\)。</p> <p>接下来是一个比较神奇的套路，就是我们假设 \(h(n)\) 为第 \(n\) 步回到 \(0\) 的概率，那我们可以得到一个卷积式子：</p> \[h(n)=\sum_{k=0}^{n-2}g(n-k)h(k)\] <p>也就是先走 \(n-k\) 步第一次回到 \(0\)，然后继续走 \(k\) 步回到 \(0\)。所以我们现在只要解决 \(h\) 就可以了。</p> <p>我们考虑 \(h(n)\) 的生成函数</p> \[H(s)=\mathbb{E}\left[s^{T_0^r}\cdot\mathbb{1}_{T_0^r&lt;\infty}\right]=\sum_{n=0}^\infty h(n)s^n\] <p>大概推推：</p> \[\begin{aligned} H(s)&amp;=\sum_{n=0}^\infty h(n)s^n\\ &amp;=\sum_{n=0}^\infty\binom{2n}{n}p^nq^ns^{2n}\\ &amp;=\sum_{n=0}^\infty\frac{(2n)!}{(n!)^2}\left(pqs^2\right)^n\\ &amp;=\sum_{n=0}^\infty\frac{\left(\prod_{i=1}^n2i\right)\left(\prod_{i=1}^n(2i-1)\right)}{(n!)^2}\left(pqs^2\right)^n\\ &amp;=\sum_{n=0}^\infty\frac{\prod_{i=1}^n(2i-1)}{n!}\left(2pqs^2\right)^n\\ &amp;=\sum_{n=0}^\infty\frac{(-2)^n\prod_{i=1}^n\left(-\frac{1}{2}-(i-1)\right)}{n!}\left(2pqs^2\right)^n\\ &amp;=\sum_{n=0}^\infty\frac{\left(-\frac{1}{2}\right)^{\underline{i}}}{n!}\left(-4pqs^2\right)^n\\ &amp;=\left(1-4pqs^2\right)^{-\frac{1}{2}} \end{aligned}\] <p>又考虑到：</p> \[\begin{aligned} G(s)H(s)&amp;=\left(\sum_{i=1}^\infty s^ig(i)\right)\left(\sum_{j=0}^\infty s^jh(j)\right)\\ &amp;=\sum_{i=2}^\infty\sum_{j=0}^\infty s^{i+j}g(i)h(j)\\ &amp;=\sum_{k=2}^\infty s^k\sum_{i=2}^\infty g(i)h(k-i)\\ &amp;=\sum_{k=2}^\infty s^k h(k)\\ &amp;=-1 + \sum_{k=0}^\infty s^kh(k)\\ &amp;=-1 + H(s) \end{aligned}\] <p>于是乎</p> \[G(s)=1-\frac{1}{H(s)}=1-\sqrt{1-4pqs^2}\] <p>所以</p> \[\begin{aligned} \mathbb{P}\left(T_0^r=\infty\mid S_0=0\right)&amp;=1-\mathbb{P}\left(T_0^r&lt;\infty\mid S_0=0\right)\\ &amp;=1-G(1)=\sqrt{1-4pq}\\ &amp;=\sqrt{4p^2-4p+1}\\ &amp;=\lvert 2p-1\rvert\\ &amp;=\lvert p-q\rvert \end{aligned}\] <p>而根据前面的 Gambling Problems，其实我们已经解出当 \(k\neq 0\) 时：</p> \[\mathbb{P}\left(T_0^r=\infty\mid S_0=k\right)=1-\lim_{S\to\infty}f_S(k)=\max\left\{0, 1-\left(\frac{q}{p}\right)^k\right\}\] <p>然后我们来计算 \(\mathbb{E}\left[T_0^r\mid S_0=0\right]\) 的时候会发现如果 \(\mathbb{P}\left(T_0^r\mid S_0=0\right)&gt;0\) 的时候这个期望肯定是 \(\infty\)。而 \(\mathbb{P}\left(T_0^r\mid S_0=0\right)\) 只有在 \(p=q=\frac{1}{2}\) 的时候才会为 \(0\)。然鹅当 \(p=q=\frac{1}{2}\) 时：</p> \[\mathbb{E}\left[T_0^r\mid S_0=0\right]=\mathbb{E}\left[T_0^r \cdot\mathbb{1}_{\left\{T_0^r&lt;\infty\right\}}\mid S_0=0\right]=\left.\frac{\partial G}{\partial s}\right|_{s=1}=\infty\] <p>所以我们不管 \(p, q\) 都有：</p> \[\mathbb{E}\left[T_0^r\mid S_0=0\right]=\infty\] <p>关于 first time 的 distribution 的话，我们不难算出</p> \[\mathbb{P}\left(T_0^r=2k\mid S_0=0\right)=\frac{1}{(2k)!}\left.\frac{\partial^{2k}G}{\partial s^{2k}}\right|_{s=0}=\frac{1}{2k-1}\binom{2k}{k}(pq)^k\] <h3 id="discrete-time-markov-chains">Discrete-Time Markov Chains</h3> <p>Markov property 指的是下一步的 distribution 只跟当前有关：</p> \[\mathbb{P}\left(Z_{n+1}=j\mid Z_n=i_n, \cdots, Z_0=i_0\right)=\mathbb{P}\left(Z_{n+1}=j\mid Z_n=i_n\right)\] <p>\(\pi_n\) 是行向量，转移方程</p> \[\pi_{n+1}=\pi_n P\] <p>首先研究 hitting probabilities。假设状态空间为 \(\mathbb{S}\)，现在有一个点集 \(A\subset\mathbb{S}\) 是吸收点。也就是说 \(\forall s\in\mathbb{S}, P_{s, s}=1\)。我们康康从 \(k\) 开始被 \(A\) 中哪个点吸收的分布：</p> \[g_l(k)=\mathbb{P}\left(Z_{T_A}=l\mid Z_0=k\right)\] <p>其中 \(T_A\) 表示第一次撞到 \(A\) 的概率。</p> <p>显然</p> \[g_l(k) = P_{k, l} + \sum_{m\in\mathbb{S}\setminus A}P_{k, m} g_l(m)\] <p>然后我们研究从一个点开始期望多久被吸收，我们定义：</p> \[h_A(k) = \mathbb{E}\left[T_A\mid Z_0=k\right]\] <p>显然</p> \[h_A(k) = 1 + \sum_{m\in\mathbb{S}\setminus A}P_{k, m} h_A(m)\] <p>当然很多事会我们会钦定最后 \(d\) 个为吸收点，也就是：</p> \[P = \begin{bmatrix} Q &amp; R \\ 0 &amp; I_d \\ \end{bmatrix}\] <p>酱一来我们就可以简单地写成</p> \[h_A = \mathbb{1}_{n-d} + Qh_A\] <p>当然很多时候我们每个点都是有个 utility 的，也就是说：</p> \[h_A(k) = \mathbb{E}\left[\sum_{n=0}^{T_A}r(Z_n)\mid Z_0=k\right]\] <p>那酱的话就把 \(\mathbb{1}\) 换成 \(r\) 就可以了。</p> <p>接下来是 return times。我们定义 \(T_j^r\) 为第一次到 \(j\) 的时间（但不是 \(Z_0\)）：</p> \[T_j^r = \inf\{n\ge 1: Z_n=j\}\] <p>然后 \(\mu_j(i)\) 表示从 \(i\) 开始第一次回到 \(j\) 的期望时间：</p> \[\mu_j(i) = \mathbb{E}\left[T_j^r\mid Z_0=i\right]\] <p>酱紫 \(\mu_i(i)\) 就成功定义了“return times”。</p> <p>显然：</p> \[\mu_j(i) = 1 + \sum_{m\in\mathbb{S}}P_{i, m}\mu_j(m)\] <p>我们接下来定义 \(p_{i, j}\) 为从 \(i\) 能走到 \(j\) 的概率，当然一开始不算：</p> \[p_{i, j} = \mathbb{P}\left(T_j^r&lt;\infty\mid Z_0=i\right)\] <p>我们定义 \(f_{i, j}^{(n)}\) 为从 \(i\) 开始走 \(n\) 步恰好第一次走到 \(j\) 的概率：</p> \[f_{i, j}^{(n)} = \mathbb{P}\left(T_j^r=n\mid Z_0=i\right)\] <p>显然</p> \[p_{i, j} = \sum_{n=1}^\infty f_{i, j}^{(n)}\] <p>Number of returns 被定义为：</p> \[R_j = \sum_{n=1}^\infty\mathbb{1}_{\{Z_n=j\}}\] <p>那么 \(R_j\) 的分布其实是：</p> \[\mathbb{P}(R_j = m \mid Z_0 = i) = \begin{cases} 1 - p_{i, j} &amp; m = 0 \\ p_{i, j}\cdot p_{j, j}^{m - 1}\cdot (1 - p_{j, j}) &amp; m \ge 1 \end{cases}\] <p>那期望也好算：</p> \[\mathbb{E}[R_j \mid Z_0 = i] = \sum_{m = 1}^\infty m\cdot p_{i, j}\cdot p_{j, j}^{m - 1}\cdot (1 - p_{j, j}) = \frac{1}{1 - p_{j, j}}\] <p>所以说我们得到一个性质就是，如果要</p> \[\mathbb{E}[R_i\mid Z_0=i] = \frac{1}{1 - p_{i, i}}\] <p>这个东西有穷，当且仅当 \(p_{i, i} &lt; 1\)。</p> <p>而还有一个意义比较明确而且封闭的式子是：</p> \[\mathbb{E}[R_j\mid Z_0=i] = \mathbb{E}\left[\mathbb{1}_{\{X_n=j\}}\mid X_0=i\right] = \sum_{n=1}^\infty\left[P^n\right]_{i, j} = -\mathbb{1}_{\{i=j\}} + \left[(I - P)^{-1}\right]_{i, j}\] <h3 id="branching-processes">Branching Processes</h3> <p>一开始我有一个东西，然后没过一段时间这个东西随机分裂成 \(Y\) 个一样的东西，然后一直这样分裂下去。</p> \[X_0 = 1, X_{n+1} = \sum_{k=1}^{X_n} Y_k\] <p>其中</p> \[\mathbb{P}(Y &lt; \infty) = \sum_{n\ge 0}\mathbb{P}(Y = n) = 1\] <p>考虑转移矩阵 \(P\)，\(P_{i,j}\) 表示 \(i\) 个东西分裂成 \(j\) 个的概率。显然 \(P_{0,0}=1\)，没有东西就无法分裂。\(P_{1, j}=\mathbb{P}(Y=j)\)，指的是从一个东西分裂开来。</p> <p>酱紫的话是个树状结构，所以叫 Branching Process。</p> <p>考虑生成函数 \(G_n(s) = \mathbb{E}\left[s^{X_n}\mid X_0 = 1\right]\) 表示第 \(n\) 代的概率生成函数，我们有：</p> \[G_{n+1}(s) = G_n(G_1(s)) = G_1(G_n(s))\] <p>证明的话：</p> \[\begin{aligned} G_{n+1}(s) &amp;= \mathbb{E}\left[s^{X_{n+1}}\mid X_0 = 1\right]\\ &amp;=\mathbb{E}\left[s^{\sum_{l=1}^{X_n}Y_l}\mid X_0=1\right]\\ &amp;=\sum_{k=1}^\infty\mathbb{E}\left[\prod_{l=1}^{k}s^{Y_l}\mid X_n=k\right]\mathbb{P}(X_n=k\mid X_1 = 1)\\ &amp;=\sum_{k=1}^\infty\mathbb{E}\left[s^{Y}\right]^k\mathbb{P}(X_n=k\mid X_1 = 1)\\ &amp;=\sum_{k=1}^\infty G_1(s)^k\mathbb{P}(X_n=k\mid X_1 = 1)\\ &amp;=G_n(G_1(s)) \end{aligned}\] <p>于是乎对于 \(n\) 轮后的期望值我们有：</p> \[\begin{aligned} \mu_n &amp;= \mathbb{E}\left[X_n\mid X_0 = 1\right]\\ &amp;=\left.\frac{\partial G_n(s)}{\partial s}\right|_{s=1}\\ &amp;=\left.\frac{\partial G_{n-1}(G_1(s))}{\partial G_1(s)}\cdot\frac{\partial G_1(s)}{\partial s}\right|_{s=1}\\ &amp;=\left.\frac{\partial G_{n-1}(G_1(s))}{\partial G_1(s)}\right|_{s=1}\cdot\left.\lim_{s\to 1^-}\frac{\partial G_1(s)}{\partial s}\right|_{s=1}\\ &amp;=\left.\frac{\partial G_{n-1}(s)}{\partial s}\right|_{s=1}\cdot\left.\frac{\partial G_1(s)}{\partial s}\right|_{s=1}\\ &amp;=\mu_{n-1}\mu_1\\ &amp;=\mu_1^n \end{aligned}\] <p>对于一个 branching process \((X_n)_{n\in\mathbb{N}}\)：</p> <ul> <li>Supercritical: \(\mu_1 &gt; 1\)，\(\mu_n\to\infty\)</li> <li>Critical: \(\mu_1 = 1\)，\(\mu_n\to\infty\)</li> <li>Subcritical: \(\mu_1 &lt; 1\)，\(\mu_n\to 0\)</li> </ul> <p>同样的，我们考虑方差</p> \[\begin{aligned} \sigma_n^2 &amp;= \mathrm{Var}\left[X_n\mid X_0 = 1\right]\\ &amp;= \left.\frac{1}{2}\frac{\partial^2G_{n}(s)}{\partial s^2}\right|_{s=1}\\ &amp;= \left.\frac{1}{2}\frac{\partial}{\partial s}\left(\frac{\partial}{\partial s} G_{n-1}(s)\cdot\frac{\partial}{\partial s}G_1(s)\right)\right|_{s=1}\\ &amp;= \left.\frac{\partial^2}{\partial s^2}G_{n-1}(s)\cdot\frac{\partial}{\partial s}G_1(s)\right|_{s=1} + \left.\frac{\partial}{\partial s}G_{n-1}(s)\cdot\frac{\partial^2}{\partial s^2}G_1(s)\right|_{s=1}\\ &amp;= \sigma_{n-1}^2\mu_1 + \mu_{n-1}\sigma_1^2\\ &amp;= \sigma_{n-1}^2\mu_1 + \mu_{1}^{n-1}\sigma_1^2\\ &amp;=\begin{cases} n\sigma_1^2 &amp; \mu = 1\\ \sigma_1^2\mu_1^{n-1}\frac{1-\mu_1^n}{1-\mu_1} &amp; \mu\neq 1 \end{cases} \end{aligned}\] <p>接下来我们要研究的是，\(X_n\) 能延续多久，也就是“time to extinction”。我们定义</p> \[T_0 = \inf\{n\ge 0: X_n = 0\}\] <p>以及最终的 extinction probability</p> \[\alpha_k = \mathbb{P}(T_0 &lt; \infty\mid X_0 = k)\] <p>首先显然我们有：</p> \[\begin{aligned} \mathbb{P}(T_0 = n\mid X_0 = 1) &amp;= \mathbb{P}(X_n = 0\mid X_0 = 1) - \mathbb{P}(X_{n-1} = 0\mid X_0 = 1) \\ &amp;= G_n(0) - G_{n-1}(0)\\ &amp;= G_1(G_{n-1}(0)) - G_{n-1}(0) \end{aligned}\] <p>而我们来考虑 \(\alpha_k\)，首先显然这 \(k\) 个是独立的：</p> \[\alpha_k = \alpha_1^k\] <p>而我们考虑了 \(\alpha_1\)：</p> \[\begin{aligned} \alpha_1 &amp;= \lim_{n\to\infty}\mathbb{P}(T_0 &lt; n\mid X_0 = 1)\\ &amp;= \lim_{n\to\infty}\mathbb{P}(X_n=0\mid X_0 = 1)\\ &amp;= \lim_{n\to\infty}G_n(0) \end{aligned}\] <p>另一个神奇的性知识 \(\alpha_1\) 一定是方程 \(G_1(\alpha) = \alpha\) 的解：</p> \[\begin{aligned} \alpha_1 &amp;= \sum_{k=0}^\infty\alpha_k\mathbb{P}(X_1 = k\mid X_0 = 1)\\ &amp;= \sum_{k=0}^\infty\alpha_1^k\mathbb{P}(X_1 = k\mid X_0 = 1)\\ &amp;= G_1(\alpha_1) \end{aligned}\] <p>当然考虑到</p> \[\alpha = G_1(\alpha) = G_1(G_1(\alpha)) = \cdots\] <p>所以</p> \[\forall k\in\mathbb{N}, \alpha = G_k(\alpha)\] <p>我们可以进一步证明 \(\alpha_1\) 一定是方程 \(G_1(\alpha) = \alpha\) 最小的正解。</p> <p>首先因为 \(\frac{\partial}{\partial s}G_1(s) &gt; 0\)，这个函数肯定是递增的，所以说对于任意 \(k\)，\(G_k\) 肯定是递增的。</p> <p>而考虑到上面已经论证过只要满足 \(\alpha = G_1(\alpha)\)，肯定对于任意 \(k\)，能满足 \(\alpha = G_k(\alpha)\)，于是乎：</p> \[\alpha_1 = \lim_{n\to\infty}G_n(0)\le \lim_{n\to\infty}G_n(\alpha) = \alpha\] <p>也就是说对于任意符合条件的 \(\alpha\)，他肯定是大于等于 \(\alpha_1\) 的。于是乎 \(\alpha_1\) 一定是最小的正解。</p> <h3 id="continuous-time-markov-chains">Continuous-Time Markov Chains</h3> <p>首先是 Poisson Process。就是隔一段时间往上 \(+1\)。\(N_t\) 表示 \(t\) 时刻是多少，其中 \(N_0=0\)。我们定义 \(T_k\) 为第一次到达 \(k\) 的时间。</p> \[N_t = \sum_{k\ge 1} k\cdot\mathbb{1}_{t\in \left[T_{k-1}, T_k\right)} = \sum_{k\ge 1}\mathbb{1}_{t\in \left[T_{k-1}, \infty\right)}\] <p>我们需要这种过程满足两个性质：</p> <ol> <li>Independence of increments: 对于任意的 \(0\le t_1 &lt; t_2 &lt; \cdots &lt; t_n\)，\(N_{t_1}-N_{t_0}, N_{t_2}-N_{t_1}, \cdots, N_{t_n}-N_{t_{n-1}}\) 是相互独立的。</li> <li>Stationarity of increments: \(N_{t+s}-N_s\sim N_t\)。</li> </ol> <p>那其实很显然这就是一个 poison distribution 嘛：</p> \[\mathbb{P}(N_t - N_s = k) = e^{-\lambda(t-s)}\frac{(\lambda(t-s))^k}{k!}\] <p>其中</p> \[\lambda = \lim_{h\to 0^+} \frac{1}{h}\mathbb{P}(N_h = 1)\] <p>然后还有就是 \(T_1\) 个 exp distribution 有关，\(T_n\) 跟 gamma distribution 有关。</p> \[T_n \sim \Gamma(n, \lambda): f_{T_n}(t) = \lambda^n e^{-\lambda t}\frac{(\lambda t)^{n-1}}{\Gamma(n)}\] <p>然后我们把他一般化一下，得到 continuous-time Markov chain。其实核心也就是 “Memoryless”。</p> <p>只不过这次的转移矩阵是一个关于时间的函数了：</p> \[P_{i, j}(t) = \mathbb{P}(Z_{s + t} = j\mid Z_s = i)\] <p>而显然 \(P(0)=I\)。</p> <p>很显然的性质是：</p> \[P(s + t) = P(s)P(t) = P(t)P(s)\] <p>学习 Poisson process，我们来研究类似 \(\lambda\) 的一个跟“平均”有关的东西，我们考虑：</p> \[Q = \lim_{t\to 0^+}\frac{1}{t}\left(P(t) - P(0)\right) = \left.\frac{\partial}{\partial t}P(t)\right|_{t=0}\] <p>而其实我们有（这个叫“backward Kolmogorov equation”）：</p> \[\begin{aligned} \frac{\partial}{\partial t}P(t) &amp;= \lim_{h\to 0^+}\frac{1}{h}\left(P(t+h) - P(t)\right)\\ &amp;= \lim_{h\to 0^+}\frac{1}{h}\left(P(h)P(t) - P(t)\right)\\ &amp;= \lim_{h\to 0^+}\frac{1}{h}\left(P(h) - I\right)P(t)\\ &amp;= QP(t) \end{aligned}\] <p>当然同理我们还可以得到（这个叫“forward Kolmogorov equation”）：</p> \[\frac{\partial}{\partial t}P(t) = P(t)Q\] <p>解这个微分方程我们有：</p> \[P(t) = e^{Qt} = \sum_{n=0}^\infty\frac{t^n}{n!}Q^n = I + \sum_{n=1}^\infty\frac{t^n}{n!}Q^n\] <p>我们记 \(\lambda_{i,j} = Q_{i,j}\)，其实这个 \(\lambda_{i,j}\) 就和 Poisson process 那个一样了：</p> \[P(h) = I + hQ + \mathcal{O}(h^2)\] <p>也就是：</p> \[\mathbb{P}(X_{x + h} = j\mid X_t = i) = P_{i, j}(h) = \begin{cases} \lambda_{i,j}h + \mathcal{O}(h^2) &amp; i\neq j \\ 1 + \lambda_{i,i}h + \mathcal{O}(h^2) &amp; i = j \end{cases}\] <p>根据这个，我们对于 \(i\neq j\)，如果我们已经知道它下一步会到 \(j\)。我们定义停留在 \(i\) 的时间为 \(\tau_{i, j}\)，我们可以有：</p> \[\mathbb{P}(\tau_{i, j}&gt;t\mid i\to j) = e^{-\lambda_{i, j}t}\] <p>以及</p> \[\mathbb{E}[\tau_{i, j}\mid i\to j] = \int_{0}^\infty t\lambda_{i, j}e^{-\lambda_{i, j}t}\mathrm{d}t = \frac{1}{\lambda_{i, j}}\] <p>当然另外还有一个性质是：</p> \[\sum_{j\in\mathbb{S}}\lambda_{i,j} = 0\] <p>就是每个点要么出去要么留在原地嘛。</p> <p>然后我们考虑 \(\tau_i\)，也就是停留在 \(i\) 的时间：</p> \[\tau_i = \min_{j\neq i}\tau_{i, j}\] <p>我们可以计算概率：</p> \[\begin{aligned} \mathbb{P}(\tau_i &gt; t) &amp;= \mathbb{P}\left(\min_{j\neq i}\tau_{i, j}\right)\\ &amp;= \prod_{j\neq i}\mathbb{P}(\tau_{i, j} &gt; t)\\ &amp;= \exp\left(-\sum_{j\neq i}\lambda_{i, j}t\right)\\ &amp;= e^{\lambda_{i, i}t} \end{aligned}\] <p>于是乎</p> \[\mathbb{E}[\tau_i] = \frac{1}{\sum_{j\neq i}\lambda_{i, j}} = -\frac{1}{\lambda_{i, i}}\] <h2 id="discrete-time-martingales">Discrete-Time Martingales</h2>]]></content><author><name></name></author><category term="Notes"/><category term="Probability Theory"/><category term="Reinforcement Learning"/><summary type="html"><![CDATA[Notes for NTU MH3512 Stochastic Processes]]></summary></entry><entry><title type="html">Game Theory</title><link href="https://pufanyi.github.io/blog/Berkeley-STAT155/" rel="alternate" type="text/html" title="Game Theory"/><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://pufanyi.github.io/blog/Berkeley-STAT155</id><content type="html" xml:base="https://pufanyi.github.io/blog/Berkeley-STAT155/"><![CDATA[<ul> <li><a href="https://classes.berkeley.edu/content/2024-summer-stat-155-001-lec-001">STAT 155 Game Theory</a> <ul> <li><a href="https://mitpress.mit.edu/9780262650403/a-course-in-game-theory/">Textbook 1</a></li> <li><a href="https://mitpress.mit.edu/9780262061414/game-theory/">Textbook 2</a></li> <li><a href="https://doi.org/10.2307/j.ctvjsf522">Textbook 3</a></li> </ul> </li> <li><a href="https://www.ntu.edu.sg/docs/librariesprovider123/obtl/mas/updated-obtl/mh4320-spms-mas-outcomes-based-teaching-and-learning-document-(obtl)-5-jun-2023.pdf">MH4320 Computational Economics</a> <ul> <li><a href="https://www.cambridge.org/core/books/game-theory/B0C072F66E027614E46A5CAB26394C7D">Textbook</a></li> </ul> </li> </ul> <h2 id="preferences">Preferences</h2> <h3 id="一些基础定义和概念">一些基础定义和概念</h3> <p><strong>Consumption Space</strong>: \(X\subseteq\mathbb{R}_+^n\)</p> <p><strong>Preference Relation</strong>: \(x\succsim y\) or \(x\succeq y\), \(x\) is as lease as good as \(y\)</p> <p><strong>Indifference</strong>: \(x\sim y \Longleftrightarrow x\succsim y \land y\succsim x\)</p> <p><strong>Strict Preference</strong>: \(x\succ y \Longleftrightarrow x\succsim y \land x\nsim y\)</p> <h3 id="常见的-assumptions">常见的 Assumptions</h3> <p>Preference 这个概念挺大的，通常我们会带着一些 assumption 来研究。下面是一些常见的 assumption。</p> <p><strong>Rationality Assumption</strong>: completeness and transitivity。</p> <p>这样不会陷入死循环和一些矛盾的情况。</p> <p>反例：</p> <ol> <li>剪刀石头布：\(x\succ y\succ z\succ x\)，无法做出最优选择</li> <li>咖啡加糖：\(c_1\sim c_{0.9}\sim c_{0.8}\sim\cdots\sim c_{0.1}\sim c_0\)，但是\(c_1\nsim c_0\)。需要考虑一些心理的情况。</li> </ol> <p><em><strong>Convex Combination</strong></em>:</p> \[\alpha x+(1-\alpha)y, \alpha\in[0,1]\] <p><em><strong>Convex Set</strong></em>:</p> \[\forall x, y\in X\ ,\forall\alpha\in[0, 1], \alpha x+(1-\alpha)y\in X\] <p><em><strong>Convex Function</strong></em>:</p> \[f: X\to\mathbb{R}, \forall x, y\in X, \forall\alpha\in[0, 1], f(\alpha x+(1-\alpha)y)\le \alpha f(x)+(1-\alpha)f(y)\] <p>Convex 是笑脸，concave 是哭脸。</p> <p>高维空间比大小的一些记号：</p> \[\begin{aligned} x\ge y&amp;\Longleftrightarrow \forall i, x_i\ge y_i\\ x&gt;y&amp;\Longleftrightarrow x\ge y\land x\neq y\\ x\gg y&amp;\Longleftrightarrow \forall i, x_i&gt;y_i \end{aligned}\] <p><em><strong>Strongly Monotone</strong></em>:</p> \[x&gt;y\Longrightarrow f(x)&gt;f(y)\] <p><em><strong>Weakly Monotone</strong></em>:</p> \[x\gg y\Longrightarrow f(x)&gt; f(y)\] <p><strong>Locally Satisfied Preference</strong>:</p> \[\forall x\in X, \forall \epsilon&gt;0, \exists y, \lVert x-y\rVert&lt;\epsilon\land y\succsim x\] <p>旁边总有比他好的。</p> <p><strong>Weekly Monotone Preference</strong>:</p> \[x\gg y\Longrightarrow x\succ y\] <p>所有东西都来一点更好。</p> <p><strong>Strongly Monotone Preference</strong>:</p> \[x&gt;y\Longrightarrow x\succ y\] <p>越多越好。</p> <p><strong>Convex Preference</strong>:</p> \[x\succsim z, y\succsim z\Longrightarrow\forall\alpha\in[0, 1], \alpha x+(1-\alpha)y\succsim z\] <p><strong>Strictly Convex Preference</strong>:</p> \[x\succsim z, y\succsim z, y\neq x\Longrightarrow \forall\alpha\in(0, 1), \alpha x+(1-\alpha)y\succ z\] <p>我们称 \(f\) 是 quasi-concave 的当且仅当</p> \[f(\lambda x+(1-\lambda)y)\le\max\{f(x), f(y)\}\] <p><strong>Continuous Preference</strong>:</p> \[\forall n\in\mathbb{N}, x_n\succsim y_n, x_n\to x, y_n\to y\Longrightarrow x\succsim y\] <h2 id="utility-functions">Utility Functions</h2> <p>我们定义一个 \(u: X\to \mathbb{R}\) 使得</p> \[u(x)\ge u(y)\Longleftrightarrow x\succsim y\] <p>我们说是 \(u\) represents \(\succsim\)。</p> <p>这个 \(u\) 就是 \(\succsim\) 的 utility functions。</p> <p>一个定理是 \(\succsim\) 是 rational and continuous 的当且仅当存在一个连续的 \(u\)。</p> <ul> <li>\(\succsim\) is monotone \(\Longleftrightarrow\) \(u\) is monotone</li> <li>\(\succsim\) is convex \(\Longleftrightarrow\) \(u\) is quasi-concave</li> </ul> <p>当然很显然地：</p> \[\begin{cases} x\sim y\Longleftrightarrow u(x)=u(y)\\ x\succ y\Longleftrightarrow u(x)&gt;u(y) \end{cases}\] <h2 id="marginal-utility">Marginal Utility</h2> \[\mathrm{MU}(x)=\frac{\partial u(x)}{\partial x}\] <p>一般比如说对钱，我们会有 \(\mathrm{MU}(x)\) 是递减的。给的越多，多一块的价值越小。也就是 \(u''(x)&lt;0\)。</p> <h2 id="decision-making-under-uncertainty">Decision Making Under Uncertainty</h2> <h3 id="lotteries">Lotteries</h3> <p>在有 uncertainty 的情况下，用户做的决策是 lotteries，而不是确定的 goods。</p> <p>A lottery is a vector \(L = (x_1, p_1; x_2, p_2; \cdots; x_n, p_n)\). \(x_i\) 是 realization，\(p_i\) 是 probability。</p> <p>对于某个 realization，我们有其 utility \(u(x_i)\)，然后我们定义整个 lottery 的 utility 为 \(U(L) = \mathbb{E}[u(L)]\) (Von-Neumann Morgenstern Utility Function)。</p> <p>很多时候我们会对 lotteries 做线性叠加，比如一些钱买定期，一些钱买股票。所以其实我们就是在一个 convex set 上做决策。</p> <p>这时候问题就简化为我们有 \(n\) 个 realizations \(X = \{x_1, x_2, \cdots, x_n\}\)，将这 \(n\) 个 realizations 做线性组合，我们将这个 simplex 定义为 \(\mathbb{L}(X)\)，也就是：</p> \[\mathbb{L}(X) = \left\{(x_1, p_1; x_2, p_2; \cdots; x_n, p_n): p_i\ge 0, \sum_{i=1}^n p_i = 1\right\}\] <p>我们同样可以在 \(\mathbb{L}(X)\) 上定义 \(\succsim\)，而 \(\succsim\) 需要满足如下公理：</p> <ul> <li>Completeness: \(\forall L_1, L_2\in\mathbb{L}(X), L_1\succsim L_2\lor L_2\succsim L_1\)</li> <li>Transitivity: \(L_1\succsim L_2, L_2\succsim L_3\Longrightarrow L_1\succsim L_3\)</li> <li>Continuity: \(L_1\succsim L_2\succsim L_3\Longrightarrow \exists\alpha\in[0, 1], L_2\sim \alpha L_1+(1-\alpha)L_3\)</li> <li>Independence: \(L_1\succsim L_2\Longrightarrow \forall \alpha\in[0, 1], \alpha L_1+(1-\alpha)L_3\succsim \alpha L_2+(1-\alpha)L_3\)</li> </ul> <p><strong>Von Neumann–Morgenstern utility theorem</strong>: 上面四条同时 hold，等价于存在一个 \(u\)，并且任何可行的 \(u'\) 都可以通过一个 affine transformation 得到：\(u' = a+bu, b&gt;0\)。</p> <h3 id="st-petersburg-paradox">St. Petersburg paradox</h3> <p>这个是用来展示为什么必须要用 \(\mathbb{E}[u(L)]\) 而不是直接 \(\mathbb{E}[L]\) 的例子。</p> <p>假设有一个游戏，需要 1000 块钱。游戏是给你一个硬币你去抛，如果正面就给你一块钱继续抛，第二次正面两块钱，第三次四块钱，第 \(n\) 次 \(2^{n-1}\) 块钱，直到你抛到反面结束。</p> <p>这个游戏是铁不会玩的，因为你回本概率挺低的。但是</p> \[\mathbb{E}[L] = \sum_{n=1}^\infty\frac{1}{2^n}\times 2^{n-1} = \infty\] <p>这也显示了人们对赌钱是 risk averse 的。</p> <h3 id="risk-aversion">Risk Aversion</h3> <p>对于一个通过 \(u\) 来进行选择的 agent，我们判断他是否喜欢 take risks：</p> <ul> <li>Risk Averse: \(u(\mathbb{E}[L])\ge \mathbb{E}[u(L)]\)</li> <li>Risk Neutral: \(u(\mathbb{E}[L]) = \mathbb{E}[u(L)]\)</li> <li>Risk Loving: \(u(\mathbb{E}[L])\le \mathbb{E}[u(L)]\)</li> </ul> <p>用 Jensen’s inequality 很好判断: For a convex function \(f\), we have \(f(\mathbb{E}[X])\le \mathbb{E}[f(X)]\).</p> <p>对于一个 Risk Averse 的 agent，会有一个 \(\mathrm{CE}\) 来表示他愿意用这么多钱来换取一个稳定的状态：</p> \[u(\mathbb{E}[L]-\mathrm{CE}) = \mathbb{E}[u(L)]\] <p>我们考虑把一个 agent 的对 risk 的态度量化地表示，我们考虑 \(-u''(x)\) 的正负是个很好的指标。但如果 \(v=a+bu\)，那么 \(-v''(x)\neq -u''(x)\)，所以我们考虑：</p> \[R_A(x)=-\frac{u''(x)}{u'(x)}\] <p>我们将其称作 Arrow-Pratt Absolute Risk Aversion Coefficient，越大越 risk averse。</p> <p>但是有时候钱越多我们就会越极端，所以我们考虑定义 Relative Risk Aversion：</p> \[R_R(x)=-x\cdot\frac{u''(x)}{u'(x)}\] <h3 id="allais-paradox">Allais Paradox</h3> <p>考虑四个 lotteries：</p> <ul> <li>Lottery A: \(\left(\$10^6, 11\%; \$0, 89\%\right)\)</li> <li>Lottery B: \(\left(\$5\times 10^6, 10\%; \$0, 90\%\right)\)</li> <li>Lottery C: \(\left(\$10^6, 100\%\right)\)</li> <li>Lottery D: \(\left(\$5\times 10^6, 10\%; \$10^6, 89\%, \$0, 1\%\right)\)</li> </ul> <p>A 比 B，C 比 D。几乎大部分人会更喜欢 B 和 C。（虽然其实我上课的时候选了 B 和 D 呃呃呃）</p> <p>我们令 \(u(x)\) 表示获得 \(x\times 10^6\) 块钱，于是我们有：</p> \[\begin{cases} u(1)\times 0.11 &lt; u(5)\times 0.1\\ u(1)&gt;u(5)\times 0.1+u(1)\times 0.89 \end{cases}\] <p>而下面一个式子化简一下能得到：</p> \[u(1)\times 0.11 &gt; u(5)\times 0.1\] <p>两个式子是矛盾的。</p> <p>这让我们意识到其实在人真正考虑概率的时候，将“小概率发生”和“完全不发生”是分的很明确的。因为上面那个例子很多人时看到 D 项中有 \(1\%\) 的概率拿不到钱而去选 C 项。解决方法是我们对概率需要加一个修正函数 \(\pi(p)\)，使得我们的 utility function 变为 \(u(x)\times\pi(p)\)。这个 \(\pi\) 函数在 \(p=0\) 的时候会有一个陡增。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-18-Berkeley-STAT155/allais_paradox_pi-480.webp 480w,/assets/img/2024-06-18-Berkeley-STAT155/allais_paradox_pi-800.webp 800w,/assets/img/2024-06-18-Berkeley-STAT155/allais_paradox_pi-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-06-18-Berkeley-STAT155/allais_paradox_pi.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="ambiguity-aversion">Ambiguity Aversion</h3> <p>另一个叫 Ellsberg Paradox 的悖论，我们考虑现在有两个盒子，每个盒子有 100 个红球或黑球。第一个盒子有 50 红 50 黑，第二个盒子啥也不知道。</p> <p>有四个选项：</p> <ul> <li>A: 从第一个盒子里随机抽一个球，如果是红球，拿 100 块钱，否则拿 0 块钱</li> <li>B: 从第一个盒子里随机抽一个球，如果是黑球，拿 100 块钱，否则拿 0 块钱</li> <li>C: 从第二个盒子里随机抽一个球，如果是红球，拿 100 块钱，否则拿 0 块钱</li> <li>D: 从第二个盒子里随机抽一个球，如果是黑球，拿 100 块钱，否则拿 0 块钱</li> </ul> <p>很显然 \(A\sim B, C\sim D\)。尽管 A 和 B 加起来和 C 和 D 加起来是一样的，但大部分人会认为 \(A\succ C, B\succ D\)。</p> <p>这是因为人们不喜欢不确定性，也就是 ambiguity aversion。</p> <h2 id="definition-of-a-game">Definition of a Game</h2> <h3 id="actions-and-preferences">Actions and Preferences</h3> <p>接下来正是讨论 Game Theory 了。</p> <p>之前我们考虑的都是一个人的选择，现在是多人的决策问题，也就是说我们要考虑别人的操作。</p> <p>有 \(N\) 个 agents，每个 agent 有一个 action set</p> \[\mathcal{S}_i = \{s_{1}^i, s_{2}^i, \cdots, s_{N_i}^i\}\] <p>当然后面好像也有很多是写成 \(\mathcal{A}_i\) 的，反正看得懂就行。</p> <p>A strategic game consists of</p> <ul> <li>The players \(\mathcal{N} = \{1, 2, \cdots, N\}\)</li> <li>Actions: \(\mathcal{A}=\mathcal{A_1}\times\cdots\times\mathcal{A}_N\)</li> <li>Preferences: \(\succsim_i\) for each player \(i\)</li> <li>Outcomes</li> </ul> <h3 id="information-sets-and-strategies">Information Sets and Strategies</h3> <p>The set of possible strategy profiles is</p> \[\mathcal{S} = \mathcal{S}_1\times \mathcal{S}_2\times\cdots\times \mathcal{S}_N\] <p>对于一个游戏，在玩的过程中，每个人都会有一个 information \(H\)，对于第 \(i\) 个人所有可能的 information 集合我们记作 information sets \(\mathscr{H}_i\)。玩家只能看到 \(H\) 以内的东西，其他的（比如 \(H\) 以外别人的决策）他是看不见的。</p> <p>对于第 \(i\) 个玩家，假设现在有 information \(H\in\mathscr{H}_i\)，那么定义他可行的方案 \(\mathcal{C}_i(H)\subseteq\mathcal{A}_i\)（对于每个 \(H\) 可行方案肯定是一样的，否则他就有更多 information 了），然后我们定义他的 strategy 为 \(s_i: \mathscr{H}_i\to\mathcal{A}_i\) 并且 \(s_i(H)\in\mathcal{C}_i(H)\)。</p> <p>最终综合每个人的选择，我们有一个 strategy profile:</p> \[s = (s_1, s_2, \cdots, s_N)\in \mathcal{S}\] <p>而很多时候我们会关注某个 player \(i\) 的 strategy，我们一般会将 strategy profile 写作 \((s_i, s_{-i})\)，其中 \(s_{-i}\) 是除了 \(i\) 以外的其他人的 strategy。</p> <h3 id="payoff-functions">Payoff Functions</h3> <p>对于一个游戏，第 \(i\) 个人选了决策 \(s_i\)，我们有 \(s=(s_1, s_2, \cdots, s_N)\in\mathcal{S}\)。</p> <p>这样子我们可以定义第 \(i\) 个人的 utility function 为 \(u_i(s)\)，我们也叫做 payoff function。</p> <p>两个人的时候我们通常写成表格的形式：</p> <table> <thead> <tr> <th> </th> <th>Rock</th> <th>Paper</th> <th>Scissors</th> </tr> </thead> <tbody> <tr> <td><strong>Rock</strong></td> <td>\((0, 0)\)</td> <td>\((-1, 1)\)</td> <td>\((1, -1)\)</td> </tr> <tr> <td><strong>Paper</strong></td> <td>\((1, -1)\)</td> <td>\((0, 0)\)</td> <td>\((-1, 1)\)</td> </tr> <tr> <td><strong>Scissors</strong></td> <td>\((-1, 1)\)</td> <td>\((1, -1)\)</td> <td>\((0, 0)\)</td> </tr> </tbody> </table> <h3 id="strategy-form">Strategy Form</h3> <p>我们定义一个 \(\mathcal{I}\) 个人的 simple game 的 strategy / normal form 为：</p> \[\Gamma = \left&lt;\mathcal{I}, \left\{S_i\right\}, \left\{u_i(\cdot)\right\}\right&gt;\] <h3 id="pure-strategies-and-mixed-strategies">Pure Strategies and Mixed Strategies</h3> <p>Pure strategy 指的是一个人只能选一个 action，而 mixed strategy 指的是一个人可以通过概率分布选多个 actions。</p> <p>假设 \(P_1\) 选的概率为 \(\boldsymbol{p}\)，\(P_2\) 选的为 \(\boldsymbol{q}\)。\(P_1\) 的 payoff 为 \(M\)，那么我们有 \(P_1\) 的 expected payoff：</p> \[U_1(\boldsymbol{p}, \boldsymbol{q}) = \sum_{i\in\mathcal{I}}\sum_{j\in\mathcal{J}}M^{(1)}_{ij}p_iq_j=p^{\top}M_1q\] <p>对于每个人策略一定在一个 simplex 上，我们定义其为 \(\Delta(\mathcal{I})\) 和 \(\Delta(\mathcal{J})\)。</p> <h2 id="zero-sum-games">Zero-Sum Games</h2> <p>最先讨论的是零和博弈。也就是</p> \[\forall s\in \mathcal{S}, \sum_{i=1}^N u_i(s) = 0\] <p>你要获利的唯一方法是让别人变差。</p> <p>对于双人的，我们可以在表中只写第一个人的 payoff：</p> <table> <thead> <tr> <th> </th> <th>Rock</th> <th>Paper</th> <th>Scissors</th> </tr> </thead> <tbody> <tr> <td><strong>Rock</strong></td> <td>\(0\)</td> <td>\(-1\)</td> <td>\(1\)</td> </tr> <tr> <td><strong>Paper</strong></td> <td>\(1\)</td> <td>\(0\)</td> <td>\(-1\)</td> </tr> <tr> <td><strong>Scissors</strong></td> <td>\(-1\)</td> <td>\(1\)</td> <td>\(0\)</td> </tr> </tbody> </table> <p>这样子也就是 \(P_1\) 要最大化 \(P_2\) 要最小化。</p> <p>如果一个游戏不是零和的，我们可以加一个人，让</p> \[u_{N+1}(a_1, a_2, \cdots, a_N) = -\sum_{i=1}^N u_i(a_1, a_2, \cdots, a_N)\] <p>这样子就变成了零和博弈。</p> <h3 id="security-level">Security Level</h3> <p>\(\underline{v}\): \(P_1\) 先来，然后他考虑 \(P_2\) 的最优策略。</p> <p>对于 pure strategies：</p> \[\underline{v} = \max_{s_1\in\mathcal{S}_1}\min_{s_2\in\mathcal{S}_2}u_1(s_1, s_2)\] <p>对于 mixed strategies：</p> \[\underline{v} = \max_{\boldsymbol{p}\in\Delta(\mathcal{I})}\min_{\boldsymbol{q}\in\Delta(\mathcal{J})}p^{\top}Mq\] <p>\(\overline{v}\): \(P_2\) 先来，然后他考虑 \(P_1\) 的最优策略。</p> <p>对于 pure strategies：</p> \[\overline{v} = \min_{s_2\in\mathcal{S}_2}\max_{s_1\in\mathcal{S}_1}u_2(s_1, s_2)\] <p>对于 mixed strategies：</p> \[\overline{v} = \min_{\boldsymbol{q}\in\Delta(\mathcal{J})}\max_{\boldsymbol{p}\in\Delta(\mathcal{I})}p^{\top}Mq\] <p>很显然后手肯定是占优的，因为他知道先手的信息，所以</p> \[\underline{v}\le \overline{v}\] <p>如果 \(\underline{v} = \overline{v}\)，我们定义 \(v=\underline{v}=\overline{v}\) 为 the value of the game。</p> <h3 id="maxmin-and-minmax-strategies">Maxmin and Minmax Strategies</h3> <p>对于 maxmin 和 minmax，我们考虑的是假设我们告诉对手我们的策略（包含概率），对手选择最优 pure strategy。对于一个 \(n\times m\) 的矩阵 \(U\)：</p> \[\begin{aligned} \max\min(U)&amp;=\max_{\boldsymbol{p}\in\Delta[n]}\min_{y\in[m]}\sum_{i\in\mathcal{n}}p_i\cdot U_{i, y}\\ \min\max(U)&amp;=\min_{\boldsymbol{q}\in\Delta[m]}\max_{x\in[n]}\sum_{j\in\mathcal{m}}q_j\cdot U_{x, j} \end{aligned}\] <p>Von Neumann’s Minimax Theorem: 对于零和博弈</p> \[\max\min(U) = \min\max(U)\] <h2 id="dominant-strategies">Dominant Strategies</h2> <h3 id="strictly-dominant-strategies">Strictly Dominant Strategies</h3> <p>对于一个 player，如果不管对方怎么做决策，他做某个决策一定是最优的，那这个决策就是 strictly dominant 的。</p> <p>A strategy \(s_i\in \mathcal{S}_i\) is strictly dominant for player \(i\) if</p> \[\forall s_{-i}\in\mathcal{S}_{-i}, \forall s_i'\in\mathcal{S}_i, u_i(s_i, s_{-i})&gt;u_i(s_i', s_{-i})\] <h3 id="strictly-dominated-strategies">Strictly Dominated Strategies</h3> <p>我们说一个 strategy 是 strictly dominated 的，如果存在另一个 strategy 使得不管对方怎么做，这个 strategy 都比他好。换句话说，就是这个 strategy 一定不会被选。</p> <p>A strategy \(s_i\in \mathcal{S}_i\) is strictly dominated for player \(i\) if</p> \[\exists s_i'\in\mathcal{S}_i, \forall s_{-i}\in\mathcal{S}_{-i}, u_i(s_i', s_{-i})&gt;u_i(s_i, s_{-i})\] <h3 id="weakly-dominated-strategies">Weakly Dominated Strategies</h3> <p>A strategy \(s_i\in \mathcal{S}_i\) is weakly dominated for player \(i\) if</p> \[\exists s_i'\in\mathcal{S}_i, \forall s_{-i}\in\mathcal{S}_{-i}, u_i(s_i', s_{-i})\ge u_i(s_i, s_{-i})\] <h3 id="iterated-elimination-of-strictly-dominated-strategies">Iterated Elimination of Strictly Dominated Strategies</h3> <p>Strictly dominated strategies 一定不会被选，所以我们可以直接把这个策略去掉。然后我们就可以不断找每个人的 strictly dominated strategies，然后去掉来简化游戏。</p> <p>不能用 weekly dominated strategies，因为这些策略还是有可能会被选的。</p> <h3 id="strictly-dominated-strategies-in-mixed-strategies">Strictly Dominated Strategies in Mixed Strategies</h3> <p>有时候尽管 pure strategy 不存在 strictly dominated strategies，考虑 mixed strategies 时可能会存在。</p> <p>比如说</p> <table> <thead> <tr> <th> </th> <th>L</th> <th>R</th> </tr> </thead> <tbody> <tr> <td><strong>U</strong></td> <td>\((10, 1)\)</td> <td>\((0, 0)\)</td> </tr> <tr> <td><strong>M</strong></td> <td>\((4, 2)\)</td> <td>\((4, 1)\)</td> </tr> <tr> <td><strong>D</strong></td> <td>\((0, 5)\)</td> <td>\((10, 2)\)</td> </tr> </tbody> </table> <p>这玩意儿 \(M\) 是被 strictly dominated by \(\frac{1}{2}U+\frac{1}{2}D\) 的。</p> <p>我们说一个 mixed strategy \(\sigma_i\) 是 strictly dominated 的，当且仅当存在 \(\sigma_i'\) 使得</p> \[\forall\sigma_{-i}, u_i(\sigma_i', \sigma_{-i})&gt;u_i(\sigma_i, \sigma_{-i})\] <p>移项然后展开：</p> \[u_i(\sigma_i', \sigma_{-i})-u_i(\sigma_i, \sigma_{-i})=\sum_{-i\in s_{-i}}\left[\prod_{k\neq i}\sigma_{k}(s_k)\right]\left[u_i(\sigma'_i, s_{-i})-u_i(\sigma_i, s_{-i})\right]&gt;0\] <p>我们考虑这个式子，如果所有的 \(u_i(\sigma'_i, s_{-i})-u_i(\sigma_i, s_{-i})&gt;0\) 的话，很显然整个式子是正的，如果其中有一个小于 \(0\)，我们就让这一项前面的 \(\sigma_k(s_k)=1\)，其他都变成 \(0\)，这样整个式子就小于 \(0\) 了。</p> <p>于是乎对于一个 \(\sigma_i\)，我们只要 check 所有的 pure strategies 就可以。</p> <h2 id="knowledge">Knowledge</h2> <p><strong>Mutual Knowledge</strong>: 如果大家都知道某个 knowledge，那么这个 knowledge 是 mutual knowledge。</p> <p><strong>Common Knowledge</strong>: 如果任意一个 player 序列 \(i_1, i_2, \cdots, i_k\)，我们有 \(i_1\) 知道 \(i_2\) 知道 \(\cdots\) 知道 \(i_k\) 知道某个 knowledge，那么我们说这个 knowledge 是 common knowledge。</p> <p>很显然 common knowledge 是 mutual knowledge。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-18-Berkeley-STAT155/common_knowledge-480.webp 480w,/assets/img/2024-06-18-Berkeley-STAT155/common_knowledge-800.webp 800w,/assets/img/2024-06-18-Berkeley-STAT155/common_knowledge-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-06-18-Berkeley-STAT155/common_knowledge.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>假设人从左到右是 \(A, B, C\)，\(A\) 不知道 \(A\) 的帽子颜色。我们现在假定 player 序列是 \(A, B, C\)。对于 \(A\) 来说，假设 \(A\) 的帽子颜色是红的，那么 \(B\) 就能看到俩红，那他就能知道 \(C\) 肯定能看到至少一红。但是如果 \(A\) 的帽子不是红色的，那么 \(B\) 只能看到一红，他就不能确定 \(C\) 是否能看到红色了。这导致了这道题不是 common knowledge。</p> <h2 id="nash-equilibrium">Nash Equilibrium</h2> <h3 id="best-response">Best Response</h3> <p>We say that a strategy \(\sigma_i\) is a best response to \(\sigma_{-i}\) if</p> \[\forall\sigma'_i\in\Delta(\mathcal{S}_i), u_i(\sigma_i, \sigma_{-i})\ge u_i(\sigma'_i, \sigma_{-i})\] <p><strong><em>Best response correspondence</em></strong>:</p> \[b_i(\sigma_{-i})=\left\{\sigma_i\in\Delta(\mathcal{S}_i): \forall \sigma_i'\in\Delta(\mathcal{S}_i), u_i(\sigma_i, \sigma_{-i})\ge u_i(\sigma_i', \sigma_{-i})\right\}\] <h3 id="nash-equilibrium-in-pure-strategies">Nash Equilibrium in Pure Strategies</h3> <p>每个人都是 best response 的策略。</p> <p>A strategy profile \(s=(s_1, \cdots, s_\mathcal{I})\) constitutes a Nash equilibrium of a game \(\Gamma=\left[\mathcal{I}, \left\{S_i\right\}, \left\{u_i(\cdot)\right\}\right]\) if for every \(i=1, \cdots, \mathcal{I}\),</p> \[u_i(s_i, s_{-i})\ge u_i(s_i', s_{-i})\] <p>for all \(s_i'\in S_i\).</p> <h3 id="nash-equilibrium-in-mixed-strategies">Nash Equilibrium in Mixed Strategies</h3> <p>和 pure strategies 的定义是一样的，对于游戏 \(\Gamma=\left[\mathcal{I}, \left\{\Delta(\mathcal{S}_i)\right\}, \left\{u_i(\cdot)\right\}\right]\), 我们有 \(\sigma = (\sigma_1, \cdots, \sigma_\mathcal{I})\) 是 Nash equilibrium if for every \(i=1, \cdots, \mathcal{I}\),</p> \[u_i(\sigma_i, \sigma_{-i})\ge u_i(\sigma_i', \sigma_{-i})\] <p>for all \(\sigma_i'\in\Delta(\mathcal{S}_i)\).</p> <p>其实也就是</p> \[\forall i, \sigma_i\in b_i(\sigma_{-i})\] <p>所以直觉上来讲其实就是这些 correspondence 的交集。</p> <p>比如说这个游戏：</p> <table> <thead> <tr> <th> </th> <th>Bach (\(\mathscr{B}_2\))</th> <th>Stravinsky (\(\mathscr{S}_2\))</th> </tr> </thead> <tbody> <tr> <td><strong>Bach</strong> (\(\mathscr{B}_1\))</td> <td>\((10, 5)\)</td> <td>\((0, 0)\)</td> </tr> <tr> <td><strong>Stravinsky</strong> (\(\mathscr{S}_1\))</td> <td>\((0, 0)\)</td> <td>\((5, 10)\)</td> </tr> </tbody> </table> <p>我们有对于 \(P_1\)：</p> \[\mathscr{B}_1\succsim\mathscr{S}_1\Longleftrightarrow p(\mathscr{B}_2)\ge\frac{1}{3}\] <p>同理，对称地对于 \(P_2\)：</p> \[\mathscr{B}_2\succsim\mathscr{S}_2\Longleftrightarrow p(\mathscr{B}_1)\ge\frac{2}{3}\] <p>画出图：</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-18-Berkeley-STAT155/nash_equilibrium_bach-480.webp 480w,/assets/img/2024-06-18-Berkeley-STAT155/nash_equilibrium_bach-800.webp 800w,/assets/img/2024-06-18-Berkeley-STAT155/nash_equilibrium_bach-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-06-18-Berkeley-STAT155/nash_equilibrium_bach.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>交点为 \((0, 0), \left(\frac{1}{3}, \frac{2}{3}\right), (1, 1)\)，所以这三个点是 Nash equilibrium。</p> <h3 id="checking-for-nash-equilibrium">Checking for Nash Equilibrium</h3> <p>我们考虑游戏 \(\Gamma=\left[\mathcal{I}, \left\{\Delta(\mathcal{S}_i)\right\}, \left\{u_i(\cdot)\right\}\right]\)。对于一个 \(\sigma=(\sigma_1, \cdots, \sigma_\mathcal{I})\)，我们定义 \(\mathcal{S}_i^+\subseteq\mathcal{S}_i\)：</p> \[\mathcal{S}_i^+=\left\{s_j\in\mathcal{S}_i: \sigma_{i,j}&gt;0\right\}\] <p>也就是这个人有可能执行这个操作。</p> <p>于是乎一个 \(\sigma\) 要满足他是 Nash equilibrium 当且仅当对于 \(i=1, \cdots, \mathcal{I}\)：</p> <ol> <li>\(u_i(s_i, \sigma_{-i})=u_i(s_i', \sigma_{-i})\) for all \(s_i, s_i'\in\mathcal{S}_i^+\)</li> <li>\(u_i(s_i, \sigma_{-i})\ge u_i(s_i', \sigma_{-i})\) for all \(s_i\in\mathcal{S}_i^+\) and \(s_i'\in\mathcal{S}_i\setminus\mathcal{S}_i^+\)</li> </ol> <p>大概就是，对于我来说，我知道别人有一个 mixed strategy，而我出的这个 mixed strategy 中的任何一种可能性都是一样优的（否则我就不要他了），而我不选的那几个操作一定不会更优（否则我肯定会增加它的概率）。如果对于每个人来说都是这样，那么就达到了一种均衡。</p> <h3 id="existence-of-nash-equilibrium">Existence of Nash Equilibrium</h3> <p>A Nash equilibrium exists in game \(\Gamma=\left[\mathcal{I}, \left\{\mathcal{S}_i\right\}, \left\{u_i(\cdot)\right\}\right]\) if for all \(i=1, \cdots, \mathcal{I}\):</p> <ol> <li>\(\mathcal{S}_i\) is a nonempty, convex, and compact subset of some Euclidean space \(\mathbb{R}^M\);</li> <li>\(u_i(s_1, \cdots, s_\mathcal{I})\) is continuous in \((s_1, \cdots, s_\mathcal{I})\) and quasi-concave in \(s_i\).</li> </ol> <p>考虑到当 \(\mathcal{S}\) 有限的时候，\(\Delta(\mathcal{S})\) 满足了 nonempty, convex, compact 三个条件，而且 \(u_i\) 也是 continuous 和 quasi-concave 的。所以我们说对于 mixed strategies game \(\Gamma=\left[\mathcal{I}, \left\{\Delta(\mathcal{S}_i)\right\}, \left\{u_i(\cdot)\right\}\right]\)，如果 \(\mathcal{S}_i\) 是有限的，那么 Nash equilibrium 一定存在。</p> <h3 id="correlated-equilibria">Correlated Equilibria</h3> <p>前面我们考虑 \(\sigma_i\) 都是独立的。但在现实生活中，一些信息是共享的，导致我们的策略是相关的。</p> <p>比如说红绿灯，大家会看到信号灯的信息来做出决策。</p> <p>这时候我们就要定义：</p> \[U_i(\sigma_1, \cdots, \sigma_\mathcal{I})=\sum_{s}\mathbb{P}[s_1, \cdots, s_\mathcal{I}]u_i(s_1, \cdots, s_\mathcal{I})\] <p>而这时候我们的 strategy 概率就应该是定义在 \(\mathcal{S}=S_1\times\cdots\times S_\mathcal{I}\) 上了，也就是描述 NE 需要考虑 joint distribution。</p> <h3 id="the-oddness-theorem">The Oddness Theorem</h3> <p>一个神奇的结论是在大部分情况下，一个游戏一定有奇数个 NE。</p> <p>严谨的说，拥有偶数个 NE 的游戏所构成的集合 Lebesgue measure 为 \(0\)。</p> <p>比如说有一个游戏：</p> <table> <thead> <tr> <th> </th> <th>L</th> <th>R</th> </tr> </thead> <tbody> <tr> <td><strong>U</strong></td> <td>\((a_1, a_2)\)</td> <td>\((b_1, b_2)\)</td> </tr> <tr> <td><strong>D</strong></td> <td>\((c_1, c_2)\)</td> <td>\((d_1, d_2)\)</td> </tr> </tbody> </table> <p>那么我们对于任意 \(\epsilon&gt;0\)，存在 \(0\le \epsilon_1, \cdots, \epsilon_8\le \epsilon\) 使得游戏：</p> <table> <thead> <tr> <th> </th> <th>L</th> <th>R</th> </tr> </thead> <tbody> <tr> <td><strong>U</strong></td> <td>\((a_1+\epsilon_1, a_2+\epsilon_2)\)</td> <td>\((b_1+\epsilon_3, b_2+\epsilon_4)\)</td> </tr> <tr> <td><strong>D</strong></td> <td>\((c_1+\epsilon_5, c_2+\epsilon_6)\)</td> <td>\((d_1+\epsilon_7, d_2+\epsilon_8)\)</td> </tr> </tbody> </table> <p>拥有奇数个 NE。</p> <p>证明没有详细地讲，只说了个大概的思路。主要是考虑一个 \(f: X\to X\) 的连续函数，不动点在大部分情况下有奇数个。除非与 \(y=x\) 相切或者端点在 \((x, x)\) 上，这种情况下稍稍移动一下函数就可以了。</p> <h2 id="welfare--optimality">Welfare / Optimality</h2> <p>很多时候我们需要知道什么是“好的”，因为有时候 Nash equilibrium 并不是最优的。</p> <h3 id="social-welfare">Social Welfare</h3> <p>最直观的方法是定义一个函数来表示整个社会的 welfare。我们定义 social welfare function 为</p> \[\mathcal{W}(x_1, \cdots, x_\mathcal{I})=\sum_{i=1}^\mathcal{I}\alpha_i u_i(x_i, x_{-i})\] <p>但这样做有两个很显然的问题：</p> <ol> <li>谁来定义 \(\alpha_i\)？</li> <li>是否所有人都能 accept 这个定义？</li> </ol> <p>所以我们下面引出 Pareto Optimality。</p> <h3 id="pareto-optimality">Pareto Optimality</h3> <p>假设我们有两个 agents 和两种物品进行分配，第一个物品有 \(A\) 个，第二个物品有 \(B\) 个。那么我们可以定义 allocation 为：</p> \[\left((x_1^a, x_2^a), (x_1^b, x_2^b)\right)\] <p>而物品的数量有限，所以：</p> \[\begin{cases} x_1^a+x_1^b=A\\ x_2^a+x_2^b=B \end{cases}\] <p>我们定义一个 Pareto Improvement \(\left((y_1^a, y_2^a), (y_1^b, y_2^b)\right)\) 满足：</p> \[\begin{bmatrix} u_a(y_1^a, y_2^a)\\ u_b(y_1^b, y_2^b) \end{bmatrix}&gt; \begin{bmatrix} u_a(x_1^a, x_2^a)\\ u_b(x_1^b, x_2^b) \end{bmatrix}\] <p>其中的 \(&gt;\) 指的是有一个大于其余大于等于。</p> <p>如果一个 allocation 没有 Pareto Improvement，那么我们称其为 Pareto Optimal。</p> <p>然而 Pareto Optimal 也不一定是 fair 的，但是他是 bare minimum，也就是说如果不是 Pareto Optimal，那么我们应该做出改进。</p> <p>上课提到了一个叫 laissez-fairez 的名词，其实就是自由放任。让市场自由调节来达到一个 Pareto Optimal 的状态。</p> <h3 id="mechanism--market-design">Mechanism / Market Design</h3> <p>另一种让社会达到最优状态的方法叫 mechanism design，也叫 reverse game theory。我们通过设计和修改游戏机制来打到社会的最优性。</p> <h2 id="rationalizable-strategies">Rationalizable Strategies</h2> <p>我们考虑一个游戏，如果对方做出某个选择，我们会做出相应的 best response。但是有些策略不管对方选了什么，我们都不可能作为 best response 去选择，我们把这种策略叫做 NBR（never best response）。</p> <p>那很显然 NBR 是可以忽略的，所以我们就重复在游戏中删除 NBR，直到没有 NBR 为止。</p> <p>最终剩下来的策略就是 rationalizable strategies。</p> <p>如果最终只剩了一个，那么我们说这个游戏是 dominance solvable 的。</p> <p>很显然 NE 肯定是 rationalizable 的，因为如果他不是 rationalizable，那肯定在某一次删除的时候被定义为 NBR 了，而那个 best response 肯定比这个好。</p> <p>但是 rationalizable 不一定是 NE 的，比如说 Matching Pennies：</p> <table> <thead> <tr> <th> </th> <th>Heads</th> <th>Tails</th> </tr> </thead> <tbody> <tr> <td><strong>Heads</strong></td> <td>\((1, -1)\)</td> <td>\((-1, 1)\)</td> </tr> <tr> <td><strong>Tails</strong></td> <td>\((-1, 1)\)</td> <td>\((1, -1)\)</td> </tr> </tbody> </table> <p>当然，NBR 也可能不是 strictly dominated by pure 的：</p> <table> <thead> <tr> <th> </th> <th>X</th> <th>Y</th> </tr> </thead> <tbody> <tr> <td><strong>A</strong></td> <td>\((2, 1)\)</td> <td>\((0, 0)\)</td> </tr> <tr> <td><strong>B</strong></td> <td>\((0, 1)\)</td> <td>\((2, 0)\)</td> </tr> <tr> <td><strong>C</strong></td> <td>\((1, 1)\)</td> <td>\((1, 2)\)</td> </tr> </tbody> </table> <p>尽管我们考虑 pure strategies，\(b_1(X)=\{A\}, b_1(Y)=\{B\}\)，\(C\) 是不会被选的。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-18-Berkeley-STAT155/rationalizable-480.webp 480w,/assets/img/2024-06-18-Berkeley-STAT155/rationalizable-800.webp 800w,/assets/img/2024-06-18-Berkeley-STAT155/rationalizable-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-06-18-Berkeley-STAT155/rationalizable.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="games-with-trembling-hands">Games with Trembling Hands</h2> <h3 id="trembling-hand-perfect-equilibrium">Trembling Hand Perfect Equilibrium</h3> <p>有时候我们做选择会手抖出错。也就其实是我们在选操作 \(i\) 的时候，我们其实选的是一个概率分布 \(\sigma_i\)。</p> <p>We say that a strategy profile \(\sigma\) is a trembling-hand perfect Nash Equilibrium if it can be approximated by a sequence of totally mixed strategy profiles for each player. 其中的 totally mixed 指的是所有概率都大于 \(0\)。也就是说，对于一个 pure strategy，稍稍扰动一下也是没有问题的。</p> <p>就比如下面这个例子：</p> <table> <thead> <tr> <th> </th> <th>U</th> <th>D</th> </tr> </thead> <tbody> <tr> <td><strong>U</strong></td> <td>\((1, 1)\)</td> <td>\((0, -3)\)</td> </tr> <tr> <td><strong>D</strong></td> <td>\((-3, 0)\)</td> <td>\((0, 0)\)</td> </tr> </tbody> </table> <p>\((D, D)\) 是 NE 但不是 THNE，因为如果稍微扰动一下，变成 \(((\epsilon_1, 1-\epsilon_1), (\epsilon_2, 1-\epsilon_2))\)，这时候是不如 \(((1-\epsilon_1, \epsilon_1), (\epsilon_2, 1-\epsilon_2))\) 的。也就是说，在有 trembling hand 的情况下，\(P_1\) 和 \(P_2\) 其实会考虑换到 \(U\)。</p> <h3 id="evolutionary-stable">Evolutionary Stable</h3> <p>当然根据这个定义，所有 totally mixed strategies 都是 THNE 的。但是我们可以沿用这个 idea 来定义和 “stability”。我们把一个 NE 扰动一下，如果还是 NE，那么我们就说这个 NE 是 stable 的。我们定义这个东西叫做 evolutionary stable。</p> <p>A mixed strategy profile \(\sigma^*\) is Evolutionary Stable if:</p> <ol> <li>\(u_i(\sigma_i^*, \sigma_{-i}^*)\ge u_i(\sigma_i, \sigma_{-i}^*)\) for all \(i\) and \(\sigma_i\in\Delta(\mathcal{S}_i)\)</li> <li>if \(u_i(\sigma_i^*, \sigma_{-i}^*)=u_i(\sigma_i, \sigma_{-i}^*)\), then \(u_i(\sigma_i^*, \sigma_{-i})&gt;u_i(\sigma_i, \sigma_{-i})\)</li> </ol>]]></content><author><name></name></author><category term="Notes"/><category term="Game Theory"/><summary type="html"><![CDATA[Notes for UC Berkeley STAT 155 Game Theory + NTU MH4320 Computational Economics]]></summary></entry><entry><title type="html">UC Berkeley CS 161 Computer Security</title><link href="https://pufanyi.github.io/blog/Berkeley-CS161/" rel="alternate" type="text/html" title="UC Berkeley CS 161 Computer Security"/><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://pufanyi.github.io/blog/Berkeley-CS161</id><content type="html" xml:base="https://pufanyi.github.io/blog/Berkeley-CS161/"><![CDATA[<p>居然有幸能来线下上课。</p> <p>这不记个笔记。</p> <p><a href="https://su24.cs161.org/">Summer 2024</a></p> <hr/> <h2 id="lecture-1-introduction-and-security-principles">Lecture 1: Introduction and Security Principles</h2> <h3 id="security-principles">Security Principles</h3> <ol> <li><strong>Know your threat model</strong>: 谁会来攻击你，为什么要攻击你。Threat Model 指的就是对攻击者建立一个合适的模型（比如他们有多少资源，他们的动机）。<em>You often just need to have “good enough” defense to make attackers turn somewhere else.</em></li> <li><strong>Consider Human Factors</strong>：考虑用户习惯，别让用户太麻烦。</li> <li><strong>Security is economics</strong>：搞 security 需要米，所以要考虑你保护的是啥。</li> <li><strong>Detect if You Can’t Prevent</strong>: <ul> <li><strong>Deterrence</strong>: Stop the attack before it happens.</li> <li><strong>Prevention</strong>: Stop the attack as it happens.</li> <li><strong>Detection</strong>: Learn that there was an attack (after it happened).</li> <li><strong>Response</strong>: Do something about the attack (after it happened). 对于 response，需要做的事 Mitigation and Recovery，就是比如在发现勒索软件的时候赶紧进行文件备份和转移。</li> </ul> </li> <li><strong>Defense in depth</strong>：多层防御，但是要考虑成本。</li> <li><strong>Least Privilege</strong>：然鹅其实现在的操作系统做的都很遭糕。</li> <li><strong>Ensure complete mediation</strong>：You should check <em>every</em> access to <em>every</em> object. 课上提出了一个叫做 <em>Reference Monitor</em> 的概念，就是一个任何 access 操作都必须经过的结点，比如说 network firewall。用这种方法来保证每个 access 都被检查过。</li> <li><strong>Separation of Responsibility</strong>：例子是原子弹发射，要两个人同时操作才行。</li> <li><strong>Shannon’s Maxim</strong>：你不能依赖于 Security Through Obscurity，也就是通过源代码的保密性来保护系统。想到 Kerckhoff’s Principle？</li> <li><strong>Use Fail-Safe Defaults</strong>：当系统崩溃的时候，应该让系统保持最安全的状态。比如说一些防盗门在断电的时候要自动关上。</li> <li><strong>Design in Security from the Start</strong></li> </ol> <h3 id="the-trusted-computing-base-tcb">The Trusted Computing Base (TCB)</h3> <p>TCB 指的是 The components of a system that security relies upon。</p> <p><strong>TCB Design Principles</strong>:</p> <ol> <li><strong>Unbypassable</strong> (or <strong>completeness</strong>)</li> <li><strong>Tamper-resistant</strong> (or <strong>security</strong>)：不能改，必须保证 TCB 的完整性</li> <li><strong>Verifiable</strong> (or <strong>correctness</strong>)：TCB 应该设计得越小越好（<strong>KISS principle</strong>: Keep It Simple, Stupid）</li> </ol> <p>这种让 TCB 和其他系统分离出来的方法让我们能够更方便的 focus on one thing。</p> <h3 id="tocttou-vulnerabilities">TOCTTOU Vulnerabilities</h3> <p>在讲 Ensure Complete Mediation 的时候提到了一个例子挺有意思的，叫做 The time of check to time of use (TOCTTOU) vulnerability。</p> <p>就是考虑一个 ATM 机提款的操作。假设现在你的银行账户里有 \(1000\) 块钱。一般情况下很直觉地会这样写提款程序：</p> <pre><code class="language-pseudocode">\begin{algorithm}
\caption{Withdrawal}
\begin{algorithmic}
\PROCEDURE{Withdrawal}{$w$}
  \STATE $b =$ \CALL{GetBalance}{} \COMMENT{Step (1)}
    \IF{$b &lt; w$}
      \STATE Abort
    \ENDIF
    \STATE \CALL{SetBalance}{$b - w$} \COMMENT{Step (2)}
    \STATE \CALL{DispenseCash}{$w$}
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre> <p>但这个程序事实上有一个巨大的漏洞，就是假设我现在开两个并行程序，两次都取 \(1000\) 块钱，当第一个程序运行到 <code class="language-plaintext highlighter-rouge">(2)</code> 之前时，我让第二个程序运行到 <code class="language-plaintext highlighter-rouge">(1)</code>。这时候我们会发现两个程序都能通过 <code class="language-plaintext highlighter-rouge">(1)</code> 进入 <code class="language-plaintext highlighter-rouge">(2)</code>，钱就会被取两次。这样我们能从 \(1000\) 块钱的账户中取出 \(2000\) 块钱。</p> <hr/> <h2 id="lecture-2-x86-assembly-and-call-stack">Lecture 2: x86 Assembly and Call Stack</h2> <h3 id="number-representation">Number Representation</h3> <ul> <li>nibble 是一个十六进制数的大小，1 nibble = 4 bits</li> <li>1 byte = 8 bits</li> <li>word 是指针的大小，32 位下是 32 bits，64 位下是 64 bits</li> </ul> <h3 id="call-compiler-assembler-linker-loader">CALL: Compiler, Assembler, Linker, Loader</h3> <ul> <li><strong>Compiler</strong>: 高级语言 -&gt; Assembly Code</li> <li><strong>Assembler</strong>: Assembly Code -&gt; Machine Code</li> <li><strong>Linker</strong>: Deals with dependencies and libraries</li> <li><strong>Loader</strong>: Sets up memory space and runs the machine code</li> </ul> <h3 id="c-memory-layout">C Memory Layout</h3> <p>讲课的时候考虑的是 32 位机，也就是 memory 是从 <code class="language-plaintext highlighter-rouge">0x00000000</code> 到 <code class="language-plaintext highlighter-rouge">0xFFFFFFFF</code>。其实可以把内存看作一个一维的数组，当然我们通常将其画成一张 \(n\times 4 \text{ bytes}\) 的表。</p> <pre><code class="language-typograms">                          4 bytes
                      |&lt;-----------&gt;|
     0xFFFFFFFF       +-------------+
--------------------&gt; |             |
                      |             |
   Higher Address     |             |
         ^            |             |
         |            |             |
         |            |             |
                      |   Memory    |
                      |             |
         |            |             |
         |            |             |
         v            |             |
    Lower Address     |             |
                      |             |
--------------------&gt; |             |
     0x00000000       +-------------+
                      --------------&gt;
                           index
</code></pre> <p>x86 中都是以 Little-endian 存储的，也就是说比如说一个东西是 <code class="language-plaintext highlighter-rouge">0x0123456789abcdef</code>，那么他在地址中应该存储为：</p> <pre><code class="language-typograms">+---------------------------+
| 0x67 | 0x45 | 0x23 | 0x01 |
+------+------+------+------+
| 0xef | 0xcd | 0xab | 0x89 |
+---------------------------+
</code></pre> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;stdint.h&gt;</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">union</span> <span class="p">{</span>
        <span class="kt">uint64_t</span> <span class="n">num_int64</span><span class="p">;</span>
        <span class="kt">unsigned</span> <span class="kt">char</span> <span class="n">num_char</span><span class="p">[</span><span class="mi">8</span><span class="p">];</span>
    <span class="p">}</span> <span class="n">num</span><span class="p">;</span>
    <span class="n">num</span><span class="p">.</span><span class="n">num_int64</span> <span class="o">=</span> <span class="mh">0x0123456789abcdef</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"%02x "</span><span class="p">,</span> <span class="n">num</span><span class="p">.</span><span class="n">num_char</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>输出：</p> <pre><code class="language-plain">ef cd ab 89 67 45 23 01
</code></pre> <p>但是数组和 <code class="language-plaintext highlighter-rouge">struct</code> 仍然是从小到大的顺序。</p> <p>然后 Memory 按照一下几个块分：</p> <pre><code class="language-typograms">+-------------------+
|       Stack       |
+~~~~~~~~~~~~~~~~~~~+
|         |         |
|         v         |
|                   |
|         ^         |
|         |         |
+~~~~~~~~~~~~~~~~~~~+
|       Heap        |
+-------------------+
|       Data        |
+-------------------+
|       Code        |
+-------------------+
</code></pre> <h3 id="function-call">Function Call</h3> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">foo</span><span class="p">(</span><span class="kt">int</span><span class="p">,</span> <span class="kt">int</span><span class="p">);</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">foo</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">foo</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="n">b</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <pre><code class="language-assembly">main:
  pushq $2
  pushq $1

  call  foo


foo:
  movq  %rsp, %rbp
  subq  $32, %rsp
</code></pre> <p><code class="language-plaintext highlighter-rouge">leave</code> 等价于</p> <pre><code class="language-ass">mov   %ebp, %esp
pop   %ebp
</code></pre> <hr/> <h2 id="lecture-3-memory-safety-vulnerabilities">Lecture 3: Memory Safety Vulnerabilities</h2> <h3 id="buffer-overflow">Buffer Overflow</h3> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">vulnerable</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">char</span> <span class="n">name</span><span class="p">[</span><span class="mi">20</span><span class="p">];</span>
    <span class="n">gets</span><span class="p">(</span><span class="n">name</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">gets</code> 时将 <code class="language-plaintext highlighter-rouge">SHELLCODE</code> 写入内存，然后覆盖 <code class="language-plaintext highlighter-rouge">RIP</code> 使其指向他。</p> <pre><code class="language-typograms">+---------------+             +-------------------------+
|      ...      |             |          ...            |
+---------------+             +-------------------------+
|      RIP      |             |     (RIP)  &amp;SHELLCODE   |------+
+---------------+             +-------------------------+      |
|      SFP      |             |     (SFP)  'AAAA'       |      |
+---------------+             +-------------------------+      |
|     name      |    gets     |     (name) 'AAAA'       |      |
+---------------+  --------&gt;  +-------------------------+      |
|     name      |             |     (name) 'AAAA'       |      |
+---------------+             +-------------------------+      |
|     name      |             |     (name) SHELLCODE    |      |
+---------------+             +-------------------------+      |
|     name      |             |     (name) SHELLCODE    |      |
+---------------+             +-------------------------+      |
|     name      |             |     (name) SHELLCODE    |&lt;-----+
+---------------+             +-------------------------+
</code></pre> <p>当然 SHELLCODE 也可以写在其他地方比如说 <code class="language-plaintext highlighter-rouge">RIP</code> 上面这个随心情而定。</p> <pre><code class="language-diff2html">diff --git a/vulnerable.c b/vulnerable.c
--- a/vulnerable.c
+++ b/vulnerable.c
@@ -1,4 +1,4 @@
 void vulnerable(void) {
     char name[20];
-    gets(name);
+    fgets(name, 20, stdin);
 }
</code></pre> <p>实操的时候，看 RIP 和 SFP：</p> <pre><code class="language-gdb">(gdb) info frame
Stack level 0, frame at 0x7fffffffd9a0:
 rip = 0x555555555155 in foo (test.c:8); saved rip = 0x555555555140
 called by frame at 0x7fffffffd9b0
 source language c.
 Arglist at 0x7fffffffd990, args: a=1, b=2
 Locals at 0x7fffffffd990, Previous frame's sp is 0x7fffffffd9a0
 Saved registers:
  rbp at 0x7fffffffd990, rip at 0x7fffffffd998
</code></pre> <p>其中 <code class="language-plaintext highlighter-rouge">Saved registers</code> 中 <code class="language-plaintext highlighter-rouge">rbp</code> 指的是上课讲的 <code class="language-plaintext highlighter-rouge">sfp</code>，<code class="language-plaintext highlighter-rouge">rip</code> 是 <code class="language-plaintext highlighter-rouge">rip</code>，不太一样的是显示的 <code class="language-plaintext highlighter-rouge">rip</code> 是因为机器是 64 位的，是 register 里的 <code class="language-plaintext highlighter-rouge">rip</code>，如果是 32 位会显示 <code class="language-plaintext highlighter-rouge">ebp</code> 和 <code class="language-plaintext highlighter-rouge">eip</code>。这俩相差 $8$ bytes 正好一个 word。</p> <p>一些 vulnerable 的函数：</p> <ol> <li><code class="language-plaintext highlighter-rouge">gets</code>：改成 <code class="language-plaintext highlighter-rouge">fgets</code></li> <li><code class="language-plaintext highlighter-rouge">strcpy</code>：改成 <code class="language-plaintext highlighter-rouge">strncpy</code>（more compatible, less safe）或者 <code class="language-plaintext highlighter-rouge">strlcpy</code>（less compatible, more safe）</li> <li><code class="language-plaintext highlighter-rouge">strlen</code>：改成 <code class="language-plaintext highlighter-rouge">strnlen</code> 或者 <code class="language-plaintext highlighter-rouge">memchr</code></li> </ol> <h3 id="off-by-one-exploit">Off-by-One Exploit</h3> <p>这是很多初学者经常犯的错误就是</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">vulnerable</span><span class="p">()</span> <span class="p">{</span>
  <span class="kt">char</span> <span class="n">s</span><span class="p">[</span><span class="mi">32</span><span class="p">];</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="mi">32</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">scanf</span><span class="p">(</span><span class="s">"%c"</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">vulnerable</span><span class="p">();</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>这样允许了我们有一个额外的输入。然而这个输入最多只能允许我们部分控制 <code class="language-plaintext highlighter-rouge">SFP</code>。</p> <p>我们需要这样修改：</p> <pre><code class="language-typograms">+--------------------------+                +---------------------------+
|       RIP of main        |                |        RIP of main        |
+--------------------------+                +---------------------------+
|       SFP of main        |&lt;---+           |        SFP of main        |
+--------------------------+    |           +---------------------------+
|    RIP of vulnerable     |    |           |     RIP of vulnerable     |
+--------------------------+    |           +---------------------------+
|    SFP of vulnerable     |----+           |   Fake SFP of vulnerable  |----+
+--------------------------+                +---------------------------+    |
|          x[2]            |      gets      |       Garbage bytes       |    |
+--------------------------+  -----------&gt;  +---------------------------+    |
|          x[1]            |                |      Fake RIP of main     |----+---&gt; SHELLCODE
+--------------------------+                +---------------------------+    |
|          x[0]            |                |      Fake SFP of main     |&lt;---+
+--------------------------+                +---------------------------+
</code></pre> <hr/> <h2 id="homework-1">Homework 1</h2> <p>也记录一下做 Homework 时学到的东西。</p> <h3 id="gdb">GDB</h3> <pre><code class="language-gdb">(gdb) x/4xw buf
0xbffffdf8:  0xbffffeac  0xb7ffc165  0x00000000  0x00000000
</code></pre> <p><code class="language-plaintext highlighter-rouge">0xbffffdf8</code> 是 <code class="language-plaintext highlighter-rouge">buf</code> 的地址，<code class="language-plaintext highlighter-rouge">4xw</code> 显示了 $4$ 个 word 的内容。</p> <pre><code class="language-gdb">(gdb) layout split
</code></pre> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-17-Berkeley-CS161/layout_split-480.webp 480w,/assets/img/2024-06-17-Berkeley-CS161/layout_split-800.webp 800w,/assets/img/2024-06-17-Berkeley-CS161/layout_split-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-06-17-Berkeley-CS161/layout_split.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>离开：<code class="language-plaintext highlighter-rouge">Ctrl + X</code> 再按 <code class="language-plaintext highlighter-rouge">A</code>。</p> <p>切换窗口：<code class="language-plaintext highlighter-rouge">Ctrl + X</code> 再按 <code class="language-plaintext highlighter-rouge">O</code>。</p> <p>看汇编代码</p> <pre><code class="language-gdb">(gdb) disas main
Dump of assembler code for function main:
   0x00001629 &lt;+0&gt;:     lea    0x4(%esp),%ecx
   0x0000162d &lt;+4&gt;:     and    $0xfffffff0,%esp
   0x00001630 &lt;+7&gt;:     push   -0x4(%ecx)
   0x00001633 &lt;+10&gt;:    push   %ebp
   0x00001634 &lt;+11&gt;:    mov    %esp,%ebp
   0x00001636 &lt;+13&gt;:    push   %ebx
   0x00001637 &lt;+14&gt;:    push   %ecx
   0x00001638 &lt;+15&gt;:    sub    $0x20,%esp
   0x0000163b &lt;+18&gt;:    call   0x1142 &lt;__x86.get_pc_thunk.bx&gt;
   ...
</code></pre>]]></content><author><name></name></author><category term="Notes"/><category term="Computer Security"/><summary type="html"><![CDATA[Notes for UC Berkeley CS 161 Computer Security]]></summary></entry><entry><title type="html">2021 Zhejiang Gao Kao</title><link href="https://pufanyi.github.io/blog/GaoKao/" rel="alternate" type="text/html" title="2021 Zhejiang Gao Kao"/><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://pufanyi.github.io/blog/GaoKao</id><content type="html" xml:base="https://pufanyi.github.io/blog/GaoKao/"><![CDATA[<h1 id="导数大题">导数大题</h1> <h2 id="question">Question</h2> <p>Let \(a, b\) be real numbers, and \(a&gt;1\). Consider the function \(f(x)=a^x-bx+e^2\) (\(x\in\mathbb{R}\)).</p> <ol> <li>Find the monotonic intervals of the function \(f(x)\).</li> <li>If for any \(b&gt;2e^2\), the function \(f(x)\) has two different zeros, find the range of \(a\).</li> <li>When \(a=e\), prove that: for any \(b&gt;e^4\), the function \(f(x)\) has two different zeros \(x_1, x_2\), satisfying \(x_2&gt;\frac{b\ln b}{2e^2}x_1 + \frac{e^2}{b}\).</li> </ol> <p><strong>Note</strong>:</p> <ol> <li>\(e=2.71828\cdots\) is the base of the natural logarithm.</li> <li>We define the natural logarithm as \(\ln x\), which is \(\ln x=\log_e x\).</li> </ol> <details><summary>中文原题</summary> <p>设 \(a, b\) 为实数，且 \(a&gt;1\)，函数 \(f(x)=a^x-bx+e^2\) (\(x\in\mathbb{R}\))</p> <ol> <li>求函数 \(f(x)\) 的单调区间；</li> <li>若对任意 \(b&gt;2e^2\)，函数 \(f(x)\) 有两个不同的零点，求 \(a\) 的取值范围；</li> <li>当 \(a=e\) 时，证明：对任意 \(b&gt;e^4\)，函数 \(f(x)\) 有两个不同的零点 \(x_1, x_2\)，满足 \(x_2&gt;\frac{b\ln b}{2e^2}x_1 + \frac{e^2}{b}\)。</li> </ol> <p>注：\(e=2.71828\cdots\) 是自然对数的底数。</p> </details> <h2 id="solution">Solution</h2> <h3 id="part-1">Part 1</h3> <p>The derivative of \(f(x)\) is</p> \[f'(x)=a^x\ln a-b\] <p>\(f''(x)=a^x\ln^2 a&gt;0\), so \(f'(x)\) is an strictly increasing function.</p> <p>When \(b&lt;0\), \(f'(x)&gt;\lim_{x\to-\infty}f'(x)=0\), so \(f(x)\) is an strictly increasing function.</p> <p>When \(b\ge 0\), solving \(f'(x)=0\) we can get \(x=\log_a\frac{b}{\ln a}\). Since \(f'(x)\) is increasing, we have:</p> <table> <thead> <tr> <th style="text-align: center">$x$</th> <th style="text-align: center">\(x\in\left(-\infty, \log_a\frac{b}{\ln a}\right)\)</th> <th style="text-align: center">\(x=\log_a\frac{b}{\ln a}\)</th> <th style="text-align: center">\(x\in\left(\log_a\frac{b}{\ln a}, +\infty\right)\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">\(f'(x)\)</td> <td style="text-align: center">\(f'(x)&lt;0\)</td> <td style="text-align: center">\(f'(x)=0\)</td> <td style="text-align: center">\(f'(x)&gt;0\)</td> </tr> <tr> <td style="text-align: center">\(f(x)\)</td> <td style="text-align: center">\(f(x)\) is decreasing</td> <td style="text-align: center">\(f(x)\) has a local minimum</td> <td style="text-align: center">\(f(x)\) is increasing</td> </tr> </tbody> </table> <p>As a result, when \(b&lt;0\), \(f(x)\) has a monotonically increasing interval \((-\infty, +\infty)\); when \(b\ge 0\), \(f(x)\) has a monotonically decreasing interval \(\left(-\infty, \log_a\frac{b}{\ln a}\right)\) and a monotonically increasing interval \(\left(\log_a\frac{b}{\ln a}, +\infty\right)\).</p> <h3 id="part-2">Part 2</h3> <p>Given that \(b&gt;2e^2&gt;0\), the function \(f(x)\) has a local minimum at \(x=\log_a\frac{b}{\ln a}\).</p> <p>Also, we have:</p> \[\begin{aligned} \lim_{x\to-\infty}f(x)&amp;=\lim_{x\to-\infty}a^x-bx+e^2=\infty\\ \lim_{x\to+\infty}f(x)&amp;=\lim_{x\to+\infty}a^x-bx+e^2=\infty \end{aligned}\] <p>Therefore, \(f(x)\) has two distinct zeros if and only if \(f\left(\log_a\frac{b}{\ln a}\right)&lt;0\).</p> <p>Let’s simplify \(f\left(\log_a\frac{b}{\ln a}\right)\):</p> \[\begin{aligned} f\left(\log_a\frac{b}{\ln a}\right)&amp;=a^{\log_a\frac{b}{\ln a}}-b\log_a\frac{b}{\ln a}+e^2\\ &amp;=\frac{b}{\ln a}-b\cdot\left(\log_ab-\log_a\ln a\right)+e^2\\ &amp;=\frac{b}{\ln a}-\frac{b\ln b}{\ln a}+\frac{b\ln\ln a}{\ln a} + e^2\\ &amp;=\frac{b}{\ln a}\left(1-\ln b+\ln\ln a\right)+e^2 \end{aligned}\] <p>For \(f\left(\log_a\frac{b}{\ln a}\right)&lt;0\) to hold, we must ensure:</p> \[\lim_{b\to \left(2e^{2}\right)^+}f\left(\log_a\frac{b}{\ln a}\right)\le 0\] <p>This implies:</p> \[\begin{aligned} 0&amp;\ge \lim_{b\to {\left(2e^{2}\right)}^+}f\left(\log_a\frac{b}{\ln a}\right)\\ &amp;=\lim_{b\to \left(2e^{2}\right)^+}\frac{b}{\ln a}\left(1-\ln b+\ln\ln a\right)+e^2\\ &amp;=\frac{2e^2}{\ln a}\left(1-\ln\left(2e^2\right)+\ln\ln a\right)+e^2\\ &amp;=\frac{2e^2}{\ln a}\left(\ln\ln a-1-\ln 2\right)+e^2\\ \end{aligned}\] <p>Hence, we should have:</p> \[2e^2\left(\ln\ln a-1-\ln 2\right)+e^2\ln a\le 0\] <p>This simplifies to:</p> \[2\ln\ln a+\ln a\le 2\ln 2+2\] <p>Let’s define \(\phi(x)=2\ln x+x\). The inequality above is equivalent to:</p> \[\phi(\ln a)\le \phi(2)\] <p>Given that \(\phi(x)\) is a strictly increasing function, we can have:</p> \[\ln a\le 2\Rightarrow a\le e^2\] <p>However, this is not the final answer since we should consider the whole range of \(b&gt;2e^2\) instead of a specific value of \(b\to \left(2e^2\right)^+\). But we have a necessary condition for \(a\) to satisfy.</p> <p>From \(f\left(\log_a\frac{b}{\ln a}\right)&lt;0\), we can get:</p> \[\frac{b}{\ln a}(1-\ln b+\ln\ln a)+e^2&lt;0\] <p>Let</p> \[g(a, b)=\frac{b}{\ln a}(1-\ln b+\ln\ln a)+e^2\] <p>We have:</p> \[\frac{\partial g}{\partial b}=\frac{1}{\ln a}(1-\ln b+\ln\ln a)-\frac{b}{\ln a}\cdot\frac{1}{b}=\frac{\ln\ln a-\ln b}{\ln a}\] <p>When \(a\le e^2, b&gt;2e^2\),</p> \[\ln\ln a-\ln b\le \ln\ln e^2-\ln 2e^2=\ln 2-\ln 2-2=-2&lt;0\] <p>So \(\frac{\partial g}{\partial b}&lt;0\) which means when \(a\le e^2\),</p> \[g(a, b) &lt; g(a, 2e^2)\le 0\] <p>So when \(a\le e^2\), \(f(x)=0\) must have two distinct solutions.</p> <p>In conclusion, the range of \(a\) is \(\left(1, e^2\right]\).</p> <h3 id="part-3">Part 3</h3> <p>When \(a=e\in(1, e^2], b&gt;e^4&gt;2e^2\), so \(f(x)=0\) has two distinct solutions.</p> <p>Let \(x_1, x_2\) be the two solutions, and \(x_1&lt;x_2\):</p> \[\begin{cases} e^{x_1}-bx_1+e^2=0 &amp; (1)\\ e^{x_2}-bx_2+e^2=0 &amp; (2) \end{cases}\] <p>\((1) - (2)\), we have:</p> \[e^{x_1}-e^{x_2}-b(x_1-x_2)=0\Rightarrow \frac{e^{x_1}-e^{x_2}}{x_1-x_2}=b\] <p>然后不会了啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊，有空再来更</p>]]></content><author><name>Pu Fanyi</name></author><category term="Notes"/><category term="Gao Kao"/><category term="Calculus"/><summary type="html"><![CDATA[明天就要高考去了，今天临时抱个佛脚]]></summary></entry><entry><title type="html">Classical Mechanics</title><link href="https://pufanyi.github.io/blog/ClassicalMechanics/" rel="alternate" type="text/html" title="Classical Mechanics"/><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://pufanyi.github.io/blog/ClassicalMechanics</id><content type="html" xml:base="https://pufanyi.github.io/blog/ClassicalMechanics/"><![CDATA[<p>偶然间刷到 Leonard Susskind 在 Stanford 的讲课视频，学着玩一玩。</p> <p><a href="https://theoreticalminimum.com/courses/classical-mechanics/2011/fall">视频链接</a></p> <hr/> <h1 id="state-diagrams-and-the-nature-of-physical-laws">State diagrams and the nature of physical laws</h1> <p>用状态来描述世界，这玩意儿在机器学习中也这么搞。</p> <p>但是在经典物理中我们认为如果我们知道这个世界处于某个状态，我们可以</p> <ol> <li>追溯过去，在状态图中就是每个状态入度为 \(1\)；</li> <li>预测未来，在状态图中就是每个状态出度为 \(1\)。</li> </ol> <p>那么在有穷图中这个图肯定是由多个环构成，当然其实 prof 在讲的时候说无穷图里我们也可以认为 \((\cdots\to -2\to 0\to 2\to 4\to\cdots)\) 是一个环。</p> <p>上课过程中有一个同学问了个问题，就是说按这样的话 classical statistical mechanics 怎么搞。这个是因为在状态中我们认为世界是已知的，但是在经典统计力学中一部分东西是未知的才能导致有概率的引入。我们认为世界是确定的。</p> <p>一个记号是 \(\dot{f}\) 表示 \(f\) 对事件的导数，也就是说 \(\dot{f} = \frac{\mathrm{d}f}{\mathrm{d}t}\)。</p> \[\begin{cases} \vec{v}=\dot{\vec{x}}\\ \vec{a}=\dot{\vec{v}}=\ddot{\vec{x}} \end{cases}\] <p>然后他举的例子是圆周运动：</p> \[\begin{cases} \vec{x} = \begin{bmatrix}r\cos\omega\theta\\ r\sin\omega\theta\end{bmatrix}\\ \vec{v} = \dot{\vec{x}} = \begin{bmatrix}-\omega r\sin\omega\theta\\ \omega r\cos\omega\theta\end{bmatrix}\\ \vec{a} = \ddot{\vec{x}} = \begin{bmatrix}-\omega^2r\cos\omega\theta\\-\omega^2r\sin\omega\theta\end{bmatrix}\\ \end{cases}\] <hr/> <h1 id="newtons-law-phase-space-momentum-and-energy">Newton’s law, phase space, momentum and energy</h1> <p>一个粒子的状态可以被描述为 \((\vec{x}, \vec{p})\)，即位置与动量。我们称整个空间为 phase space。</p> <p>根据牛顿第二定律：</p> \[\vec{F}=\dot{\vec{p}}\] <p>我们与 \(\vec{p}=m\dot{\vec{x}}\) 联立，我们就能解出 \((\vec{x}, \vec{p})\) 和 \(t\) 的关系。因此我们说牛顿力学是可逆的。</p>]]></content><author><name>Pu Fanyi</name></author><category term="Notes"/><category term="Classical Mechanics"/><category term="Physics"/><summary type="html"><![CDATA[学点好玩的]]></summary></entry><entry><title type="html">NTU MH3700 Numerical Analysis I</title><link href="https://pufanyi.github.io/blog/MH3700-Notes/" rel="alternate" type="text/html" title="NTU MH3700 Numerical Analysis I"/><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://pufanyi.github.io/blog/MH3700-Notes</id><content type="html" xml:base="https://pufanyi.github.io/blog/MH3700-Notes/"><![CDATA[<p>可选太多学不完了。</p> <p>趁着暑假对着 ppt 复习一下。</p> <p>尽管上课时候讲这玩意儿还是很严谨的，但时间原因以及毕竟选这门课纯属玩，很多情况下就不给出严格证明或者一笔带过了。</p> <hr/> <h2 id="root-finding">Root Finding</h2> <p>首先登场的是二分法，不讲。</p> <p>接下来说 fixed-point iteration。就是我们要解决 \(g(x)-x=0\) 也就是 \(g(x)=x\) 的问题。</p> <p>那么首先很显然的是如果 \(x\in[a, b]\) 时，\(g(x)\in[a, b]\)，那么在 \([a, b]\) 里铁定有一个解的。</p> <p>那啥时候不动点唯一呢？如果存在 \(k&lt;1\) 使得 \(\forall x\in[a, b], \left\lvert g'(x)\right\rvert \le k\)，那么我们说是唯一的。</p> <p>感性理解是可以嘟，严格证明用 Largrange 中值定理搞搞。</p> <p>然后 fixed-point iteration 就是我们一开始选个 \(p_0\)，然后 \(p_{n+1}=g(p_n)\)，蹲着他收敛就行。</p> <p>一定收敛吗？我们有一个定理就是如果存在一个 \(k\in(0, 1)\) 使得 \(\left\lvert g'(x)\right\rvert \le k\)，那么就一定收敛。</p> <p>证明的话因为我们有 Largrange 中值定理</p> \[\left|p_{n+1}-p\right|=\left|g(p_n)-g(p)\right|=\left|g'(\xi_n)\right|\cdot\left|p_n-p\right|\le k\cdot\left|p_n-p\right|\] <p>也就是说 \(k\) 越小收敛得越快。</p> <p>然后我们考虑我们要解 \(f(x)=0\)，我们要构造一个 \(g(x)\) 使得</p> \[f(x)=0\Leftrightarrow g(x)=x\] <p>我们可以有：</p> \[g(x)=x-\phi(x)f(x)\] <p>咱现在需要做的就是找到一个合适得 \(\phi(x)\) 让他收敛的尽量快。</p> <p>那怎么看收敛的尽量快呢，我们假设有个序列 \(\{p_n\}_{n=0}^\infty\)，如果存在 \(\alpha, \lambda&gt;0\) 使得</p> \[\lim_{n\to\infty}\frac{\left|p_{n+1}-p\right|}{\left|p_n-p\right|^\alpha}=\lambda\] <p>我们定义 \(\{p_n\}_{n=0}^\infty\) converge to \(p\) with order \(\alpha\).</p> <p>我们转到 \(g(x)\) 上的时候，我们发现 \(g'(p)=0\) 意味着这个序列的收敛速度一定是大于 \(1\) 的。我们考虑泰勒展开：</p> \[\lim_{n\to\infty}\frac{\left|p_{n+1}-p\right|}{\left|p_n-p\right|}=\lim_{n\to\infty}\frac{\left|g(p_n)-g(p)\right|}{\left|p_n-p\right|}=\left|g'(p)\right|=0\] <p>也就是说 \(\alpha=1\) 的时候 \(\lambda=0\)，而定义里说 \(\lambda&gt;0\)，也就是我们得让下面的阶数更大，即 \(\alpha&gt;1\)。</p> <p>那我们导一下 \(g(x)\)：</p> \[g'(x)=1-\phi(x)f'(x)-\phi'(x)f(x)\] <p>考虑到 \(f(p)=0\)，我们有</p> \[g'(p)=1-\phi(p)f'(p)\] <p>当 \(f'(p)=0\) 时，方程无解，否则我们可以让 \(\phi(p)=\frac{1}{f'(p)}\)。</p> <p>于是我们就推出了 Newton’s method：</p> \[p_{n+1}=p_n-\frac{f(p_n)}{f'(p_n)}\] <p>另一种 secant method 是因为有时候 \(f'(x)\) 很难求，我们可以用两个点的斜率代替：</p> \[p_{n+1}=p_n-f(p_n)\cdot\frac{p_n-p_{n-1}}{f(p_n)-f(p_{n-1})}\] <p>尽管这玩意儿的 rate of convergence 是 \(\frac{1+\sqrt{5}}{2}\approx1.618\)，但不需要求导，通常情况下 have better convergence than Newton per evaluation of \(f\)。</p> <p>然后现在问题是我们需要求解更高维度的方程，那我们就可以构造一个 \(\mathbb{R}^n\to\mathbb{R}^n\) 的函数 \(\boldsymbol{f}\) 求解</p> \[\boldsymbol{f}(\boldsymbol{x})=\boldsymbol{0}\] <p>大概感性理解一下，我们对 \(\boldsymbol{f}\) 做泰勒展开：</p> \[\boldsymbol{f}(\boldsymbol{a}+\boldsymbol{\delta})=\boldsymbol{f}(\boldsymbol{a})+\mathsf{D}\boldsymbol{f}(\boldsymbol{a})\cdot\boldsymbol{\delta}+\mathcal{O}\left(\left\|\boldsymbol{\delta}\right\|^2\right)\] <p>其中 \(\mathsf{D}\boldsymbol{f}(\boldsymbol{a})\) 是 Jacobian matrix，我们通常记作 \(\mathsf{J}(\boldsymbol{x})\)：</p> \[\mathsf{J}(\boldsymbol{x})=\mathsf{D}\boldsymbol{f}(\boldsymbol{x})=\begin{bmatrix} \frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_1}{\partial x_n} \\ \frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_2}{\partial x_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial f_n}{\partial x_1} &amp; \frac{\partial f_n}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_n}{\partial x_n} \end{bmatrix}\] <p>于是我们有迭代公式：</p> \[\boldsymbol{x}_{n+1}=\boldsymbol{x}_n-\mathsf{J}^{-1}(\boldsymbol{x}_n)\cdot\boldsymbol{f}(\boldsymbol{x}_n)\] <h2 id="interpolation">Interpolation</h2> <h3 id="polynomial-interpolation">Polynomial Interpolation</h3> <p>我们有一组点 \((x_0, y_0), (x_1, y_1), \cdots, (x_n, y_n)\)，我们要找一个多项式 \(\mathcal{P}(x)\) 使得 \(\mathcal{P}(x_i)=y_i\)。</p> <p>最直接的方法是解方程：</p> \[\boldsymbol{a}=\mathcal{V}^{-1}\boldsymbol{y}\] <p>其中 \(\mathcal{V}\) 是 Vandermonde matrix：</p> \[\mathcal{V}=\begin{bmatrix} 1 &amp; x_0 &amp; x_0^2 &amp; \cdots &amp; x_0^n \\ 1 &amp; x_1 &amp; x_1^2 &amp; \cdots &amp; x_1^n \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; x_n &amp; x_n^2 &amp; \cdots &amp; x_n^n \end{bmatrix}\] <p>复杂度 \(\mathcal{O}(n^3)\)。</p> <p>在历史上的话欧陆这边最有名的是 Lagrange 插值，以及 Newton 那边的 divided difference。</p> <p>Lagrange 插值是一种 \(\mathcal{O}(n^2)\) 的算法。</p> <p>我们想要的是构造一组基：</p> \[\mathcal{L}_{n, k}(x_i)=[i=k]\] <p>那我们就可以有：</p> \[\mathcal{P}_n(x)=\sum_{k=0}^n y_k\cdot \mathcal{L}_{n, k}(x)\] <p>而基的构造方式是：</p> \[\mathcal{L}_{n, k}(x)=\prod_{j\neq k}\frac{x-x_j}{x_k-x_j}\] <p>关于准确性我们用 Rolle 定理可以证明，对于 \((x_0, x_1, \cdots, x_n)\in[l, r]^n\)，如果 \(x\in[l, r]\)，我们有存在 \(\xi\in[l, r]\) 使得：</p> \[f(x)=\mathcal{P}_n(x)+\frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{i=0}^n(x-x_i)\] <p>接下来讲的是 Neville’s scheme，我们令 \(\mathcal{P}_{n-1, i}\) 表示用 \(x_0, x_1, \cdots, x_{i-1}, x_{i+1}, \cdots, x_n\) 插值出的多项式，我们有：</p> \[\mathcal{P}_n(x)=\frac{(x-x_j)\cdot\mathcal{P}_{n-1, j}(x)-(x-x_i)\cdot\mathcal{P}_{n-1, i}(x)}{x_i-x_j}\] <p>于是乎我们就能用这种方法进行 dp，令 \(\mathcal{P}_{l, r}(x)\) 表示通过 \(x_l, x_{l+1}, \cdots, x_r\) 插值出的多项式。我们就有：</p> \[\mathcal{P}_{l, r}(x)=\frac{(x-x_l)\cdot\mathcal{P}_{l+1, r}(x)-(x-x_r)\cdot\mathcal{P}_{l, r-1}(x)}{x_r-x_l}\] <p>这个做法的一个好处是可以在线性时间内增加一个点。</p> <p>另外一种方法叫做 Newton’s divided difference，我们定义：</p> \[\begin{aligned} f[x_i]&amp;=f(x_i) \\ f[x_i, x_{i+1}]&amp;=\frac{f[x_{i+1}]-f[x_i]}{x_{i+1}-x_i} \\ f[x_i, x_{i+1}, \cdots, x_{i+k}]&amp;=\frac{f[x_{i+1}, x_{i+2}, \cdots, x_{i+k}]-f[x_i, x_{i+1}, \cdots, x_{i+k-1}]}{x_{i+k}-x_i} \end{aligned}\] <p>我们有：</p> \[\mathcal{P}_n(x)=f[x_0]+\sum_{k=1}^n\left(f[x_0, x_1, \cdots, x_k]\cdot\prod_{j=0}^{k-1}(x-x_j)\right)\]]]></content><author><name>Pu Fanyi</name></author><category term="Notes"/><category term="Numerical Analysis"/><summary type="html"><![CDATA[Notes for NTU MH3700 Numerical Analysis I.]]></summary></entry></feed>