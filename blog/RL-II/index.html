<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="EPlMR3j8io1DcEScvNJuBPxLxCTSnNUjNwF4ZBhRO-I"> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reinforcement Learning Notes II - Policy Gradients &amp; Actor-Critic Methods | Fanyi Pu </title> <meta name="author" content="Fanyi Pu"> <meta name="description" content="Policy Gradients, Actor-Critic Methods, Model-based RL"> <meta name="keywords" content="Fanyi, Fanyi Pu, 濮凡轶"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?16404ec2cd2689e8d0f38f73fe0d38f9"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/avatar_very_small.webp?3828c08bd9576f9a024f464411e622b9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pufanyi.github.io/blog/RL-II/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Reinforcement Learning Notes II - Policy Gradients & Actor-Critic Methods",
            "description": "Policy Gradients, Actor-Critic Methods, Model-based RL",
            "published": "April 12, 2025",
            "authors": [
              
              {
                "author": "Pu Fanyi",
                "authorURL": "https://pufanyi.github.io",
                "affiliations": [
                  {
                    "name": "NTU, Singapore",
                    "url": "https://www.ntu.edu.sg/"
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Fanyi</span> Pu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="https://scholar.google.com/citations?user=58tv6skAAAAJ" rel="external nofollow noopener" target="_blank">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/oi-blog/">oi-blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume/resume.pdf">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Reinforcement Learning Notes II - Policy Gradients &amp; Actor-Critic Methods</h1> <p>Policy Gradients, Actor-Critic Methods, Model-based RL</p> </d-title> <d-byline></d-byline> <d-article> <h2 id="direct-policy-differentiation">Direct Policy Differentiation</h2> <p>REINFORCE Algorithm <d-cite key="williams1992simple"></d-cite>:</p> \[\begin{aligned} \nabla_{\theta} \mathcal{J}(\theta) &amp;= \int \nabla_{\theta} \pi_{\theta}(\tau) r(\tau) \mathrm{d}\tau \\ &amp;= \mathbb{E}_{\tau \sim \pi_{\theta}(r)}\left[r(\tau) \cdot \nabla_{\theta} \log \pi_{\theta}(\tau)\right] \\ &amp;= \mathbb{E}_{\tau \sim \pi_{\theta}(r)}\left[\left(\sum_{t=1}^T \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\right) \left(\sum_{t=1}^T r(s_t, a_t)\right)\right] \end{aligned}\] <p>所以说 \(r(\tau) \cdot {\nabla}_{\theta} \log \pi_{\theta}(\tau)\) 是 \({\nabla}_{\theta} \mathcal{J}(\theta)\) 的无偏估计，可惜这玩意儿的 variance 很高。</p> <h2 id="reduce-variance">Reduce Variance</h2> <p>仔细观察这个式子，其实这玩意儿是 MLE 那个梯度对 \(r(s_t, a_t)\) 加权了：</p> \[\nabla_{\theta} \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(r)}\left[\sum_{t=0}^T \Psi_t \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\right]\] <p>当前我们是有</p> \[\Psi_t = \sum_{t'=0}^T r(s_t', a_t')\] <h2 id="dont-let-the-past-distract-you">Don’t Let the Past Distract You</h2> <p>一种简单的方法来减小 varience，是我们令</p> \[\Psi_t = \sum_{t'=t}^T r(s_t', a_t')\] <p>因为其实对于 \(a_t\) 来说，他做啥对于 \(t\) 之前的 reward 来说是不具有参考价值的。因此我们主要考虑后面的 reward。这玩意儿直觉上挺清楚的，但数学上想了半天才想明白为啥是对的。主要参考了<a href="https://spinningup.openai.com/en/latest/spinningup/extra_pg_proof1.html" rel="external nofollow noopener" target="_blank">这篇文章</a> <d-cite key="SpinningUp2018"></d-cite>。</p> <p>证明的不造为啥让我想起了 MLE。主要用到的就是一个叫做 EGLP lemma 的东西（其实好像用这个 lemma 需要积分和导数的可交换性，貌似 <d-cite key="hogg2013introduction"></d-cite> 里写挺详细的）：</p> \[\mathbb{E}_{x \sim \mathbb{P}_{\theta}}\left[ \nabla_{\theta} \log \mathbb{P}_{\theta}(x)\right] = \int \mathbb{P}_{\theta}(x) \frac{\nabla_{\theta} \mathbb{P}_{\theta}(x)}{\mathbb{P}_{\theta}(x)} \mathrm{d}x = \nabla_{\theta} \int \mathbb{P}_{\theta}(x) \mathrm{d}x = 0\] <p>其实跟 MLE 是一样的嘛：</p> \[\mathbb{E}\left[ \nabla_{\theta} \log \mathcal{L}\left(x \mid \theta\right)\right] = 0\] <p>其实我们是要证明嘟是：</p> \[\mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}\left[\sum_{t=0}^T \sum_{t' &lt; t} r\left(s_t', a_t'\right) {\nabla}_{\theta} \log \pi_{\theta}\left(a_t \mid s_t\right)\right] = 0\] <p>也就是要证明当 $t’&lt;t$ 这个时候：</p> \[\mathbb{E}_{s_t, a_t, s_t', a_t' \sim \pi_{\theta}\left(s_t, a_t, s_t', a_t'\right)}\left[r\left(s_t', a_t'\right) \nabla_{\theta} \log \pi_{\theta}\left(a_t \mid s_t\right)\right] = 0\] <p>那么中心思想其实就是咋来区分 $t’&lt;t$ 捏，我们考虑 $t’&lt;t$ 是先 reward，再选择：</p> \[\mathbb{E}_{s_t', a_t' \sim \pi_{\theta}}\left[r\left(s_t', a_t'\right) \cdot \mathbb{E}_{s_t, a_t \sim \pi_{\theta}\left(s_t, a_t\right)}\left[\nabla_{\theta} \log \pi_{\theta}\left(a_t \mid s_t\right) \mid s_{t'}, a_{t'}\right]\right] = 0\] <p>其实也就是当 \(r(s_t', a_t')\) 不依赖于 \(s_t, a_t\) 的时候，本身这个</p> \[\mathbb{E}_{s_t, a_t \sim \pi_{\theta}\left(s_t, a_t\right)}\left[\nabla_{\theta} \log \pi_{\theta}\left(a_t \mid s_t\right) \mid s_{t'}, a_{t'}\right]\] <p>他就是 \(0\)。</p> <p>所以说最终结果是整个期望 \(0\)。</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-04-12-RL-II.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"pufanyi/pufanyi.github.io","data-repo-id":"R_kgDOJnYv-A","data-category":"General","data-category-id":"DIC_kwDOJnYv-M4CW4n7","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Fanyi Pu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: April 14, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?eaf77346e117baa09987a278a117b9a7"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?8d7ebe8276cfa922ec1506a5c6b20c13"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>