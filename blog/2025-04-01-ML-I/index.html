<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="math-review">Math Review</h2> <p>All modern machine learning algorithms are just nearest neighbors. It’s only that the neural networks are telling you the space in which to compute the distance.</p> <h4 id="svd">SVD</h4> <p><a href="https://web.stanford.edu/class/cs168/l/l9.pdf" rel="external nofollow noopener" target="_blank">Notes</a></p> \[A = U \Sigma V^\top=\sum_{i=1}^{\min\{m, n\}}\sigma_i u_i v_i^\top\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-01-ML-I/SVD-480.webp 480w,/assets/img/2025-04-01-ML-I/SVD-800.webp 800w,/assets/img/2025-04-01-ML-I/SVD-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2025-04-01-ML-I/SVD.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Compute largest \(k\) singular values and vectors: \(\mathcal{O}(kmn)\).</p> <p>Approximation:</p> \[\hat{A} = \sum_{i=1}^{k}\sigma_i u_i v_i^\top = U_k \Sigma_k V_k^\top\] <p>For all rank \(k\) matrices \(B\):</p> \[\|A - \hat{A}\|_F \le \|A - B\|_F\] <h3 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3> <p>Maximum likelihood estimation:</p> \[\hat{\theta} = \arg\max_{\theta\in\Theta} p(D\mid\theta)\] <p>Properties:</p> <ol> <li> <em>Consistency</em>: more data, more accurate (but maybe biased).</li> <li> <em>Statistically efficient</em>: least variance.</li> <li>The value of \(p(D\mid\theta_{\text{MLE}})\) is invariant to re-parameterization.</li> </ol> <h3 id="entropy">Entropy</h3> <p>要搞一个 “degree of surprise” 函数 \(h(p(x))\)，满足：</p> <ol> <li> \[h(p) \ge 0\] </li> <li> \[h(p) = 0 \iff p = 1\] </li> <li> \[x \perp y \iff h(p(x\land y)) = h(p(x)) + h(p(y))\] </li> <li> \[h(p_1) &gt; h(p_2)\iff p_1&lt;p_2\] </li> </ol> <p>根据 3 我们有</p> \[h(p_1 p_2) = h(p_1) + h(p_2)\] <p>如果我们令 \(f(\log p) = h(p)\) 的话，我们有</p> \[f(\log p_1 + \log p_2) = f(\log p_1) + f(\log p_2)\] <p>所以 \(f(p)\) 是一个线性函数。又因为 \(f(0)=0\)，所以 \(f(x)=cx\)。\(c&lt;0\) 因为 \(f\) 要单调递减。</p> </body></html>