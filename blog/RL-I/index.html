<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="EPlMR3j8io1DcEScvNJuBPxLxCTSnNUjNwF4ZBhRO-I"> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reinforcement Learning Notes I - Introduction &amp; Imitation Learning | Fanyi Pu </title> <meta name="author" content="Fanyi Pu"> <meta name="description" content="Introductions, MDPs, Imitation Learning"> <meta name="keywords" content="Fanyi, Fanyi Pu, 濮凡轶"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?16404ec2cd2689e8d0f38f73fe0d38f9"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/avatar_very_small.webp?3828c08bd9576f9a024f464411e622b9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pufanyi.github.io/blog/RL-I/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Reinforcement Learning Notes I - Introduction & Imitation Learning",
            "description": "Introductions, MDPs, Imitation Learning",
            "published": "April 11, 2025",
            "authors": [
              
              {
                "author": "Pu Fanyi",
                "authorURL": "https://pufanyi.github.io",
                "affiliations": [
                  {
                    "name": "NTU, Singapore",
                    "url": "https://www.ntu.edu.sg/"
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Fanyi</span> Pu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="https://scholar.google.com/citations?user=58tv6skAAAAJ" rel="external nofollow noopener" target="_blank">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/oi-blog/">oi-blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume/resume.pdf">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Reinforcement Learning Notes I - Introduction &amp; Imitation Learning</h1> <p>Introductions, MDPs, Imitation Learning</p> </d-title> <d-byline></d-byline> <d-article> <h2 id="introduction">Introduction</h2> <p>我们把 \(\left&lt;s_t, a_t\right&gt;\) 打包起来，其实他就构成了一个 Markov chain:</p> \[p_\theta (\tau) = p_\theta (s_1) \prod_{t=1}^T \pi_\theta (a_t \mid s_t) \cdot \mathbb{P}(s_{t+1} \mid s_t, a_t)\] <p>对于 learning objective：</p> \[\theta^{*} = \arg \max_{\theta} \mathcal{J}\left(\theta\right) = \arg \max_{\theta} \mathbb{E}_{\tau \sim {\pi}_{\theta} \left(\tau\right)}\left[r\left(\tau\right)\right]\] <p>我们定义我现在在 \(s_t\)，做了 action \(a_t\)，然后按照 \(\pi\) 所得到的期望 reward 为</p> \[\mathcal{Q}^{\pi} \left(s_t, a_t\right) = \sum_{t'=t}^T \mathbb{E}_{\left(s_t', a_t'\right) \sim \pi}\left[r\left(s_t', a_t'\right) \mid s_t, a_t\right]\] <p>然后我们定义 \(\mathcal{V}\) 表示现在我在 \(s_t\) 的时候遵循 \(\pi\) 所得到的期望 reward：</p> \[\mathcal{V}^{\pi} \left(s_t\right) = \sum_{t'=t}^T \mathbb{E}_{\left(s_t', a_t'\right) \sim \pi}\left[r\left(s_t', a_t'\right) \mid s_t\right] = \mathbb{E}_{a_t \sim \pi\left(a_t \mid s_t\right)}\left[\mathcal{Q}^{\pi} \left(s_t, a_t\right)\right]\] <h2 id="types-of-algorithms">Types of Algorithms</h2> <ul> <li>Policy Gradients: 跟往常一样，就是求导然后去做优化</li> <li>Value-based: 去直接计算最优解的 \(\mathcal{Q}\) 和 \(\mathcal{V}\)，然后通过这个推出 \(\pi\)</li> <li>Actor-critic: 通过计算当前 \(\hat{\theta}\) 的 \(\hat{\mathcal{Q}}\) 和 \(\hat{\mathcal{V}}\)，然后通过这个去进行优化，得到最终的 \(\pi\)</li> <li>Model-based RL: 使用模型来代表各种参数，然后进行优化</li> </ul> <h2 id="imitation-learning">Imitation Learning</h2> <p>数据 \(\mathcal{D} = \left\{(s_i, a_i)\right\}_{i=1}^N\)，然后我们要设计 \(\pi\) 去你和 \(a\)</p> <p>首先最直觉的想法是直接去最优化</p> \[\theta^* = \arg\min_\theta \frac{1}{\left|\mathcal{D}\right|} \sum_{(s, a)\in \mathcal{D}} \left\|\hat{a} - a\right\|^2\] <p>但这样肯定是不对的。比如说我现在前面有一个障碍物，一半人要左转，一半人要右转。结果这样拟合出来，最优的 \(\hat{a}\) 就成直行撞墙了。</p> <p>所以说我们考虑生成模型，也就是说去拟合 \(p_\theta (a \mid s)\)：</p> \[\theta^* = \arg\min_\theta \mathbb{E}_{(s, a)\sim \mathcal{D}}\left[-\log p_\theta (a \mid s)\right]\] <p>那怎么搞这个 \(p\) 呢？离散的好说，对于连续的，我们一般有三种方法：</p> <ol> <li>Mixture of Gaussians</li> <li>Discretize + Autoregressive</li> <li>Diffusion</li> </ol> <p>但是这些 imitation learning 有一个问题，就是 \(p_\pi(s)\neq p_\mathcal{D}(s)\)，这就导致了我们学习的东西，和我们现实中遇见的是稍有不同的。这边有个东西叫做“covariate shift”。<d-cite key="ross2011reduction"></d-cite> 详细解释了这一现象。</p> <p>DAgger<d-cite key="ross2011reduction"></d-cite>: 现根据 \(p_\theta\) 走到 \(s'\)，然后去询问 expert 应该怎么做，然后把这个数据加到 \(\mathcal{D}\) 里面去。</p> <h2 id="paper-reading">Paper Reading</h2> <h3 id="a-reduction-of-imitation-learning-and-structured-prediction-to-no-regret-online-learning-">A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning <d-cite key="ross2011reduction"></d-cite> </h3> <h3 id="diffusion-policy-visuomotor-policy-learning-via-action-diffusion-">Diffusion Policy: Visuomotor Policy Learning via Action Diffusion <d-cite key="chi2023diffusion"></d-cite> </h3> <h3 id="training-diffusion-models-with-reinforcement-learning-">Training Diffusion Models with Reinforcement Learning <d-cite key="black2023training"></d-cite> </h3> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-04-11-RL-I.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"pufanyi/pufanyi.github.io","data-repo-id":"R_kgDOJnYv-A","data-category":"General","data-category-id":"DIC_kwDOJnYv-M4CW4n7","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Fanyi Pu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: April 18, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?eaf77346e117baa09987a278a117b9a7"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?8d7ebe8276cfa922ec1506a5c6b20c13"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>